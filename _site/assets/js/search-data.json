{
  "1": {
    "id": "1",
    "title": "Diagnosing Errors",
    "content": "Diagnosing Errors This is a list of potential issues that could arise while running the pipeline, which are typically associated with runtime errors, which could be due to issues with the cluster (e.g., worker-node failure during your job) or job parameterisation (e.g., underestimating RAM required). This page is, by no means, exhaustive, and will be updated as more issues are reported. We note here that although the pipeline is constructed to have job dependencies, i.e., if one job fails, all the subsequent jobs are meant to be cancelled, this does not always work. We find that there are several cases where CASA will crash, and SLURM does not recognise that the job has terminated and will continue to execute the other jobs in the pipeline. We suspect that this is due to a difference in error handling between SLURM and CASA. Please keep a watch on the status of your jobs using the ./summary.sh script, as well as the log files created to make sure that the pipeline is proceeding correctly. ORTE error An error of the form -- ORTE has lost communication with its daemon located on node: hostname: slwrk-067 This is usually due to either a failure of the TCP network connection to the node, or possibly an internal failure of the daemon itself. We cannot recover from this failure, and therefore will terminate the job. -- -- An ORTE daemon has unexpectedly failed after launch and before communicating back to mpirun. This could be caused by a number of factors, including an inability to create a connection back to mpirun due to a lack of common network interfaces and/or no route found between them. Please check network connectivity (including firewalls and network routing requirements). -- where slwrk-067 can be any of the SLURM worker nodes, typically means that there was a communication/network error between the SLURM head node and the worker node. Please report this issue to the ILIFU support team at support@ilifu.ac.za. This is indicative of an underlying hardware issue, rather than a software problem. Unable to launch jobs in SLURM If the status of your job is (launch failed requeued held), please file a ticket with support@ilifu.ac.za. Memory error If you see the phrase MemoryError in the stderr logs (this can be located by grep -i MemoryError logs/*.err) this is typically indicative that CASA did not have enough memory to complete the task. This often happens while running flagdata and does not always halt execution of the pipeline. Reduce the number of tasks per node in the config file before re-launching the pipeline, as that will allocate more memory per task.",
    "url": "http://localhost:4000/docs/processMeerKAT/Diagnosing-Errors/",
    "relUrl": "/docs/processMeerKAT/Diagnosing-Errors/"
  },
  "2": {
    "id": "2",
    "title": "Example Use Cases",
    "content": "Calibration Our algorithmic approach toward calibration in the pipeline can be found in [[Calibration-in-processMeerKAT]]. Stokes I Calibration (Continuum) Stokes I calibration is achieved in xx_yy_solve.py, and includes standard delay, bandpass and gain calibration. Within this pipeline, this is done to obtain better statistics for a second round of flagging. To calibrate for only Stokes I, the xy_yx_solve.py and xy_yx_apply.py scripts may be replaced with xx_yy_solve.py and xx_yy_apply.py, or removed completely. However, since the 2nd round of flagging is different to the 1st, and much more effective, it should not be skipped. Furthermore, for good calibration, new solutions should always be derived after flagging. Lastly, it is recommended the ‘xy_yx’ scripts be run anyway, since linear feeds tend to have non-negligible coupling between the feeds, and so even to get a good Stokes I image, full Stokes calibration may be required. Therefore, for Stokes I calibration, the ‘xx_yy’ scripts may be replaced with the ‘xx_yy’ scripts, in which case only a minimal speedup will be gained. Therefore, this use case is generally discouraged. One reason for this use case may be where issues arise with the calibration, and to simplify the processing to what is well understood. Full Stokes Calibration (Polarisation) Full Stokes calibration is the default mode of the pipeline, which, in addition to the calibration listed above, includes calibration of instrumental leakage, X-Y phase, source polarisation, and Stokes Q and U from gain variations, using the CASA helper task GainFromQU. This is achieved by using the phase calibrator to solve as a function of parallactic angle. Currently the pipeline does not solve for absolute polarisation angle, since the CALIBRATE_POLARIZATION intent is missing from MeerKAT datasets. Additionally, CASA currently has no functionality to solve for wide-band leakage and QU, but assumes a constant value across single spectral windows. Both of these issues will be addressed in future releases. Calibrating a Small Sub-Band (Spectral Line) A small sub-band of frequencies can be selected and calibrated by specifying a range of frequencies with argument spw in your config file. e.g. spw = &#39;0:1350~1450MHz&#39; means partition will extract only frequencies between 1350-1400 MHz for calibration. For very small bandwidth of several MHz, it may not be necessary to use MMS and MPI (see section MS only below). This mode is useful for calibration of spectral line data. Data Format Default: MS -&gt; MMS By default, the pipeline will convert the input measurement set (MS) into a multi-measurement set (MMS), partitioning the data by scan, such that the number of sub-MSs making up the MMS is equal to the number of scans. The input MS does not need to be copied to your working directory, since partition.py needs only to read the input data (e.g. from /data/projects/). At the end of the pipeline, split.py will split each of the field IDs specified in your config file into MMS format. This is the default behaviour, since the default value of keepmms in the config file is True. This also ensures tclean makes use of MPI (and multiple CPUs) during quick_tclean.py, so that your imaging runs much more quickly. This mode is encouraged for users who only want to have quicklook images, or who have written a tclean script that will make use of MPI, that they can insert at the end of the pipeline (see [[Using-the-pipeline#Inserting-your-own-scripts]]. MS -&gt; MMS -&gt; MS When setting keepmms=False in your config file, the pipeline will convert your data to MMS as usual, but during running split, each of the field IDs specified in your config file will be written in MS format. By default, quick_tclean.py will still run, and will make use of multiple CPUs, but not MPI, meaning your imaging will run slower. This mode is encouraged when the user wishes to do their own imaging that requires an MS. MS only (single thread processing) The pipeline can be run on a MS, where the user would have to request 1 node and 1 task per node. In the case, the user would need to skip the partition.py script, by removing it from the scripts list in the config file, copy the MS do their working directory, and use this as the input dataset. This mode in generally not encouraged, but may be useful for small datasets (tens of GB - e.g. small bandwidth of several MHz), when using MMS has little to no advantage. In such a case, the user may need to manually run partition (e.g. to select a spw), and set createmms=False. Alternatively, the user can run split or mstransform to select an spw. Field IDs Default: Primary and Secondary Calibrator A standard observation will have a primary calibrator (e.g. J1939-6342 or J0408-6545), which is both the bandpass and total flux calibrator, and a secondary calibrator, which is the phase calibrator (and is also used for polarisation calibration). Single calibrator and target In rare cases, a primary calibrator may be close enough to the target(s) that only one calibrator is used throughout the observation. This use case is supported, but caution must be used to ensure the correct flux scale. Multiple calibrator field IDs Multiple field IDs can be specified by writing a comma-separated list to your config file. However, this use case is not supported, since tasks such as setjy and qufromgain cannot handle this. Restarting or Using Part of the Pipeline There are a few steps within the pipeline that only need to be run once for a given dataset. For datasets that have already been through the default pipeline, the reference antenna and list of bad antennas would have been calculated, and can be found in logs/calc_refant-*.out. These can be written your new config file, where you also set calcrefant=False. More generally, users can run any selected parts of the pipeline by editing the scripts argument or passing scripts in via the command-line arguments (see [[Using-the-pipeline#Inserting-your-own-scripts]]). The pipeline is not designed to run twice within the same directory, since CASA will complain the files already exist. The pipeline can be killed and re-run from the start at any point, but care should also be taken to ensure the data are not corrupted (e.g. during flagging or solving).",
    "url": "http://localhost:4000/docs/processMeerKAT/Example-Use-Cases/",
    "relUrl": "/docs/processMeerKAT/Example-Use-Cases/"
  },
  "3": {
    "id": "3",
    "title": "Quick Start",
    "content": "1. In order to use the processMeerKAT.py script, source the setup.sh file: source /data/exp_soft/pipelines/master/setup.sh which will add the correct paths to your $PATH and $PYTHONPATH in order to correctly use the pipeline. We recommend you add this to your ~/.profile, for future use. 2. Build a config file: processMeerKAT.py -B -C myconfig.txt -M mydata.ms This defines several variables that are read by the pipeline while calibrating the data, as well as requesting resources on the cluster. The config file parameters are described by in-line comments in the config file itself wherever possible. 3. To run the pipeline: processMeerKAT.py -R -C myconfig.txt This will create submit_pipeline.sh, which you can then run like ./submit_pipeline.sh to submit all pipeline jobs to the SLURM queue. Other convenience scripts are also created that allow you to monitor and (if necessary) kill the jobs. summary.sh provides a brief overview of the status of the jobs in the pipeline, findErrors.sh checks the log files for commonly reported errors, and killJobs.sh kills all the jobs from the current run of the pipeline, ignoring any other jobs you might have running. For help, run processMeerKAT.py -h, which provides a brief description of all the command line arguments. The documentation can be accessed on the pipelines website, or on the Github wiki.",
    "url": "http://localhost:4000/docs/processMeerKAT/Quick-Start/",
    "relUrl": "/docs/processMeerKAT/Quick-Start/"
  },
  "4": {
    "id": "4",
    "title": "Release Notes",
    "content": "Version 1.0 This is the first release of the IDIA Pipelines’ processsMeerKAT package, to be used on the Ilifu SLURM cluster. The software uses a parallelized implementation of CASA to calibrate interferometric (imaging) data from the MeerKAT telescope. The current release includes the following functionality: The processMeerKAT.py script builds a config based on an input measurement set (MS). The pipeline currently only does cross-calibration, or a’priori (1GC) calibration. This includes parallelised flagging using FLAGDATA (tfcrop and rflag). Flux bootstrapping, gain and bandpass calibration. Full stokes calibration. Quick-look imaging (i.e. without selfcal, w-projection, etc) of the calibrators and science targets. Diagnostic plots of the calibration tables and corrected data. Uses CASA 5.4.1, Python 2.7, and Singularity 2.6.1. Please consult the documentation on https://idia-pipelines.github.io/ for more information. Known Issues Broadband polarization: CASA does not natively support solving broad-band polarizations, i.e., it is not sensitive to rotation measure (RM). The assumption is that the RM within a single spectral window (SPW) is constant, however MeerKAT has only a single SPW that spans the entire bandwidth. We have identified future workarounds (which is to split up the band into several SPWs), however presently the broadband polarization models do contain systematic errors. SLURM partition: We recommend using the Test02 SLURM partition for the moment (set via the –partition option in processMeerKAT.py), due to ongoing issues in the Main SLURM partition. Slow plotting: Generating plots of the calibrated visibilities is very time consuming, often running to a few hours. However, as this is the last step of the pipeline, the calibrated, split measurement sets and images should be ready for further analysis while the plots are being generated. The speed of plotting is limited by how quickly plotms can generate the plots. Field IDs: The pipeline does not currently support specifying multiple fields for anything other than the targets. Flux scale: Although the fluxes of the calibrated targets and calibrator sources are typically accurate to within a few percent, we find that there are certain datasets that result in a flux scale that is down by a factor of a few. We are in the process of tracking down the root cause of these issues, and expect to issue a fix soon. However, if you do happen to notice that the fluxes of one or more of the sources/targets are off (either higher/lower), please report it by creating an issue in the Github repository. Calculation of antenna statistics: The amplitude and RMS per antenna computed in calc_refant.py does not match what is found by CASA task visstat, and decreases as a function of antenna number. We expect to issue a fix soon. Resource allocation: We set the number of threads to half the number of scans + 1 (master) + 10%, so that for tasks reading the target or phasecal (since we partition by scans - i.e. the number of sub-MSs = the number of scans), the number of threads is approximately the number of sub-MSs being read. For tasks reading only sub-MSs corresponding to other calibrators (e.g. bandpass), many threads will not be used Similarly, we use a single memory value for all threadsafe tasks, and hardcode 100 GB for single thread tasks Empty rows in sub-MSs: Some tasks might complain that no valid data were found in a sub-MS, but generally this seems to be a “harmless” error, and doesn’t seem to affect the progress of the calibration/pipeline. Exit codes Some jobs fail in the queue that shouldn’t have, and others don’t fail when they should Generally the pipeline continues even when dependencies legitimately fail",
    "url": "http://localhost:4000/docs/processMeerKAT/Release-Notes/",
    "relUrl": "/docs/processMeerKAT/Release-Notes/"
  },
  "5": {
    "id": "5",
    "title": "SLURM and MPICASA",
    "content": "Parallel CASA Using SLURM at IDIA SLURM is a resource and job management system that is available on many clusters. Jobs/tasks are typically submitted to the job management system, and are inserted into a job queue; the job is executed when the requested resources become available. SLURM is currently used with the IDIA cluster. Please consult the Appendix at the bottom of this page on the computing environments available at IDIA, and how to use Singularity containers. While SLURM Clusters provide the option to request and reserve resources to work in an interactive mode, its preferred to submit jobs to the queue to be run in a non-interactive way. To run a CASA script in a non-interactive way in the SLURM cluster, you would use the following steps. Write your CASA script. Write an associated SBATCH script for your job. Submit the script (i.e., your job) to the queue using sbatch. The image below illustrates these different steps. Write your CASA Script CASA scripts are written in Python. An entire pipeline can be written in such a script, that includes flagging, initial calibration and imaging. It is important to note that different CASA tasks use different schemes for parallelism, when writing your script. For example, flagdata parallelises by scan and is thus RAM intensive; tclean parallelises by frequency and is thus CPU intensive. Therefore, a single script that includes flagging and imaging could have sub-optimal usage of a cluster resources for some tasks, and optimal usage for others. Keep this in mind when writing your script. vis = &#39;blah&#39; var1 = &#39;something&#39; var2 = 1e-3 casatask(vis=vis, var1=var1, var2=var2) casatask2(vis=vis) Write your SBATCH Script An SBATCH script is a bash script that wraps the relevant SLURM parameters needed for your script. Consult the following website for more details on how to use SBATCH: https://slurm.schedmd.com/sbatch.html Here’s an example of an SBATCH script that submits a TCLEAN job: #!/bin/bash #SBATCH -N 4-4 #SBATCH --tasks-per-node 48 #SBATCH -J tclean #SBATCH -m plane=8 #SBATCH -o casameer-batch-1530187312-tclean.sh.stdout.txt #SBATCH -e casameer-batch-1530187312-tclean.sh.stderr.txt #Run the application: /data/users/frank/casa-cluster/casa-prerelease-5.3.0-115.el7/bin/mpicasa /usr/bin/singularity exec ~/casameer.simg &quot;casa&quot; --nologger --log2term --nogui -c tclean.py Please consult our documentation/wiki pages for more details on how software and containers are used on the IDIA Cloud. There are a few important SBATCH parameters to define: --nodes or -N specifies the node count, i.e., the nodes requested for the job. --tasks-per-node specifies the number of parallel tasks to execute on each node. --mode or -m specifies the mode in which the tasks are distributed to each node. This parameter is useful for scripts that include flagging. Since flagging is parallelised by scan, the first node(s) could run out of RAM for a particular flagging job. This would lead to SLURM killing the offending task(s), hence killing the main job. There are two distribution modes that can be used to solve this problem. plane=X distributes X jobs at a time, in a round-robin fashion across nodes. The cyclic mode distributes single tasks at a time in a round-robin fashion across nodes. plane=X or cyclic modes are useful for jobs that are RAM limited, i.e., when you need to use the aggregated RAM that’s in the total pool requested. The -J, -o and -e parameters More about SLURM SLURM is a workload manager that will distribute jobs across a specified cluster environment. It understands how to control an MPI-aware job via mpirun and hence also via mpicasa. In principle this means that SLURM should be able to schedule and manage a CASA job that is running across a cluster. Following is a “practical” definition of some SLURM keywords that should help clarify how to best to allocate resources. task : A “task” by SLURM’s definition is what one would usually call a “process” on a regular computer. Similar to a process, a task has its own memory allocation that it does not share with other tasks. Each task is then operated on independently via MPI. This also means a more fine-grained parallelism can be employed per task, by using multiple threads (e.g. via openMP) to work on a single task. –cpus-per-task : Defines the number of CPUs to dedicate to a single task. If each task can take advantage of multiple threads, setting this value to more than one can speed things up further (e.g., tclean in CASA is parallelised across openMP and MPI) –mem-per-cpu : The RAM dedicated to each CPU in the node. At the moment, the IDIA cluster is set to 4096MB per CPU. If a job is running out of memory, setting this to a larger value can help. Alternatively, as mentioned above if the task can take advantage of more threads, it may be preferable to set --cpus-per-task instead. -m/–distribution : This controls how the tasks are allocated across the requested nodes. The sbatch man page has a very good explanation on the various modes available. Notes on CASA Tasks and Parallelism Running CASA through SLURM requires calling CASA via mpicasa. CASA understands how to use mpi on tasks that are optimised for mpi (such as flagdata, tclean, setjy, and applycal) while operating as per usual on tasks that are not mpi aware (like gaincal). Ideally, the only change to an existing script would be to add a call to partition at the top. Below are some notes on tasks. partition: In order to run across a cluster the partition task needs to be called prior to running any other tasks. partition creates a multi-measurement set (MMS) that is a collection of multiple SUBMS’s, each of which will be operated upon as a task in SLURM. By default CASA will split the MS along the spectral window (spw) axis, and across scans. The number of SUBMSes created can be specified in partition, however it seems that specifying a number larger than what CASA would decide leads to some strangeness with the metadata (and a failure of tasks that operate on the MMS). tclean: In order to run across a cluster, parallel=True should be specified in tclean. However, if savemodel=&#39;modelcolumn&#39; is also specified, it triggers some kind of a race condition between the different nodes where they are competing for write access, and the task crashes. So setting savemodel=&#39;virtual&#39; or savemodel=&#39;none&#39; are the only options that work. Both the makePSF step and the minor cycles of deconvolution are openMP aware, and can exploit additional resources specified via --cpus-per-task in the SLURM sbatch file.",
    "url": "http://localhost:4000/docs/processMeerKAT/SLURM-and-MPICASA/",
    "relUrl": "/docs/processMeerKAT/SLURM-and-MPICASA/"
  },
  "6": {
    "id": "6",
    "title": "Access to IDIA Machines",
    "content": "Accessing IDIA Machines This page provides details on how to get access to the IDIA virtual machines. While this page is specific to the IDIA-Pipelines project, the details related to requesting access and connecting to a machine are identical. You can request access using the following form. In particular, please mention the team that you are a part of when you request access. This is extremely important, since it will help us provide you with access to the relevant project files. Joining the IDIA Pipelines Team Access to the IDIA-Pipeline resources can be arranged if you are part of the Pipelines Team. Please contact Bradley Frank (bradley@idia.ac.za) if you are interested in joining. Documentation and wiki access are provided through a GitHub project, so please sign-up to GitHub (if you haven’t already) and join the IDIA Pipelines repo as a contributor. Access There are a pool of IDIA machines that are available for usage. Please note that the machine names mentioned here are meant for usage by the IDIA-Pipelines team. In practice, we will provide you with a machine name after you’ve requested access. You can access IDIA machines via a Jupyter-Hub (preferred) or SSH (traditional). The URL usually corresponds to the name of the VM, and is served on the IDIA domain. For example, our helo node can be accessed via https://helo.idia.ac.za. Jupyter-Hub A Virtual Machine (VM) can be accessed online using your browser (Chrome/Firefox preferred). Simply type in the URL provided. You will be presented with a login window for the Jupyter-Hub. Use your previously generated (LDAP) username and password to access your account. SSH To SSH into an IDIA VM, you will need your SSH key. This is how you would SSH into helo with X-forwarding: $ ssh -XY -i /path/to/your/key.pem helo.idia.ac.za -l &lt;username&gt; Changing your Password You need to change your generic password as soon as possible. You can do this with the following command (one line on the prompt): $ ldappasswd -H ldap://10.102.4.109 -x -D &quot;cn=username,ou=users,dc=idia,dc=arc,dc=ac,dc=za&quot; -W -S -A Important: In the command above, you will need to change username to your username. You will then be prompted for passwords as follows: Twice for your current password (verification). Twice for your new password. Once for your LDAP password, which will be your old password. This is required to bind to the LDAP server to commit the change to your password. This is a little more involved then usual, because your credentials are not specific to a machine. Access on the cloud is coordinated on using the LDAP server – this is why you need to do the authenticate -&gt; enter new password -&gt; authorize/bind process to propagate your password to the server. This allows us to provide you with access to any machine provisioned for your project without having to remember a myriad of passwords. Incidentally, you can do this without ssh’ing into the machine – simply open up a terminal using the Jupyter-Hub and use this to change your password. Storage There are several storage areas available. Most (all) Pipelines machines will have the IDIA storage attached. This means that you will have access to your work and your data on any system that you’re logged into. Access to data is managed with Unix groups. Please take careful note of the following. /users/&lt;username&gt;/, where /users is a shared BeeGFS volume. /data/users/&lt;username&gt;/, where /data is a shared BeeGFS volume. This is the preferred space for you to store your longish term data products. /data/&lt;Project&gt; is a shared directory on BeeGFS for a project. You can fetch raw data from here. Please steer away from dumping data into this directory. /scratch/users/&lt;username&gt;/ is the shared working directory for , where `/scratch` is a BeeGFS volume). This is the preferred space for intermediate data products, e.g., you can use this space to do imaging. Setting up your own VM. Smaller VMs can be spun-up for (sub)-projects. Additionally, users will have to follow the aforementioned process to setup their accounts. Please use the Google form to design and Request a VM. GitHub There are two important GitHub resources that project members can use: IDIA-Pipelines Blog: This is the repo for this website, and there is an associated wiki for project members to share information. IDIA Containers: This is very important repository for the Singularity containers used at IDIA.",
    "url": "http://localhost:4000/docs/access/",
    "relUrl": "/docs/access/"
  },
  "7": {
    "id": "7",
    "title": "Calibration in processMeerKAT",
    "content": "processMeerKAT implements a CASA based wide-band full Stokes calibration pipeline (in the linear basis). Broadly, the pipeline aims to “do the right thing” and by keeping the steps as general as possible we believe that there should be no need for fine tuning in order to obtain a well calibrated dataset. The pipeline is implemented as a series of SLURM sbatch scripts that in turn call CASA scripts. The scripts are separated out to make optimal use of MPI, by splitting out sections that can be run in parallel (via mpicasa and SLURM) and sections that do not take advantage of parallel architectures. The logical steps are: Input validation : This script performs a few basic validity checks, on the default config file, and on the input MS. the existence of the input MS, and the data types of the inputs specified in the config file are all verified before the pipeline continues to the next steps. If reference antenna calculation is not requested, a simple check is performed to verify that the input reference antenna exists in the MS. Otherwise, the following paragraph describes the details of reference antenna calculation. Reference antenna calculation : If the calcrefant parameter in the config file is set to True, then this script is executed. The algorithm works by calculating the median and standard deviation over all the visibility amplitudes for a given antenna, and iterates over every antenna in the array. Any outlier antennas, in the top 2 and bottom 5 percentile of this distribution are then flagged. The reference antenna is selected to be the un-flagged antenna with the smallest visibility rms. Data partition : The input measurement set (MS) is partitioned into a multi-measurement set (MMS) using the CASA task partition. This task splits up the main MS into smaller SUBMSs that are individual units of a larger logical MMS. The number of SUBMSs created are equal to the number of scans in the input MS. Partitioning the data in this manner allows for more efficient use of computation while using MPI, since each SUBMS can be independently operated on by different MPI workers. Flagging (round 1) : The first of two rounds of pre-calibration flagging. If badfreqranges and badants are specified in the config file, they are flagged. These lists are also allowed to be empty. Following that the data are clipped at the level of 50 Jy to eliminate the strongest RFI and the tfcrop algorithm is run independently on the primary and secondary calibrators and the target(s). setjy : The setjy task is run on the specified primary calibrators - this step is run once each before the first and second rounds of calibration. Parallel hand calibration : Standard delay, bandpass and gain calibration is run on the data, in order to obtain better statistics for a second round of flagging. Flagging (round 2) : Similar to the first round, the tfcrop algorithm is run independently on the primary and secondary calibrator and the target(s). The thresholds are lower than the first round as the algorithm is now operating on calibrated data. Cross hand calibration : The cross hand calibration recipe broadly follows the CASA guide for polarisation calibration in the linear basis. The previous round of calibration is cleared, and a new set of bandpass, delay and parallel hand gains are computed. Using the CASA helper routine qufromgain, the Q and U values of the secondary calibrator are computed, and are then used to solve for the frequency dependent leakages and XY calibration (i.e., “Dflls” and “XYf” calibration in polcal). The fluxes are then bootstrapped from the primary calibrator to the other fields specified in the config file. If the same source is specified as both the flux and secondary calibrator, no bootstrapping is performed as the time-dependent gain solutions should be correctly scaled to the fluxes specified in setjy. Splitting out calibrated data : Finally the calibrated data are averaged down in time and frequency by the amount specified in the config file, and the target(s) and calibrators are split out into separate MMSs for further imaging/processing. Detailed description What follows is a more detailed description of each of the steps described above, where applicable. Flagging (round 1) : If a list of bad frequency ranges and bad antennas is specified, those are flagged. Further, any autocorrelations are also flagged using mode=&#39;manual&#39; and autocorr=True in the flagdata parameters. Subsequently, flagdata is called on the calibrators and target sources with conservative limits to clip out the worst RFI. It also makes a single call to tfcrop to flag data at a 6 $ sigma$ limit. tfcrop in this case is preferred, since the as yet uncalibrated bandpass shape should be taken care of by fitting a piecewise polynomial across the band. setjy : By default, the ‘Perley-Butler 2010’ flux scale is used, since it is the only one which contains the popular southern calibrator PKS B1934-638. In case the calibrator J0408-6545 is present in the data, it is preferred. A broadband Stokes I model for J0408-6545 is used, via the manual mode of setjy. Cross hand calibration : The full Stokes calibration procedure is done across as much of the SPW as is requested in the config file. In the default case, the entire SPW (spanning ~ 800 MHz) is calibrated across. The caveat here is that CASA does not support a true wideband, full polarization calibration. For example the Stokes Q and U values of a source with non-zero RM across the band will not be correctly accounted for. The assumption CASA makes is that the bandwidth is split into several smaller SPWs (such as is the case of VLA or ALMA) and that the Stokes parameters within each SPW can be assumed to be a constant. We have identified work-arounds to this, and will be implementing the fix in upcoming versions of the pipeline. The cross-hand calibration performs the following steps: Delay calibration (the K term), time averaged, parallel hand Bandpass calibration (the B term), time averaged, parallel hand Cross hand delay calibration (the KCROSS term), time averaged, cross hand after the cross-hand delay calibration is performed, we iterate over calculating the time dependent gains. Initially the time-dependent gains are calculated for the primary and secondary calibrators, as a function of time and parallactic angle. The polarization properties of the secondary are assumed to be unknown, and are determined from the gain variation as a function of parallactic angle. This is fit for by the qufromgain task, which is contained in almapolhelpers and can be accessed in CASA by from almapolhelpers import * This imports several helper tasks that are meant to solve ALMA polarization, but are general enough to work with any telescope that has linear feeds. Once the fractional Q and U values are determined for the phase calibrator, the gain solutions are recomputed with the fractional polarization as an input, in theory resulting in more accurate gain solutions. This is followed by a call to xyamb, also within almapolhelpers that breaks the ambiguity in the X-Y phases for the solutions generated by qufromgain. These can be cross-checked with the solutions obtained by running gaincal with gaintype=&#39;XYf+QU which solves for the X-Y phase as a function of frequency, assuming an unknown source Q, U value. Finally we run polcal in the Dflls mode in order to calculate the polarization leakage (the D term) as a function of frequency (f), using a linear least squares algorithm (lls). Finally, we bootstrap the fluxes from the primary to the secondary using fluxscale.",
    "url": "http://localhost:4000/docs/processMeerKAT/calibration-in-processmeerkat/",
    "relUrl": "/docs/processMeerKAT/calibration-in-processmeerkat/"
  },
  "8": {
    "id": "8",
    "title": "Singularity Containers",
    "content": "Singularity Containers At IDIA, astronomical software packages are provided and managed using Singularity containers. Containers are managed and built by our developers. If you have any questions/issues relating to containers, please send an email to support@ilifu.ac.za. You can find more documentation about containers on Ilifu here. Available Containers The most recent, stable versions of containers are available in /data/exp_soft/containers/. Some of these containers can be accessed via the Jupyter-Hub, but all of them can be accessed via the terminal (i.e., once you’ve ssh’d into an IDIA machine). Here are a few examples of the latest stable Singularity container builds that are available in /data/exp_soft/containers/: jupyter-casa-latest.simg Allows you to use CASA tasks via the Jupyter notebook. Note that visualisation tools like plotms or viewer will not work on the notebook. casa-stable-5.3.0.simg Contains the CASA binaries for the 5.3.0. release. kern4.simg Contains all the software packages provided by the Kern repository. sourcefinding_py3.simg Builds of commonly used source finding packages, e.g., PyBDSF. Reporting a bug / Requesting Software Please send an email to support@ilifu.ac.za to report issues, or to request new containers, or for existing containers to be updated. Using Containers Shell You can access software in Singularity containers using two methods. Firstly, you can shell into the container. You will enter a shell which provides access to the software provided in that container: $ singularity shell /data/exp_soft/containers/casa-stable-5.3.0.simg Singularity: Invoking an interactive shell within container... Singularity casa-stable-5.3.0.simg:~&gt; casa --nologger --log2term --nogui ========================================= The start-up time of CASA may vary depending on whether the shared libraries are cached or not. ========================================= IPython 5.1.0 -- An enhanced Interactive Python. CASA 5.3.0-143 -- Common Astronomy Software Applications 2018-08-07 12:37:28 INFO ::casa CASA Version 5.3.0-143 --&gt; CrashReporter initialized. Enter doc(&#39;start&#39;) for help getting started with CASA... Using matplotlib backend: TkAgg CASA &lt;1&gt;: print &quot;Hello World&quot; Hello World Exec You can also pass a command to a container (using the exec argument), which then gets executed via the shell in that container. For example, here’s an illustration of how to use the exec argument to jump into an interactive CASA session: $ singularity exec /data/exp_soft/containers/casa-stable-5.3.0.simg casa --nologger --log2term --nogui ========================================= The start-up time of CASA may vary depending on whether the shared libraries are cached or not. ========================================= IPython 5.1.0 -- An enhanced Interactive Python. CASA 5.3.0-143 -- Common Astronomy Software Applications 2018-08-07 12:41:19 INFO ::casa CASA Version 5.3.0-143 --&gt; CrashReporter initialized. Enter doc(&#39;start&#39;) for help getting started with CASA... Using matplotlib backend: TkAgg CASA &lt;1&gt;: inp listobs --&gt; inp(listobs) # listobs :: List the summary of a data set in the logger or in a file vis = &#39;&#39; # Name of input visibility file (MS) selectdata = True # Data selection parameters field = &#39;&#39; # Selection based on field names or field index numbers. Default is all. spw = &#39;&#39; # Selection based on spectral-window/frequency/channel. antenna = &#39;&#39; # Selection based on antenna/baselines. Default is all. timerange = &#39;&#39; # Selection based on time range. Default is entire range. correlation = &#39;&#39; # Selection based on correlation. Default is all. scan = &#39;&#39; # Selection based on scan numbers. Default is all. intent = &#39;&#39; # Selection based on observation intent. Default is all. feed = &#39;&#39; # Selection based on multi-feed numbers: Not yet implemented array = &#39;&#39; # Selection based on (sub)array numbers. Default is all. uvrange = &#39;&#39; # Selection based on uv range. Default: entire range. Default units: meters. observation = &#39;&#39; # Selection based on observation ID. Default is all. verbose = True # Controls level of information detail reported. True reports more than False. listfile = &#39;&#39; # Name of disk file to write output. Default is none (output is written to logger only). listunfl = False # List unflagged row counts? If true, it can have significant negative performance impact. cachesize = 50 # EXPERIMENTAL. Maximum size in megabytes of cache in which data structures can be held. CASA &lt;2&gt;: The true utility of the exec argument is to execute commands non-interactively: $ singularity exec /data/exp_soft/containers/casa-stable-5.3.0.simg casa --nologger --log2term --nogui -c &quot;print &#39;Hello World&#39;&quot; ========================================= The start-up time of CASA may vary depending on whether the shared libraries are cached or not. ========================================= IPython 5.1.0 -- An enhanced Interactive Python. CASA 5.3.0-143 -- Common Astronomy Software Applications 2018-08-07 12:45:43 INFO ::casa CASA Version 5.3.0-143 --&gt; CrashReporter initialized. Hello World $ While the command may seem cumbersome, it is very useful when trying to build scripts that utilise several containers.",
    "url": "http://localhost:4000/docs/containers/",
    "relUrl": "/docs/containers/"
  },
  "9": {
    "id": "9",
    "title": "DEEP 2 Tutorial",
    "content": "DEEP 2 Tutorial This tutorial walks you through running the various steps of the pipeline for a single DEEP 2 dataset, which is a snapshot (~20 minutes on source), 16-dish MeerKAT observation of a random patch of sky using the old ROACH-2 correlator, 11 GB in size. To begin, ssh into the ilifu cluster (slurm.ilifu.ac.za), and create a working directory somewhere on the filesystem (e.g. /scratch/users/your_username/tutorial/). 1. Source setup.sh, which will add to your PATH and PYTHONPATH source /data/exp_soft/pipelines/master/setup.sh 2. Build a config file, using verbose mode, and pointing to the DEEP 2 dataset processMeerKAT.py -B -C tutorial_config.txt -M /data/projects/deep/1491550051.ms -v After some initial debug output, you should get the following output, with different timestamps 2019-02-28 02:36:14,421 INFO: Extracting field IDs from measurement set &quot;/data/projects/deep/1491550051.ms&quot; using CASA. 2019-02-28 02:36:14,422 DEBUG: Using the following command: srun --nodes=1 --ntasks=1 --time=10 --mem=4GB --partition=Main singularity exec /data/exp_soft/pipelines/casameer-5.4.1.xvfb.simg casa --nologger --nogui --nologfile -c /data/exp_soft/pipelines/master/processMeerKAT/cal_scripts/get_fields.py -B -M /data/projects/deep/1491550051.ms -C tutorial_config.txt -N 8 -t 4 . . . 2019-02-28 02:37:39,788 WARNING: The number of threads (8 node(s) x 4 task(s) = 32) is not ideal compared to the number of scans (12) for &quot;/data/projects/deep/1491550051.ms&quot;. 2019-02-28 02:37:39,788 WARNING: Config file has been updated to use 2 node(s) and 4 task(s) per node. 2019-02-28 02:37:39,788 INFO: For the best results, update your config file so that nodes x tasks per node = 7. 2019-02-28 02:37:40,045 INFO: Multiple fields found with intent &quot;CALIBRATE_FLUX&quot; in dataset &quot;/data/projects/deep/1491550051.ms&quot; - [0 1]. 2019-02-28 02:37:40,110 WARNING: Only using field &quot;0&quot; for &quot;fluxfield&quot;, which has the most scans (1). 2019-02-28 02:37:40,110 WARNING: Putting extra fields with intent &quot;CALIBRATE_FLUX&quot; in &quot;targetfields&quot; - [1] 2019-02-28 02:37:40,111 INFO: Multiple fields found with intent &quot;CALIBRATE_BANDPASS&quot; in dataset &quot;/data/projects/deep/1491550051.ms&quot; - [0 1]. 2019-02-28 02:37:40,111 WARNING: Only using field &quot;0&quot; for &quot;bpassfield&quot;, which has the most scans (1). 2019-02-28 02:37:40,112 INFO: Multiple fields found with intent &quot;CALIBRATE_PHASE&quot; in dataset &quot;/data/projects/deep/1491550051.ms&quot; - [1 2]. 2019-02-28 02:37:40,112 WARNING: Only using field &quot;2&quot; for &quot;phasecalfield&quot;, which has the most scans (5). 2019-02-28 02:37:40,123 INFO: [fields] section written to &quot;tutorial_config.txt&quot;. Edit this section to change field IDs (comma-seperated string for multiple IDs). 2019-02-28 02:37:41,990 INFO: Config &quot;tutorial_config.txt&quot; generated. This calls CASA via the default singularity container without writing log files, and runs get_fields.py. It calls srun, requesting only 1 node, 1 task, 4 GB of memory, and a 10 minute time limit, to increase the likelihood of jumping to the top of the queue. The purpose of this call is to extract the field IDs corresponding to our different targets, and check the nodes and tasks per node against the number of scans, each of which is handled by a thread (see section 3). The output statements with DEBUG correspond to those output during verbose mode. The warnings display when multiple fields are present with the same intent, but only one is extracted, corresponding to the field with the most scans. In this case the extras are moved to targetfields (i.e. for applying calibration and imaging). 3. View the config file created, which has the following contents: [crosscal] minbaselines = 4 # Minimum number of baselines to use while calibrating specavg = 1 # Number of channels to average after calibration (during split) timeavg = &#39;8s&#39; # Time interval to average after calibration (during split) keepmms = True # Output MMS (True) or MS (False) during split spw = &#39;0:860~1700MHz&#39; # Spectral window / frequencies to extract for MMS calcrefant = True # Calculate reference antenna in program (overwrites &#39;refant&#39;) refant = &#39;m005&#39; # Reference antenna name / number standard = &#39;Perley-Butler 2010&#39; # Flux density standard for setjy badants = [] # List of bad antenna numbers (to flag) badfreqranges = [ &#39;944~947MHz&#39;, # List of bad frequency ranges (to flag) &#39;1160~1310MHz&#39;, &#39;1476~1611MHz&#39;, &#39;1670~1700MHz&#39;] [slurm] nodes = 2 ntasks_per_node = 4 plane = 2 mem = 236 partition = &#39;Main&#39; time = &#39;12:00:00&#39; submit = False container = &#39;/data/exp_soft/pipelines/casameer-5.4.1.xvfb.simg&#39; mpi_wrapper = &#39;/data/exp_soft/pipelines/casa-prerelease-5.3.0-115.el7/bin/mpicasa&#39; name = &#39;&#39; verbose = True scripts = [(&#39;validate_input.py&#39;, False, &#39;&#39;), (&#39;partition.py&#39;, True, &#39;&#39;), (&#39;calc_refant.py&#39;, False, &#39;&#39;), (&#39;flag_round_1.py&#39;, True, &#39;&#39;), (&#39;setjy.py&#39;, True, &#39;&#39;), (&#39;xx_yy_solve.py&#39;, False, &#39;&#39;), (&#39;xx_yy_apply.py&#39;, True, &#39;&#39;), (&#39;flag_round_2.py&#39;, True, &#39;&#39;), (&#39;setjy.py&#39;, True, &#39;&#39;), (&#39;xy_yx_solve.py&#39;, False, &#39;&#39;), (&#39;xy_yx_apply.py&#39;, True, &#39;&#39;), (&#39;split.py&#39;, True, &#39;&#39;), (&#39;quick_tclean.py&#39;, True, &#39;&#39;), (&#39;plot_solutions.py&#39;, False, &#39;&#39;)] [data] vis = &#39;/data/projects/deep/1491550051.ms&#39; [fields] bpassfield = &#39;0&#39; fluxfield = &#39;0&#39; phasecalfield = &#39;2&#39; targetfields = &#39;3,1&#39; This config file contains four sections - crosscal, slurm, data, and fields. The fields IDs we just extracted, seen in section [fields], correspond to field 0 for the bandpass calibrator, field 0 for the total flux calibrator, field 2 for the phase calibrator, and fields 3 and 1 for the science targets (i.e. the DEEP 2 field + another calibrator). Only the target may have multiple fields. If a field isn’t found according to its intent, a warning is displayed, and the field for the total flux calibrator is selected. If the total flux calibrator isn’t present, the program will display an error and terminate. The SLURM parameters in section [slurm] correspond to those seen by running processMeerKAT.py -h. By default, for all threadsafe scripts (i.e. those with True in the scripts list), we use 8 nodes, 4 tasks per node (=32 threads), 236 GB of memory (per node), and plane=2 (which distributes four tasks onto one node before moving onto next node). During step 2, only 12 scans were found, and since partition.py partitions the data into one sub-measurement set (sub-MS) per scan, only 12 sub-MSs will exist in the multi-measurement set (see section 19). Assuming each observation has a phase calibrator bracketing each target scan, at most, 6 sub-MSs will be operated on at any given time, each handled by one thread, and a master thread. So we aim to have a limit of nscans+1+10% threads, with the 10% to account for the occasional thread that hangs. For this dataset, this limit is 7 threads, so get_fields.py attempts to match this number by using the specified number of tasks per node and increasing the node count from 1 until the number of threads is more than the limit, terminating at 2 nodes x 4 tasks per node = 8 threads. For script that aren’t threadsafe (i.e. those with False in the scripts list), we use a single node, and a single task per node. For both scripts that are threadsafe and those that aren’t, we use a single CPU per task, and explicitly export OMP_NUM_THREADS=1, since there is little evidence of a speedup with more than one CPU per task. However, for quick_tclean.py we use 4 CPUs per task. The cross-calibration parameters in section [crosscal] correspond to various CASA parameters passed into the calibration tasks that the pipeline used, each of which is documented in [[Calibration-in-processMeerKAT]]. By default all frequency ranges listed in badfreqranges, and all antenna numbers listed in badants, will be flagged out entirely. The third script the pipeline runs (calc_refant.py) will likely change the value of refant, and add a list of bad antennas to badant. 4. Run the pipeline using your config file processMeerKAT.py -R -C tutorial_config.txt You should get the following output, with different timestamps 2019-02-28 02:44:06,078 DEBUG: Copying &#39;tutorial_config.txt&#39; to &#39;.config.tmp&#39;, and using this to run pipeline. 2019-02-28 02:44:06,096 WARNING: Changing [slurm] section in your config will have no effect unless you [-R --run] again 2019-02-28 02:44:06,131 DEBUG: Wrote sbatch file &quot;validate_input.sbatch&quot; 2019-02-28 02:44:06,138 DEBUG: Wrote sbatch file &quot;partition.sbatch&quot; 2019-02-28 02:44:06,144 DEBUG: Wrote sbatch file &quot;calc_refant.sbatch&quot; 2019-02-28 02:44:06,150 DEBUG: Wrote sbatch file &quot;flag_round_1.sbatch&quot; 2019-02-28 02:44:06,156 DEBUG: Wrote sbatch file &quot;setjy.sbatch&quot; 2019-02-28 02:44:06,172 DEBUG: Wrote sbatch file &quot;xx_yy_solve.sbatch&quot; 2019-02-28 02:44:06,247 DEBUG: Wrote sbatch file &quot;xx_yy_apply.sbatch&quot; 2019-02-28 02:44:06,253 DEBUG: Wrote sbatch file &quot;flag_round_2.sbatch&quot; 2019-02-28 02:44:06,260 DEBUG: Wrote sbatch file &quot;setjy.sbatch&quot; 2019-02-28 02:44:06,267 DEBUG: Wrote sbatch file &quot;xy_yx_solve.sbatch&quot; 2019-02-28 02:44:06,273 DEBUG: Wrote sbatch file &quot;xy_yx_apply.sbatch&quot; 2019-02-28 02:44:06,279 DEBUG: Wrote sbatch file &quot;split.sbatch&quot; 2019-02-28 02:44:06,291 DEBUG: Wrote sbatch file &quot;quick_tclean.sbatch&quot; 2019-02-28 02:44:06,331 DEBUG: Wrote sbatch file &quot;plot_solutions.sbatch&quot; 2019-02-28 02:44:06,338 INFO: Master script &quot;submit_pipeline.sh&quot; written, but will not run. A number of sbatch files have now been written to your working directory, each of which corresponds to the python script in the list of scripts set by the scripts parameter in our config file. Our config file was copied to .config.tmp, which is the config file written and edited by the pipeline, which the user should not touch. A bash script called submit_pipeline.sh was written, which we will look at soon. However, this script was not run, since we set submit = False in our config file (you can change this in your config file, or by using option [-s --submit] when you build your config file with processMeerKAT.py). Lastly, a logs directory was created, which will store the log files from the SLURM output. 5. View validate_input.sbatch, which has the following contents: #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=100GB #SBATCH --job-name=validate_input #SBATCH --distribution=plane=1 #SBATCH --output=logs/validate_input-%j.out #SBATCH --error=logs/validate_input-%j.err #SBATCH --partition=Main #SBATCH --time=12:00:00 export OMP_NUM_THREADS=1 srun singularity exec /data/exp_soft/pipelines/casameer-5.4.1.xvfb.simg casa --nologger --nogui --logfile logs/validate_input-${SLURM_JOB_ID}.casa -c /data/exp_soft/pipelines/master/processMeerKAT/cal_scripts/validate_input.py --config .config.tmp Since this script is not threadsafe, the job is called with srun, and is configured to run a single task on a single node, with 100 GB of memory. The last line shows the CASA call of the validate_input.py task, which will validate the parameters in the config file. 6. Run the first sbatch job sbatch validate_input.sbatch You should see the following output, corresponding to your SLURM job ID Submitted batch job 1097914 7. View your job in the SLURM queue squeue You will see something similar to the following, with other people’s jobs mixed into the queue. JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1097914 Main validate jcollier R 0:13 1 slwrk-008 We can see the job with name validate was submitted to SLURM worker node 8, amongst a number of jobs in the main partition, the JupyterSpawner partition, and possible other partitions. Your job may list (Priority), which means it is too low a priority to be run at this point, or (Resources), which means it is waiting for resources to be made available. NOTE: You can view just your jobs with squeue -u your_username, an individual job with squeue -j 1097914, and just the jobs in the main partition with squeue -p Main. You can view which nodes are allocated, which are idle, which are mixed (i.e. partially allocated), and which are down in the main partition with sinfo -p Main. Often it is good idea to check this before selecting your SLURM parameters. 8. View partition.sbatch, which has the following contents: #!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 #SBATCH --cpus-per-task=1 #SBATCH --mem=236GB #SBATCH --job-name=partition #SBATCH --distribution=plane=2 #SBATCH --output=logs/partition-%j.out #SBATCH --error=logs/partition-%j.err #SBATCH --partition=Main #SBATCH --time=12:00:00 export OMP_NUM_THREADS=1 /data/exp_soft/pipelines/casa-prerelease-5.3.0-115.el7/bin/mpicasa singularity exec /data/exp_soft/pipelines/casameer-5.4.1.xvfb.simg casa --nologger --nogui --logfile logs/partition-${SLURM_JOB_ID}.casa -c /data/exp_soft/pipelines/master/processMeerKAT/cal_scripts/partition.py --config .config.tmp Here we see the same default SLURM parameters for threadsafe tasks, as discussed in section 3. We now use mpicasa as the mpi wrapper, since we are calling a threadsafe script partition.py, which calls CASA task partition, which partitions the data into several sub measurement sets (sub-MSs - see section 14 below) and selects only frequencies specified by your spectral window with parameter spw in your config file. 9. Submit your job and watch it in the queue sbatch partition.sbatch Submitted batch job 1097917 squeue -j 1097917 You will see something similar to the following, showing that 2 nodes are now being used (worker nodes 60 &amp; 61). JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1097917 Main partitio jcollier R 0:22 2 slwrk-[060-061] Wait until the job completes, before step 10. 10. View the contents of 1491550051.mms. You should see two new files - 1491550051.mms and 1491550051.mms.flagversions, which corresponds to the data and flag data of a multi-measurement set (MMS). From now on, the pipeline operates on these data, rather than the . The same path to the original MS will remain in your config file under section [data], but each task will point to your MMS. Inside this MMS, you will find the same tables and metadata as in a normal MS, but you will also see a SUBMSS directory, which should have the following contents. 1491550051.mms.0000.ms 1491550051.mms.0004.ms 1491550051.mms.0008.ms 1491550051.mms.0001.ms 1491550051.mms.0005.ms 1491550051.mms.0009.ms 1491550051.mms.0002.ms 1491550051.mms.0006.ms 1491550051.mms.0010.ms 1491550051.mms.0003.ms 1491550051.mms.0007.ms 1491550051.mms.0011.ms These are the 12 sub-MSs, partitioned by this observation’s 12 scans of the various targets. If we now view the CASA log, you will find a bunch of junk output from mpicasa (often including nominal “errors”, sometimes severe), and 13 calls of partition, corresponding to 12 workers for your 12 sub-MSs, and one master process. Similarly, your standard output logs will contains 8 sets of output from CASA launching, corresponding to the 8 threads (i.e. 2 nodes x 4 tasks per node) and some junk output from mpicasa. Again, your standard error log should be empty. 11. Run calc_refant.sbatch and watch your submitted job sbatch calc_refant.sbatch watch sacct You will initially see something similar to the following JobID JobName Partition Account AllocCPUS State ExitCode - - - - - -- 1097917 partition Main b03-pipel+ 8 COMPLETED 0:0 1097917.bat+ batch b03-pipel+ 4 COMPLETED 0:0 1097917.0 orted b03-pipel+ 1 COMPLETED 0:0 1097918 calc_refa+ Main b03-pipel+ 1 RUNNING 0:0 1097918.0 singulari+ b03-pipel+ 1 RUNNING 0:0 sacct lists all recently submitted jobs and their status. If your job fails, it will list FAILED under State. However, please note jobs running mpicasa often state they have failed when they haven’t. Similarly, when jobs do genuinely fail, the pipeline may continue to run. Both of these are issues we are working to overcome. When your job completes, you will see something similar to the following JobID JobName Partition Account AllocCPUS State ExitCode - - - - - -- 1097917 partition Test02 b03-pipel+ 8 COMPLETED 0:0 1097917.bat+ batch b03-pipel+ 4 COMPLETED 0:0 1097917.0 orted b03-pipel+ 1 COMPLETED 0:0 1097918 calc_refa+ Main b03-pipel+ 1 COMPLETED 0:0 1097918.bat+ batch b03-pipel+ 1 COMPLETED 0:0 1097918.0 singulari+ b03-pipel+ 1 COMPLETED 0:0 Control-C to exit out of watch. 12. View the contents of your logs directory ls logs calc_refant-1097918.casa calc_refant-1097918.err calc_refant-1097918.out partition-1097917.casa partition-1097917.err partition-1097917.out validate_input-1097914.casa validate_input-1097914.err validate_input-1097914.out As specified in our sbatch file, standard out is written to logs/calc_refant-1097918.out, standard error is written to logs/calc_refant-1097918.err, and the CASA logs are written to logs/calc_refant-1097918.casa. Your standard output log and CASA log will have little of interest, but your standard error log will contain the following output: 2019-02-28 03:02:33,870 INFO: Flux field scan no: 1 2019-02-28 03:02:34,034 INFO: Antenna statistics on total flux calibrator 2019-02-28 03:02:34,035 INFO: (flux in Jy averaged over scans &amp; channels, and over all of each antenna&#39;s baselines) 2019-02-28 03:02:34,035 INFO: ant median rms 2019-02-28 03:03:10,900 INFO: All 8.92 275.02 2019-02-28 03:03:10,900 INFO: 7 8.10 199.94 (best antenna) 2019-02-28 03:03:10,900 INFO: 0 8.84 305.28 (1st good antenna) 2019-02-28 03:03:10,900 INFO: setting reference antenna to: 7 2019-02-28 03:03:10,900 INFO: Bad antennas: [5, 15] Here we see calc_refant.py has selected antenna 7 as the best reference antenna, which measures comparable amplitude for the total flux calibrator compared to antenna 0, but a lower RMS. It has also found antennas 5 and 15 to be bad enough to flag out. 13. View ant_stats.txt You should see the following contents, corresponding to the amplitude and RMS each of the antennas measure ant median rms 0 8.84 305.28 1 7.80 251.12 2 7.69 279.64 3 8.13 219.86 4 9.37 322.45 5 7.10 253.62 6 7.95 254.35 7 8.10 199.94 8 8.21 326.82 9 10.15 306.20 10 9.00 258.92 11 11.57 270.17 12 10.55 270.40 13 13.38 396.25 14 13.66 434.22 15 243.71 569.51 14. View .config.tmp You should now see refant = 7 and badants = [5, 15]. 15. Edit your config file to run the next steps Edit tutorial_config.txt to remove the first two and last six tuples in the scripts parameter, so it looks like the following: scripts = [(&#39;flag_round_1.py&#39;, True, &#39;&#39;), (&#39;setjy.py&#39;, True, &#39;&#39;), (&#39;xx_yy_solve.py&#39;, False, &#39;&#39;), (&#39;xx_yy_apply.py&#39;, True, &#39;&#39;)] Replace refant and badants with what was found by validate_input.py, and select the submit option, so it looks like the following: [crosscal] refant = 7 badants = [5, 15] 16. Run the pipeline using your config file processMeerKAT.py -R -C tutorial_config.txt You should see the following output, with different timestamps 2019-02-28 03:18:26,585 DEBUG: Copying &#39;tutorial_config.txt&#39; to &#39;.config.tmp&#39;, and using this to run pipeline. 2019-02-28 03:18:26,605 WARNING: Changing [slurm] section in your config will have no effect unless you [-R --run] again 2019-02-28 03:18:26,615 DEBUG: Wrote sbatch file &quot;flag_round_1.sbatch&quot; 2019-02-28 03:18:26,624 DEBUG: Wrote sbatch file &quot;setjy.sbatch&quot; 2019-02-28 03:18:26,631 DEBUG: Wrote sbatch file &quot;xx_yy_solve.sbatch&quot; 2019-02-28 03:18:26,639 DEBUG: Wrote sbatch file &quot;xx_yy_apply.sbatch&quot; 2019-02-28 03:18:26,647 INFO: Running master script &quot;submit_pipeline.sh&quot; Copying tutorial_config.txt to .config.tmp, and using this to run pipeline. Submitting flag_round_1.sbatch SLURM queue with following command: sbatch flag_round_1.sbatch Submitting setjy.sbatch SLURM queue with following command sbatch -d afterok:1097919 --kill-on-invalid-dep=yes setjy.sbatch Submitting xx_yy_solve.sbatch SLURM queue with following command sbatch -d afterok:1097919,1097920 --kill-on-invalid-dep=yes xx_yy_solve.sbatch Submitting xx_yy_apply.sbatch SLURM queue with following command sbatch -d afterok:1097919,1097920,1097921 --kill-on-invalid-dep=yes xx_yy_apply.sbatch Submitted sbatch jobs with following IDs: 1097919,1097920,1097921,1097922 Run killJobs.sh to kill all the jobs. Run summary.sh to view the progress. Run findErrors.sh to find errors (after pipeline has run). Run displayTimes.sh to display start and end timestamps (after pipeline has run). As before, we see the sbatch files being written to our working directory. Since we set submit=True, submit_pipeline.sh has been run, and all output after that (without the timestamps) comes from this bash script. After the first job is run (sbatch flag_round_1.sbatch), each other job is run with a dependency on all previous jobs (e.g. sbatch -d afterok:1097919,1097920,1097921 --kill-on-invalid-dep=yes xx_yy_apply.sbatch). We can see this by calling squeue -u your_username, which shows those jobs (Dependency). submit_pipeline.sh then writes four job scripts, all of which are explained in the output, written to jobScripts with a timestamp appended to the filename, and symlinked from your working directory. findErrors.sh finds errors after this pipeline run has completed, overlooking all nominal MPI errors. These tasks follow the first step of a two-step calibration process that is summarised in [[Calibration-in-processMeerKAT]]. 17. Run ./summary.sh This script simply calls sacct for all jobs submitted within this pipeline run. You should get output similar to the following. JobID JobName Partition Elapsed NNodes NTasks NCPUS MaxDiskRead MaxDiskWrite NodeList TotalCPU State ExitCode - - -- -- - - -- 1097927 flag_round_1 Main 00:00:04 2 8 slwrk-[060-061] 00:00:00 RUNNING 0:0 1097927.0 orted 00:00:04 1 1 1 slwrk-061 00:00:00 RUNNING 0:0 1097928 setjy Main 00:00:00 2 8 None assigned 00:00:00 PENDING 0:0 1097929 xx_yy_solve Main 00:00:00 1 1 None assigned 00:00:00 PENDING 0:0 1097930 xx_yy_apply Main 00:00:00 2 8 None assigned 00:00:00 PENDING 0:0 Those PENDING are the jobs with dependencies. Once this pipeline run has completed, ./summary.sh should give output similar to the following. JobID JobName Partition Elapsed NNodes NTasks NCPUS MaxDiskRead MaxDiskWrite NodeList TotalCPU State ExitCode - - -- -- - - -- 1097927 flag_round_1 Main 00:08:25 2 8 slwrk-[060-061] 11:04.530 COMPLETED 0:0 1097927.batch batch 00:08:25 1 1 4 11759M 2971M slwrk-060 04:46.830 COMPLETED 0:0 1097927.0 orted 00:08:25 1 1 1 14493M 1084M slwrk-061 06:17.699 COMPLETED 0:0 1097928 setjy Main 00:06:18 2 8 slwrk-[060-061] 03:21.451 COMPLETED 0:0 1097928.batch batch 00:06:18 1 1 4 24722M 24575M slwrk-060 02:23.206 COMPLETED 0:0 1097928.0 orted 00:06:17 1 1 1 5779M 5655M slwrk-061 00:58.245 COMPLETED 0:0 1097929 xx_yy_solve Main 00:03:28 1 1 slwrk-066 01:59.767 COMPLETED 0:0 1097929.batch batch 00:03:28 1 1 1 0.22M 0.18M slwrk-066 00:00.068 COMPLETED 0:0 1097929.0 singularity 00:03:28 1 1 1 10275M 5M slwrk-066 01:59.699 COMPLETED 0:0 1097930 xx_yy_apply Main 00:06:30 2 8 slwrk-[060-061] 03:32.520 COMPLETED 0:0 1097930.batch batch 00:06:30 1 1 4 8237M 4228M slwrk-060 01:29.291 COMPLETED 0:0 1097930.0 orted 00:06:29 1 1 1 13386M 6554M slwrk-061 02:03.228 COMPLETED 0:0 18. View caltables directory The calibration solution tables have been written to caltables/1491550051.*, including bcal, gcal, fluxscale and kcal, corresponding to the calibration solutions for bandpass, complex gains, flux-scaled complex gains, and delays, respectively. 19. Run ./displayTimes.sh You should see output similar to the following, which shows this run took 24 minutes to complete, the longest of which was flagging for 8.5 minutes. logs/flag_round_1-1097927.casa 2019-02-28 01:32:37 2019-02-28 01:40:32 logs/setjy-1097928.casa 2019-02-28 01:41:02 2019-02-28 01:46:49 logs/xx_yy_solve-1097929.casa 2019-02-28 01:47:15 2019-02-28 01:50:22 logs/xx_yy_apply-1097930.casa 2019-02-28 01:50:47 2019-02-28 01:56:49 20. Run ./findErrors.sh logs/flag_round_1-1097927.out logs/setjy-1097928.out logs/xx_yy_solve-1097929.out logs/xx_yy_apply-1097930.out *** Error *** Error in data selection specification: MSSelectionNullSelection : The selected table has zero rows. (The same error repeated another 23 times) This error likely corresponds to empty sub-MS(s) with data completely flagged out, which give a worker node nothing to do for whichever CASA tasks are being called. 21. Rebuild your config file without verbose mode processMeerKAT.py -B -C tutorial_config.txt -M 1491550051.mms This way we reset the list of scripts in our config file, and set verbose=False and submit=False. We will manually remove the scripts we already ran in step [[23 #Run-the-pipeline-using-your-updated-config-file]], so leave the scripts parameter as is. 22. Edit your config file Edit tutorial_config.txt to update the reference antenna to what calc_refant.py found as the best reference antenna. If you’ve forgotten that was, view it in jobScripts/tutorial_config_*.txt (antenna 7). We don’t need to update badants as only flag_round_1.py uses this parameter, which we will not be running. 23. Run the pipeline using your updated config file processMeerKAT.py -R -C tutorial_config.txt Since we have set verbose=False and submit=False, the pipeline will not yet run, and you should see simplified output like the following: 2019-02-28 11:47:20,011 WARNING: Changing [slurm] section in your config will have no effect unless you [-R --run] again 2019-01-16 20:30:00,180 INFO: Master script &quot;submit_pipeline.sh&quot; written, but will not run. 24. Edit submit_pipeline.sh You will see in submit_pipeline.sh that each sbatch job is submitted on its own line, and that the job ID is extracted. Remove everything from #validate_input.sbatch to one line before #flag_round_2.sbatch, so it looks like the following #!/bin/bash cp tutorial_config.txt .config.tmp #flag_round_2.sbatch IDs=$(sbatch flag_round_2.sbatch | cut -d &#39; &#39; -f4) #setjy.sbatch IDs+=,$(sbatch -d afterok:$IDs --kill-on-invalid-dep=yes setjy.sbatch | cut -d &#39; &#39; -f4) #xy_yx_solve.sbatch IDs+=,$(sbatch -d afterok:$IDs --kill-on-invalid-dep=yes xy_yx_solve.sbatch | cut -d &#39; &#39; -f4) #xy_yx_apply.sbatch IDs+=,$(sbatch -d afterok:$IDs --kill-on-invalid-dep=yes xy_yx_apply.sbatch | cut -d &#39; &#39; -f4) #split.sbatch IDs+=,$(sbatch -d afterok:$IDs --kill-on-invalid-dep=yes split.sbatch | cut -d &#39; &#39; -f4) #quick_tclean.sbatch IDs+=,$(sbatch -d afterok:$IDs --kill-on-invalid-dep=yes quick_tclean.sbatch | cut -d &#39; &#39; -f4) #plot_solutions.sbatch IDs+=,$(sbatch -d afterok:$IDs --kill-on-invalid-dep=yes plot_solutions.sbatch | cut -d &#39; &#39; -f4) #Output message and create jobScripts directory echo Submitted sbatch jobs with following IDs: $IDs mkdir -p jobScripts . . . Note the first line has been edited to replace +=, with = and remove -d afterok:$IDs --kill-on-invalid-dep=yes, since it does not have any dependencies. 25. Run ./submit_pipeline.sh Again, we see simplified output Submitted sbatch jobs with following IDs: 1097948,1097949,1097950,1097951,1097952,1097953,1097954 Run killJobs.sh to kill all the jobs. Run summary.sh to view the progress. Run findErrors.sh to find errors (after pipeline has run). Run displayTimes.sh to display start and end timestamps (after pipeline has run). These job IDs comprise the new pipeline run we’ve just launched. So now ./summary.sh will display sacct for the new job IDs, similar to the following: JobID JobName Partition Elapsed NNodes NTasks NCPUS MaxDiskRead MaxDiskWrite NodeList TotalCPU State ExitCode - - -- -- - - -- 1097948 flag_round_2 Main 00:01:49 2 8 slwrk-[013,020] 00:00:00 RUNNING 0:0 1097948.0 orted 00:01:45 1 1 1 slwrk-020 00:00:00 RUNNING 0:0 1097949 setjy Main 00:00:00 2 8 None assigned 00:00:00 PENDING 0:0 1097950 xy_yx_solve Main 00:00:00 1 1 None assigned 00:00:00 PENDING 0:0 1097951 xy_yx_apply Main 00:00:00 2 8 None assigned 00:00:00 PENDING 0:0 1097952 split Main 00:00:00 2 8 None assigned 00:00:00 PENDING 0:0 1097953 quick_tclean Main 00:00:00 2 64 None assigned 00:00:00 PENDING 0:0 1097954 plot_solutions Main 00:00:00 1 1 None assigned 00:00:00 PENDING 0:0 The 4 new ancillary (bash) jobScripts will also correspond to these 7 new job IDs. After this pipeline run has completed, viewing the output of ./displayTimes.sh shows this run took XX minutes. If you want to see the output from the job scripts referring to the old pipeline runs, don’t worry, they’re still in the jobScripts directory with an older timestamp in the filename. Only the symlink in your working directory has been updated. These new tasks follow the second step of a two step calibration process that is summarised on this page. After split.py has run, you will see three new files 1491550051.0252-712.mms 1491550051.0408-65.mms 1491550051.DEEP_2_off.mms This corresponds to the data split out from 1491550051.mms, for the bandpass/flux calibrator (0408-65), the phase calibrator (0252-712), and the science target (DEEP_2_off). 26. View the images in the images directory quick_tclean.py creates quick-look images (i.e. with no selfcal, w-projection, threadholding, no-multiscale, etc) with robust weighting 0, for all fields specified in the config file, creating 512x512 images of the calibrator fields, and 2048x2048 images of the target field(s), both with 2 arcsec pixel sizes. For data with &gt; 100 MHz bandwidth, two taylor terms are used, otherwise the ‘clark’ deconvolver is used. You can view the images by running salloc to allocate a compute node, or connecting to a fat node (e.g. racetrack.idia.ac.za) and launching ds9 or CASA viewer, respectively with the syntax (replace /scratch/users/your_username/tutorial below):: singularity exec /data/exp_soft/containers/sourcefinding_py3.simg ds9 /scratch/users/your_username/tutorial/images/*fits singularity exec /data/exp_soft/pipelines/casameer-5.4.1.xvfb.simg casa --nologger --log2term -c &quot;viewer(infile=&#39;/scratch/users/your_username/tutorial/images/1491550051_DEEP_2_off.im.image.tt0/&#39;); raw_input()&quot; Here’s what your images of the flux calibrator (1934-638) and field (DEEP_2_off) should look like. Since we imaged a snapshot 16-dish MeerKAT observation using the old ROACH-2 correlator, with an on source time of ~20 minutes, we do not get very good image quality. Below is a more typical image produced by quick_tclean.py for a 64-dish observation using the SKARAB correlator, spanning ~8 hours, and only 5 MHz bandwidth. 27. View the figures in plots directory The last script that runs is plot_solutions.py, which calls CASA task plotms to plot the calibration solutions for the bandpass calibrator and the phase calibrator, as well as plots of the corrected data to eyeball for RFI. Below are a few selected plots. That’s it! You have completed the tutorial! Now go forth and do some phenomenal MeerKAT science! Also see Calibration in processMeerKAT Diagnosing Errors Using the pipeline",
    "url": "http://localhost:4000/docs/processMeerKAT/deep-2-tutorial/",
    "relUrl": "/docs/processMeerKAT/deep-2-tutorial/"
  },
  "10": {
    "id": "10",
    "title": "Home",
    "content": "IDIA Pipelines Welcome to the IDIA Pipelines Website. Here you’ll find help and documentation to help you do calibration and imaging of Radio Data Using the IDIA system. The IDIA Pipelines Team The team includes members from many of the MeerKAT Large Survey Projects. This list includes the active members who are working on the IDIA system, or the related software/algorithms: Jordan Collier Bradley Frank (Project Lead) Srikrishna Sekhar Russ Taylor Contact Email the project lead at bradley@idia.ac.za, or submit a ticket to our helpdesk: support@ilifu.ac.za.",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },
  "11": {
    "id": "11",
    "title": "processMeerKAT",
    "content": "The processMeerKAT software has been written to do calibration and imaging of MeerKAT interferometric data. Typical calibration and imaging of radio data comprises three steps: Initial or a priori calibration, which involves bootstrapping phase and flux gains from observations of reference calibrators. Self-calibration or a posteriori’ calibration, where the residual on-target errors are solved for by (iteratively) building a good representation of the field. 3rd Generation Calibration (3GC), aka post-selfcal, which deals with higher-order effects in the pursuit of high dynamic range imaging. This includes compensating for the primary beam and direction dependent effects. The processMeerKAT currently does full-polarisation a priori calibration on MeerKAT data, and includes automated flagging. processMeerKAT is written solely for the processing of data on IDIA’s SLURM cluster, but future revisions will allow you to run the software on any HPC platform. The main features of processMeerKAT are as follows: Is written in Python 2.7 (for CASA 5.4.X). Calibration algorithms only use CASA 5.4.X tasks and helper functions. ** Uses a purpose-built CASA Singularity container for parallel processing at IDIA, i.e, is fully thread-safe. Uses MPICASA to run parallel jobs over the cluster. Generates SBATCH files and ancillary helper scripts for processing. Please read the documentation to learn how to use the pipeline for your imaging data at IDIA.",
    "url": "http://localhost:4000/docs/processMeerKAT",
    "relUrl": "/docs/processMeerKAT"
  },
  "12": {
    "id": "12",
    "title": "Using the Pipeline",
    "content": "Usage The usage can be seen by running processMeerKAT.py -h which gives usage: /data/exp_soft/pipelines/master/processMeerKAT/processMeerKAT.py [-h] [-M path] [-C path] [-N num] [-t num] [-P num] [-m num] [-p name] [-T time] [-S script threadsafe container] [-w path] [-c path] [-n unique] [-l] [-s] [-v] (-B | -R | -V) Process MeerKAT data via CASA measurement set. Version: 1.0 optional arguments: -h, --help show this help message and exit -M path, --MS path Path to measurement set. -C path, --config path Path to config file. -N num, --nodes num Use this number of nodes [default: 8; max: 35]. -t num, --ntasks-per-node num Use this number of tasks (per node) [default: 4; max: 128]. -P num, --plane num Distribute tasks of this block size before moving onto next node [default: 2; max: ntasks-per-node]. -m num, --mem num Use this many GB of memory (per node) for threadsafe scripts [default: 236; max: 236]. -p name, --partition name SLURM partition to use [default: &#39;Main&#39;]. -T time, --time time Time limit to use for all jobs, in the form d-hh:mm:ss [default: &#39;12:00:00&#39;]. -S script threadsafe container, --scripts script threadsafe container Run pipeline with these scripts, in this order, using this container (3rd value - empty string to default to [-c --container]). Is it threadsafe (2nd value)? -w path, --mpi_wrapper path Use this mpi wrapper when calling threadsafe scripts [default: &#39;/data/exp_soft/pipelines/casa- prerelease-5.3.0-115.el7/bin/mpicasa&#39;]. -c path, --container path Use this container when calling scripts [default: &#39;/data/exp_soft/pipelines/casameer-5.4.1.xvfb.simg&#39;]. -n unique, --name unique Unique name to give this pipeline run (e.g. &#39;run1_&#39;), appended to the start of all job names. [default: &#39;&#39;]. -l, --local Build config file locally (i.e. without calling srun) [default: False]. -s, --submit Submit jobs immediately to SLURM queue [default: False]. -v, --verbose Verbose output? [default: False]. -B, --build Build config file using input MS. -R, --run Run pipeline with input config file. -V, --version Display the version of this pipeline and quit. Simple usage To get things working, source setup.sh, which will add to your $PATH and $PYTHONPATH (add this to your ~/.profile, for future use) source /data/exp_soft/pipelines/master/setup.sh To print the version of the pipeline, run processMeerKAT.py -V To build a config file, which the pipeline reads as input for how to process the data, run processMeerKAT.py -B -C myconfig.txt -M mydata.ms To run the pipeline, run processMeerKAT.py -R -C myconfig.txt This will create submit_pipeline.sh, which you can then run to submit all pipeline jobs to a SLURM queue: ./submit_pipeline.sh Display a summary of the submitted jobs ./summary.sh Kill the submitted jobs ./killJobs.sh If the pipeline crashes, or reports an error, find the error(s) by running (after the pipeline has run) ./findErrors.sh Once the pipeline has completed, display the start and end times of each job by running ./displayTimes.sh Detailed usage Build config file locally (e.g. on a fat node) using a custom SLURM configuration (nodes and tasks per node may be overwritten in your config file with something more appropriate by the end of the build step) processMeerKAT.py -l -B -C myconfig.txt -M mydata.ms -p Test02 -N 10 -t 8 -P 4 -m 100 -T 06:00:00 -n mydata_ Build config file using different MPI wrapper and container processMeerKAT.py -B -C myconfig.txt -M mydata.ms --mpi_wrapper /path/to/another/mpi/wrapper --container /path/to/another/container Build config file with different set of (python) scripts processMeerKAT.py -B -C myconfig.txt -S /absolute/path/to/my/script.py False /absolute/path/to/container.simg -S partition.py True &#39;&#39; -S relative/path/to/my/script.py True relative/path/to/container.simg -S flag_round_1.py True &#39;&#39; -S script_in_bash_PATH.py False container_in_bash_PATH.simg run_setjy.py True &#39;&#39; Run the pipeline immediately in verbose mode processMeerKAT.py -R -v -s -C myconfig.txt NOTE: All other command-line arguments passed into processMeerKAT.py when using option [-R --run] will have no effect, since the arguments are read from the config file at this point. Only options [-v --verbose] and [-C --config] will have any effect at this point. Config files The config file is where you set parameters affecting how you run the pipeline. The default config contains the following [crosscal] minbaselines = 4 # Minimum number of baselines to use while calibrating specavg = 1 # Number of channels to average after calibration (during split) timeavg = &#39;8s&#39; # Time interval to average after calibration (during split) spw = &#39;0:860~1700MHz&#39; # Spectral window / frequencies to extract for MMS calcrefant = True # Calculate reference antenna in program (overwrites &#39;refant&#39;) refant = &#39;m005&#39; # Reference antenna name / number standard = &#39;Perley-Butler 2010&#39; # Flux density standard for setjy badants = [] # List of bad antenna numbers (to flag) badfreqranges = [ &#39;944~947MHz&#39;, # List of bad frequency ranges (to flag) &#39;1160~1310MHz&#39;, &#39;1476~1611MHz&#39;, &#39;1670~1700MHz&#39;] [slurm] # See processMeerKAT.py -h for documentation nodes = 8 ntasks_per_node = 4 plane = 2 mem = 236 # Use this many GB of memory (per node) partition = &#39;Main&#39; # SLURM partition to use time = &#39;12:00:00&#39; submit = False container = &#39;/data/exp_soft/pipelines/casameer-5.4.1.xvfb.simg&#39; mpi_wrapper = &#39;/data/exp_soft/pipelines/casa-prerelease-5.3.0-115.el7/bin/mpicasa&#39; verbose = False scripts = [ (&#39;validate_input.py&#39;,False,&#39;&#39;), (&#39;partition.py&#39;,True,&#39;&#39;), (&#39;calc_refant.py&#39;,False,&#39;&#39;), (&#39;flag_round_1.py&#39;,True,&#39;&#39;), (&#39;setjy.py&#39;,True,&#39;&#39;), (&#39;xx_yy_solve.py&#39;,False,&#39;&#39;), (&#39;xx_yy_apply.py&#39;,True,&#39;&#39;), (&#39;flag_round_2.py&#39;,True,&#39;&#39;), (&#39;setjy.py&#39;,True,&#39;&#39;), (&#39;xy_yx_solve.py&#39;,False,&#39;&#39;), (&#39;xy_yx_apply.py&#39;,True,&#39;&#39;), (&#39;split.py&#39;,True,&#39;&#39;), (&#39;plot_solutions.py&#39;,False,&#39;&#39;), (&#39;quick_tclean.py&#39;,True,&#39;&#39;)] When the pipeline is run, the contents of your config file are copied to .config.tmp and each python script reads the parameters from this file as it is run. This way, the user cannot easily break the pipeline during the time it is running. This also means changing the [slurm] section in your config file will have no effect unless you once again run processMeerKAT.py -R. Selecting MS and fields IDs As previously stated, to build a config file, run processMeerKAT.py -B -C myconfig.txt -M mydata.ms This calls CASA and adds a [data] section to your config file, which points to your MS, and a [fields] section, which points to the field IDs you want to process as bandpass, total flux and phase calibrators, and science target(s). Only targets may have multiple fields separated by a comma, and all extra calibrator fields are appended as “targets”, to allow for solutions to be applied to them, and images to be made of them (see [[Release-Notes:-V1.0]]). The following is an example of what is appended to the bottom of your config file. [data] vis = &#39;/data/projects/deep/1497056411.ms&#39; [fields] bpassfield = &#39;0&#39; fluxfield = &#39;0&#39; phasecalfield = &#39;1&#39; targetfields = &#39;2&#39; You can edit your config file and change the field IDs. Inserting your own scripts Our design allows the user to insert their own scripts into the pipeline, along with or instead of our own scripts. All scripts are assumed to be written in python, with extension .py. They must either have hard-coded values for input such as the MS name, or be able to read the config file and extract the values (e.g. as in the main() function of most of our scripts). To insert your own scripts, either build a config file and edit the scripts argument to contain your list of scripts, or pass your scripts via command line during building your config file. For each script that is added, three arguments are needed The path to the script Whether the script is threadsafe (for MPI - i.e. it can use mpicasa) The path to the container with which to call the script - use ‘ ‘ for the default container The path to the scripts (and containers) can be an absolute path, a relative path, or in your bash path. If none of these exist, the script (or container) is assumed to be in the calibration scripts directory (/data/exp_soft/pipelines/master/processMeerKAT/cal_scripts/). Hence simply using partition.py will call the partition script in the calibration scripts directory. Adding scripts to config file Edit the scripts argument in your config file, which must be a list of lists/tuples. Adding scripts via command line Build a config file pointing to your scripts, each time appending the same three arguments (listed above): processMeerKAT.py -B -C myconfig.txt -S /absolute/path/to/my/script.py False /absolute/path/to/container.simg -S partition.py True &#39;&#39; -S relative/path/to/my/script.py True relative/path/to/container.simg -S flag_round_1.py True &#39;&#39; -S script_in_bash_PATH.py False container_in_bash_path.simg run_setjy.py True &#39;&#39; An error will be raised if any of the scripts or containers aren’t found.",
    "url": "http://localhost:4000/docs/processMeerKAT/using-the-pipeline/",
    "relUrl": "/docs/processMeerKAT/using-the-pipeline/"
  }
  
}
