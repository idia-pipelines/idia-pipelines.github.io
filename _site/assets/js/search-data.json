{
  "1": {
    "id": "1",
    "title": "Advanced Usage",
    "content": "Inserting your own scripts Our design allows the user to insert their own scripts into the pipeline, along with or instead of our own scripts. All scripts are assumed to be written in python, with extension .py. They must either have hard-coded values for input such as the MS name, or be able to read the config file and extract the values (e.g. as in the main() function of most of our scripts). To insert your own scripts, either build a config file and edit the scripts, precal_scripts or postcal_scripts parameters (see SPW splitting) argument to contain your list of scripts, or pass your scripts via command line during building your config file. For each script that is added, three arguments are needed The path to the script Whether the script is threadsafe (for MPI - i.e. it can use casampi) The path to the container with which to call the script - use &#39;&#39; for the default container The path to the scripts (and containers) can be an absolute path, a relative path, or in your bash path. If none of these exist, the script is assumed to be in one of the pipeline scripts directories (e.g. /idia/software/pipelines/master/processMeerKAT and below). Hence simply using partition.py will call the partition script in the cross-calibration scripts directory. Adding scripts to config file Edit the scripts, precal_scripts or postcal_scripts parameters (see SPW splitting) in your config file, which must be a list of lists/tuples. Adding scripts via command line Build a config file pointing to your scripts, each time appending the same three arguments (listed above): processMeerKAT.py -B -C myconfig.txt -S /absolute/path/to/my/script.py False /absolute/path/to/container.simg -S partition.py True &#39;&#39; -S relative/path/to/my/script.py True relative/path/to/container.simg -S flag_round_1.py True &#39;&#39; -S script_in_bash_PATH.py False container_in_bash_path.simg -S setjy.py True &#39;&#39; An error will be raised if any of the scripts or containers aren’t found. Similarly, the [-b --precal_scripts] and [-a --postcal_scripts] parameters can be used to add precal and postcal scripts (see SPW splitting) via the command line, with the same syntax. Editing pipeline scripts Users may wish to edit some of the pipeline scripts to do their own custom progressing by changing CASA task parameters not exposed via the config file. To do so, first copy the script (e.g. from /idia/software/pipelines/master/processMeerKAT/crosscal_scripts/) to your working directory, and then edit it from there. The pipeline will first look for a local copy of the pipeline scripts (in your working directory and the one above) and run this instead of the normal version of the script. Restarting or Resuming the Pipeline The pipeline can be killed (via ./killJobs.sh) and re-run from the start at any point. However, this is not always necessary. The pipeline is generally not designed to run twice within the same directory, since CASA will complain the files already exist. However, the pipeline can be resumed by skipping certain steps, achieved by editing the scripts parameter in your config file and removing the steps that have already been run. As the hidden config (.config.tmp) is updated during your pipeline run, you may first wish to check the differences (e.g. with diff myconfig.txt .config.tmp) and update your config accordingly. For example, the vis or reference antennas (and bad antennas) may have been updated. If you remove the partition step, and you have nspw &gt; 1, the pipeline will attempt to update the config files in your SPW directories by updating the vis to the partitioned (M)MS. Particular care should also be taken in cases where a job crashed during writing to a column of your (M)MS (e.g. during flagging, applying, or setjy), to ensure the data are not corrupted. Checking the logs should reveal which step was being run, and if the job crashed while writing to the column, it is best to remove that (M)MS and re-run the relevant steps (or if necessary, wipe everything in your working directory and restart the pipeline). Similar care must be taken during steps where a new product was being written (e.g. partition, split, concat, or any of the imaging jobs). In such cases, it is best to remove the incomplete product(s) and re-run that step (and the subsequent ones). For example, if an imaging step crashed, remove the image products, patch any bugs that may have caused the crash (by correcting your config, or editing the script for that step), and re-run that step (and subsequent ones). Or if a split step crashed, remove the incomplete (M)MS, patch any bugs, and re-run. There are a few steps within the pipeline that only need to be run once for a given dataset. For datasets that have already been through the default pipeline, the solutions from xx_yy_solve (or xy_yx_solve) have been written to the caltables directory (renamed to caltables_round1 if a 2nd round is written, which is then written to caltables), which will automatically be applied during running xx_yy_apply (or xy_yx_apply). Simlarly, if calcrefant=True and the calc_refant.py script was included, the reference antenna and list of bad antennas would have been calculated, and can be found in ant_stats.txt (and logs/calc_refant-*.err). These can be written in your new config file (e.g. check .config.tmp), where you can now set calcrefant=False. Other than editing the scripts parameter, users can run any selected parts of the pipeline by passing scripts in via the command-line arguments (see above). Users can also manually submit sbatch jobs corresponding to certain steps within the pipeline, although this will not be captured by the pipeline infrastructure, such as the bash jobScripts: summary.sh, killJobs.sh, etc. Spectral line pre-processing One advanced use case is to process continuum data at lower spectral resolution (e.g. 1k / 1024 channels), and apply that calibration to higher spectral-resolution data (e.g. 32k / 32,768 channels). There are several ways to do this, and an optimal method is to partition and fully calibrate a 1k copy of your data, and then partition a separate 32k copy. You can then use your fresh 32k copy to re-run bandpass calibration, apply your (cross-calibration and self-calibration) solutions, predict to the model column (based on your 1k images), and flag on the RESIDUAL column. This is achievable by “tricking” the pipeline into doing so by renaming or moving your previous (M)MSs, and partitioning out a fresh 32k copy with the same default names (and then replacing your bandpass caltable with a 32k bandpass solution). However, there are a few quirky steps involved in doing this, so please get in touch with us at ilifu Support to ask about doing this. Once your 32k data are calibrated, and have valid model and corrected data colums, a simple uvsub call can be made to subtract the continuum model from your data, and then a uvcontsub call with your preferred parameters can be run in parallel mode with casampi.",
    "url": "http://localhost:4000/docs/processMeerKAT/Advanced-Usage/",
    "relUrl": "/docs/processMeerKAT/Advanced-Usage/"
  },
  "2": {
    "id": "2",
    "title": "Diagnosing Errors",
    "content": "Diagnosing Errors On this page is a list of potential issues that could arise while running the pipeline, which are typically associated with runtime errors, that could be due to issues with the cluster (e.g., compute-node failure during running your job) or job parameterisation (e.g. underestimating RAM required). This is not an exhaustive list, and is intended to give the user a sense of which problems may need to be reported to support@ilifu.ac.za and which problems can be ignored. Unable to launch jobs in SLURM If the status of your job is (launch failed requeued held), please file a ticket with support@ilifu.ac.za. Node Failure You may encounter the rare error of a node failure, which shows an error message similar to the following within your logs: JOB 14024 ON compute-010 CANCELLED AT 2020-08-10T03:55:21 DUE TO NODE FAILURE, SEE SLURMCTLD LOG FOR DETAILS *** If you encounter this error, please file a ticket with support@ilifu.ac.za. Memory error If you see the phrase oom (Out of Memory) or MemoryError in the .err logs (this can be located by grep -i oom logs/*.err), this is typically indicative that CASA did not have enough memory to complete the task. This often happens while running flagdata and does not always halt execution of the pipeline. If you are not using the maximum amount of memory per node, increase your allocation (up to 232 GB on an ilifu node from the Main partition, or 480 GB on an ilifu node from the HighMem partition). If you are using the maximum amount of memory per node, reduce the number of tasks per node and consider increasing the nodes in the config file (e.g. halve tasks and double nodes) before re-launching the pipeline, as that will allocate more memory per task. summary.sh will show a State of OUT_OF_ME+ for jobs that have run out of memory, and the following error should appear in the .err logs (e.g. for jobID 1234567): slurmstepd: error: Detected 1 oom-kill event(s) in StepId=1234567.0 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. srun: error: compute-070: task 0: Out Of Memory srun: launch/slurm: _step_signal: Terminating StepId=1234567.0 slurmstepd: error: Detected 1 oom-kill event(s) in StepId=1234567.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler. There are cases where a failure in flagdata can leave the MS in an intermediate state that causes the subsequent calibration tasks to fail. If the pipeline run did not cancel, we recommend killing any currently running jobs (by running ./killJobs.sh from the parent directory), wiping the *MHz subdirectories, and re-running processMeerKAT.py -R [-C &lt;config_file&gt;] and ./submit_pipeline.sh again after making the above changes to the config file. Alternatively, you could attempt to resume the pipeline. False positives Server timeout errors Errors of the form MPIMonitorClient::get_server_timeout::MPIMonitorClient::get_server_timeout::@slwrk-155:MPIClient Found 1 servers in timeout status are benign and do not have an impact on the pipeline performance. The MPI daemon simply times out waiting for a worker node to respond, and prints out this error to the logs. SLURM is able to handle these timeouts gracefully and restarts the process once it times out. Empty rows in sub-MSs *** Error *** Error in data selection specification: MSSelectionNullSelection : The selected table has zero rows. Some tasks might complain that no valid data were found in a sub-MS, due to some combination of data selection parameters resulting in a null selection for a specific sub-MS. Generally this seems to be a “harmless” error, and doesn’t seem to affect the progress of the calibration/pipeline. “No valid SPW and Chan combination found” Errors of the form agentflagger::::MPIServer-31 (file ../../tools/flagging/agentflagger_cmpt.cc, line 35) Exception Reported: No valid SPW &amp; Chan combination found often show up in the logs of either flag_round_1 or flag_round_2 or both. Similar to the applycal error, this is caused by a combination of data selection parameters leading to a null selection for this particular subMS. it simply means that the flagging range requested lies outside the frequency range in the target MS/subMS. UTC offset by more than 1s Errors of the form Leap second table TAI_UTC seems out-of-date. Until the table is updated (see the CASA documentation or your system admin), times and coordinates derived from UTC could be wrong by 1s or more. are fairly common and are completely benign. These errors are auto-generated by CASA when the internal data repository has not been updated for some pre-defined length of time. While we try to keep our containers up to date, these errors can still occur and have no impact on the image quality or fidelity. Generic applycal error Sometimes the findErrors.sh script will report generic errors in the applycal task that are something like SEVERE applycal::::@slwrk-128::MPIServer-7 An error occurred running task applycal There are two ways to determine if this is a “false positive”. Locate the script in question inside the logs directory of the relevant SPW directory and inspect the .casa log. There should be several successful applycal instances. There should also be logs that read *** Error *** Error in data selection specification: MSSelectionNullSelection : The selected table has zero rows. in either the corresponding .out or .err file. These errors basically arise because a combination of chan/SPW/field/time selection has resulted in a null selection for one subMS inside the MMS causing applycal to fail for that one subMS. These errors do not have any impact on the final image quality.",
    "url": "http://localhost:4000/docs/processMeerKAT/Diagnosing-Errors/",
    "relUrl": "/docs/processMeerKAT/Diagnosing-Errors/"
  },
  "3": {
    "id": "3",
    "title": "Example Use Cases",
    "content": "Calibration Our algorithmic approach toward calibration in the pipeline can be found here. Stokes I Calibration (Continuum) Stokes I calibration is achieved in xx_yy_solve.py, and includes standard delay, bandpass and gain calibration. Within this pipeline, this is done to obtain better statistics for a second round of flagging. Stokes I calibration is the default mode of the pipeline, using the xx_yy_solve.py and xx_yy_apply.py scripts, instead of the xy_yx_solve.py and xy_yx_apply.py scripts. Since the 2nd round of flagging is different to the 1st, and much more effective, these two scripts are called twice, in a second round that should not be skipped. Furthermore, for good calibration, new solutions should always be derived after flagging. Short-Track Observations and Fluxscale Issues Replacing the ‘xy_yx’ scripts with the ‘xx_yy’ scripts is recommended for short-track observations (e.g. 2 hours), because the ‘xy_yx’ scripts assume that the phase calibrator has sufficient parallactic angle coverage to solve for the leakage and Stokes Q &amp; U. We also recommend this for cases where there are issues with fluxscale, which often occurs when the ‘xy_yx’ scripts are run for short-track observations. New in v1.1: Only xx_yy scripts are used by default, and if the parallactic angle coverage is &lt; 30 degrees, a warning is output recommending disabling polarisation calibration. Full Stokes Calibration (Polarisation) Full Stokes calibration can be activated with [-P --dopol], which, in addition to the calibration listed above, includes calibration of instrumental leakage, X-Y phase, source polarisation, and Stokes Q and U from gain variations, using the CASA helper task GainFromQU. This is achieved by using the phase calibrator to solve as a function of parallactic angle. Currently the pipeline does not solve for absolute polarisation angle, since the CALIBRATE_POLARIZATION intent is missing from MeerKAT datasets. Additionally, CASA currently has no functionality to solve for wide-band leakage and QU, but assumes a constant value across single spectral windows. Both of these issues are addressed in v1.1. Full-Stokes calibration is not recommended for short-track observations (see section above). Calibrating a Small Sub-Band (Spectral Line) A small sub-band of frequencies can be selected and calibrated by specifying a range of frequencies with argument spw in your config file. e.g. spw = &#39;0:1350~1450MHz&#39; means partition will extract only frequencies between 1350-1450 MHz for calibration. For very small bandwidth of several MHz, it may not be necessary to use MMS and MPI (see section MS only below). This mode is useful for calibration of spectral line data. Data Format Default: MS -&gt; MMS By default, the pipeline will convert the input measurement set (MS) into a multi-measurement set (MMS), partitioning the data by scan, such that the number of sub-MSs making up the MMS is equal to the number of scans. The input MS does not need to be copied to your working directory, since partition.py needs only to read the input data (e.g. from /idia/projects/). At the end of the pipeline, split.py will split each of the field IDs specified in your config file into MMS format. This is the default behaviour, since the default value of keepmms in the config file is True. This also ensures future tasks make use of MPI (and multiple CPUs), so that your imaging runs more quickly. This mode is encouraged for all users, as it will run the pipeline as efficiently as possible without a change in calibration strategy. However, users wishing to perform post-processing using other software packages may be required to write back into MS format. MS -&gt; MMS -&gt; MS When setting keepmms=False in your config file, the pipeline will convert your data to MMS as usual, but during running split, each of the field IDs specified in your config file will be written in MS format. Although any subsequest calibration tasks (e.g. applycal and flagdata run during selfcal) will not be able to utilise multiple tasks / CPUs, imaging tasks performed with tclean will still be able to utilise multiple tasks / CPUs, resulting in only a small difference in performance. This general mode of processing is encouraged when the user wishes to do their own imaging that requires an MS. MS only (single thread processing) The pipeline can be run on a MS, where the user would have to request 1 node and 1 task per node. In this case, the user would set createmms=False in their config. This mode isn’t generally encouraged, but may be useful for small datasets (tens of GB - e.g. small bandwidth of several MHz, or short-track observations), when using MMS has little to no advantage, or when the number of scans is very large (hundreds), in which case splitting into multiple SPWs and multiple sub-MSs is too extreme (e.g. running across tens of nodes). Field IDs Default: Primary and Secondary Calibrator A standard observation will have a primary calibrator (e.g. J1939-6342 or J0408-6545), which is both the bandpass and total flux calibrator, and a secondary calibrator, which is the phase calibrator (and is also used for polarisation calibration). Single calibrator and target In rare cases, a primary calibrator may be close enough to the target(s) that only one calibrator is used throughout the observation. This use case is supported, but caution must be used to ensure the correct flux scale is derived. Multiple calibrator field IDs Multiple field IDs can be specified by writing a comma-separated list to your config file. However, this use case is not supported, since tasks such as setjy cannot handle this. Only the targetfields and extrafields parameters from the [fields] section of your config file allow for multiple comma-separated fields within the pipeline.",
    "url": "http://localhost:4000/docs/processMeerKAT/Example-Use-Cases/",
    "relUrl": "/docs/processMeerKAT/Example-Use-Cases/"
  },
  "4": {
    "id": "4",
    "title": "LICENSE",
    "content": "GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt; Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The GNU General Public License is a free, copyleft license for software and other kinds of works. The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things. To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others. For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it. For the developers&#39; and authors&#39; protection, the GPL clearly explains that there is no warranty for this free software. For both users&#39; and authors&#39; sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions. Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users&#39; freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users. Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS 0. Definitions. &quot;This License&quot; refers to version 3 of the GNU General Public License. &quot;Copyright&quot; also means copyright-like laws that apply to other kinds of works, such as semiconductor masks. &quot;The Program&quot; refers to any copyrightable work licensed under this License. Each licensee is addressed as &quot;you&quot;. &quot;Licensees&quot; and &quot;recipients&quot; may be individuals or organizations. To &quot;modify&quot; a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a &quot;modified version&quot; of the earlier work or a work &quot;based on&quot; the earlier work. A &quot;covered work&quot; means either the unmodified Program or a work based on the Program. To &quot;propagate&quot; a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well. To &quot;convey&quot; a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying. An interactive user interface displays &quot;Appropriate Legal Notices&quot; to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion. 1. Source Code. The &quot;source code&quot; for a work means the preferred form of the work for making modifications to it. &quot;Object code&quot; means any non-source form of a work. A &quot;Standard Interface&quot; means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language. The &quot;System Libraries&quot; of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A &quot;Major Component&quot;, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it. The &quot;Corresponding Source&quot; for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work&#39;s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work. The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source. The Corresponding Source for a work in source code form is that same work. 2. Basic Permissions. All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law. You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you. Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary. 3. Protecting Users&#39; Legal Rights From Anti-Circumvention Law. No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures. When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work&#39;s users, your or third parties&#39; legal rights to forbid circumvention of technological measures. 4. Conveying Verbatim Copies. You may convey verbatim copies of the Program&#39;s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program. You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee. 5. Conveying Modified Source Versions. You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions: a) The work must carry prominent notices stating that you modified it, and giving a relevant date. b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to &quot;keep intact all notices&quot;. c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it. d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so. A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an &quot;aggregate&quot; if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation&#39;s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate. 6. Conveying Non-Source Forms. You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways: a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange. b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge. c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b. d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements. e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d. A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work. A &quot;User Product&quot; is either (1) a &quot;consumer product&quot;, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, &quot;normally used&quot; refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product. &quot;Installation Information&quot; for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made. If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM). The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network. Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying. 7. Additional Terms. &quot;Additional permissions&quot; are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions. When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission. Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms: a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or d) Limiting the use for publicity purposes of names of licensors or authors of the material; or e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors. All other non-permissive additional terms are considered &quot;further restrictions&quot; within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying. If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms. Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way. 8. Termination. You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11). However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10. 9. Acceptance Not Required for Having Copies. You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so. 10. Automatic Licensing of Downstream Recipients. Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License. An &quot;entity transaction&quot; is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party&#39;s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts. You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it. 11. Patents. A &quot;contributor&quot; is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor&#39;s &quot;contributor version&quot;. A contributor&#39;s &quot;essential patent claims&quot; are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, &quot;control&quot; includes the right to grant patent sublicenses in a manner consistent with the requirements of this License. Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor&#39;s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version. In the following three paragraphs, a &quot;patent license&quot; is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To &quot;grant&quot; such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party. If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. &quot;Knowingly relying&quot; means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient&#39;s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid. If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it. A patent license is &quot;discriminatory&quot; if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007. Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law. 12. No Surrender of Others&#39; Freedom. If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program. 13. Use with the GNU Affero General Public License. Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such. 14. Revised Versions of this License. The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License &quot;or any later version&quot; applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation. If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy&#39;s public statement of acceptance of a version permanently authorizes you to choose that version for the Program. Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version. 15. Disclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM &quot;AS IS&quot; WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. 16. Limitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 17. Interpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Programs If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the &quot;copyright&quot; line and a pointer to where the full notice is found. {one line to give the program&#39;s name and a brief idea of what it does.} Copyright (C) {year} {name of author} This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;. Also add information on how to contact you by electronic and paper mail. If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode: {project} Copyright (C) {year} {fullname} This program comes with ABSOLUTELY NO WARRANTY; for details type `show w&#39;. This is free software, and you are welcome to redistribute it under certain conditions; type `show c&#39; for details. The hypothetical commands `show w&#39; and `show c&#39; should show the appropriate parts of the General Public License. Of course, your program&#39;s commands might be different; for a GUI interface, you would use an &quot;about box&quot;. You should also get your employer (if you work as a programmer) or school, if any, to sign a &quot;copyright disclaimer&quot; for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see &lt;http://www.gnu.org/licenses/&gt;. The GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read &lt;http://www.gnu.org/philosophy/why-not-lgpl.html&gt;.",
    "url": "http://localhost:4000/docs/processMeerKAT/LICENSE/",
    "relUrl": "/docs/processMeerKAT/LICENSE/"
  },
  "5": {
    "id": "5",
    "title": "Quick Start",
    "content": "Quick Start Note: It is not necessary to copy the raw data (i.e. the MS) to your working directory. The first step of the pipeline does this for you by creating an MMS or MS, and does not attempt to manipulate the raw data (e.g. stored in /idia/projects - see data format). 1. In order to use the processMeerKAT.py script, source the setup.sh file: source /idia/software/pipelines/master/setup.sh which will add the correct paths to your $PATH and $PYTHONPATH in order to correctly use the pipeline. We recommend you add this to your ~/.profile, for future use. 2. Build a config file: a. For continuum/spectral line processing : processMeerKAT.py -B -C myconfig.txt -M mydata.ms b. For polarization processing : processMeerKAT.py -B -C myconfig.txt -M mydata.ms -P c. Including self-calibration : processMeerKAT.py -B -C myconfig.txt -M mydata.ms -2 d. Including science imaging : processMeerKAT.py -B -C myconfig.txt -M mydata.ms -I This defines several variables that are read by the pipeline while calibrating the data, as well as requesting resources on the cluster. The config file parameters are described by in-line comments in the config file itself wherever possible. The [-P --dopol] option can be used in conjunction with the [-2 --do2GC] and [-I --science_image] options to enable polarization calibration as well as self-calibration and science imaging. 3. To run the pipeline: processMeerKAT.py -R -C myconfig.txt This will create submit_pipeline.sh, which you can then run with ./submit_pipeline.sh to submit all pipeline jobs to the SLURM queue. Other convenient scripts are also created that allow you to monitor and (if necessary) kill the jobs. summary.sh provides a brief overview of the status of the jobs in the pipeline, findErrors.sh checks the log files for commonly reported errors (after the jobs have run), and killJobs.sh kills all the jobs from the current run of the pipeline, ignoring any other (unrelated) jobs you might have running. For help, run processMeerKAT.py -h, which provides a brief description of all the command line options. Using multiple spectral windows (new in v1.1) Starting with v1.1 of the processMeerKAT pipeline, the default behaviour is to split up the MeerKAT band into several spectral windows (SPWs), and process each concurrently. This results in a few major usability changes as outlined below: Calibration output : Since the calibration is performed independently per SPW, all the output specific to that SPW is within its own directory. Output such as the calibration tables, logs, plots etc. per SPW can be found within each SPW directory. Logs in the top level directory : Logs in the top level directory (i.e., the directory where the pipeline was launched) correspond to the scripts in the precal_scripts and postcal_scripts variables in the config file. These scripts are run from the top level before and after calibration respectively. By default these correspond to the scripts to calculate the reference antenna (if enabled), partition the data into SPWs, and concat the individual SPWs back into a single MS/MMS. More detailed information about SPW splitting is found here.",
    "url": "http://localhost:4000/docs/processMeerKAT/Quick-Start/",
    "relUrl": "/docs/processMeerKAT/Quick-Start/"
  },
  "6": {
    "id": "6",
    "title": "Release Notes",
    "content": "Version 2.0 This is the third release of the IDIA Pipelines processMeerKAT package, intended for use on the ilifu SLURM cluster. The software uses a parallelised implementation of CASA 6 to calibrate and image interferometric (imaging) data from the MeerKAT telescope. The current release adds the following functionality: Self-calibration and science imaging: this allows for configuration of multiple self-calibration loops, with customisable parameters per loop, as well as an additional final imaging stage to generate science-ready images, which includes primary beam correction using katbeam Support for outlier fields (experimental): it is now possible to specify an outlier threshold to identify and image bright sources outside the main field of view, which improves the run-time of imaging, and can improve image fidelity in some cases Bugfixes and improvements to polarisation calibration Support for loading modules on the ilifu SLURM cluster Uses CASA 6.X, Python 3.8, OpenMPI 4.0.3, and Singularity 3.9.1 Known Issues (minor) Beyond the unresolved issues listed below, there are two minor CASA issues associated with outlier imaging, namely that Stokes=&#39;IQUV&#39; cannot be specified during outlier imaging (but only Stokes=&#39;I&#39;), and there is an observed loss in resolution whenever outlier imaging is invoked. For more information, see here. Version 1.1 This is the second release of the IDIA Pipelines processsMeerKAT package, to be used on the ilifu SLURM cluster. The software uses a parallelised implementation of CASA to calibrate interferometric (imaging) data from the MeerKAT telescope. The current release adds the following functionality: Spectral Window (SPW) splitting (see docs here), where each separate SPW is processed independently and concurrently, providing a speed-up for large (TB) datasets, better polarisation calibration, and better flux scaling Quick-look continuum cube, across all SPWs Pre-processing during initial partition of MS, including pre-averaging of frequency channels, removal of cross-hand correlations up front for Stokes I processing, and removal of autocorrelations If running in full Stokes mode, setjy now includes the polarisation models for 3C286 and 3C138 if they are present in the data. Improved default parameters, including a smaller RFI mask that removes the persistent RFI in the ranges 933~960, 1163~1299, and 1524~1630 MHz Improved interaction with SLURM, including exclude, dependencies, account and reservation parameters, and graceful termination of pipeline after errors Uses CASA 5.6.2, Python 2.7, and Singularity 3.5.2. Known Issues Moderate: Discontinuities in the Stokes Q and U spectra: In the event that full Stokes calibration is requested (by passing the --dopol parameter during the build stage) we have noticed that the Stokes Q and U spectra of the calibrated data show discontinuities between the spectral windows (i.e. when nspw &gt; 1 in your config). While the overall shape of the Q and U spectra seem to be right, the discontinuities will affect the inferences made during rotation measure synthesis. We are in the process of debugging this and will issue a patch once we have fixed it. Minor: Resource allocation: There is an upper limit to the number of CPUs that can be utilised by MPICASA, which is given by the number of scans, plus the master MPIClient (nscans + 1). Relating this to the processing footprint is complex, since each task uses a different selection of those scans. We estimate the optimal number of MPI tasks as int(1.1*(nscans/2 + 1)), where nscans is the number of scans, typically 30-40. The motivation for this is that at most, if you have only target and phase calibrator, nscans / 2 will be selected by a task. So the master MPI client is added (+1) to this, and then 10%, in case some nodes time out. For tasks reading only sub-MSs corresponding to other calibrators (e.g. bandpass), some CPUs will not be used Similarly, we use a single memory value for all threadsafe tasks. Discontinuities in the phase solutions: We have noticed a discontinuity in the phase of the bandpass solutions between spectral windows (i.e. when nspw &gt; 1 in your config). However, this does not seem to have a significant effect on the calibration, as the spectrum of sources within the target field matches between data calibrated with nspw &gt; 1, and data calibrated with nspw=1. Slow plotting: Generating plots of the calibrated visibilities is very time consuming, often running to a few hours. However, as this is the last step of the pipeline, the calibrated, split measurement sets and images should be ready for further analysis while the plots are being generated. The speed of plotting is limited by how quickly plotms can generate the plots. Field IDs: The pipeline does not currently support specifying multiple fields for anything other than the targets. Version 1.0 This is the first release of the IDIA Pipelines processsMeerKAT package, to be used on the ilifu SLURM cluster. The software uses a parallelized implementation of CASA to calibrate interferometric (imaging) data from the MeerKAT telescope. The version 1.0 release includes the following functionality: The processMeerKAT.py script builds a config based on an input measurement set (MS). The pipeline currently only does cross-calibration, or a’priori (1GC) calibration. This includes parallelised flagging using FLAGDATA (tfcrop and rflag). Flux bootstrapping, gain and bandpass calibration. Full Stokes calibration. Quick-look imaging (i.e. without selfcal, w-projection, etc) of the calibrators and science targets. Diagnostic plots of the calibration tables and corrected data. Uses CASA 5.4.1, Python 2.7, and Singularity 2.6.1. Please consult the documentation on GitHub for more information. Three talks about version 1.0 of the pipeline, presented from the IDIA pipelines team during the 2019 South African MIGHTEE Early Science Workshop, can be found here: 1, 2, 3. Known Issues Moderate: Flux scale (not seen in V1.1): Although the fluxes of the calibrated targets and calibrator sources are typically accurate to within a few percent, we find that there are certain datasets that result in a flux scale that is down by a factor of a few, particularly for short-track (e.g. 2 hour) observations (see example use case). We are in the process of tracking down the root cause of these issues, and expect to issue a fix soon. However, if you do happen to notice that the fluxes of one or more of the sources/targets are off (either higher/lower), please report it by creating a Github issue. Broadband polarisation (resolved in V1.1): CASA does not natively support solving broad-band polarisations, i.e., it is not sensitive to rotation measure (RM). The assumption is that the RM within a single spectral window (SPW) is constant, however MeerKAT has only a single SPW that spans the entire bandwidth. We have identified future workarounds (which is to split up the band into several SPWs), however presently the broadband polarisation models do contain systematic errors. Calculation of antenna statistics (resolved in V1.1): The amplitude and RMS per antenna computed in calc_refant.py does not match what is found by CASA task visstat, and decreases as a function of antenna number. We expect to issue a fix soon. For now, we recommend users have calcrefant=False in their config files, which also disables antenna flagging (i.e. entirely flagging out bad antennas). Resource allocation: We set the number of threads to half the number of scans + 1 (master) + 10%, so that for tasks reading the target or phasecal (since we partition by scans - i.e. the number of sub-MSs = the number of scans), the number of threads is approximately the number of sub-MSs being read. For tasks reading only sub-MSs corresponding to other calibrators (e.g. bandpass), many threads will not be used Similarly, we use a single memory value for all threadsafe tasks, and hardcode 100 GB for single thread tasks Minor: Empty rows in sub-MSs: Some tasks might complain that no valid data were found in a sub-MS, but generally this seems to be a “harmless” error, and doesn’t seem to affect the progress of the calibration/pipeline. Exit codes (resolved in V1.1) Some jobs fail in the queue that shouldn’t have, and others don’t fail when they should Generally the pipeline continues even when dependencies legitimately fail Slow plotting: Generating plots of the calibrated visibilities is very time consuming, often running to a few hours. However, as this is the last step of the pipeline, the calibrated, split measurement sets and images should be ready for further analysis while the plots are being generated. The speed of plotting is limited by how quickly plotms can generate the plots. Field IDs: The pipeline does not currently support specifying multiple fields for anything other than the targets.",
    "url": "http://localhost:4000/docs/processMeerKAT/Release-Notes/",
    "relUrl": "/docs/processMeerKAT/Release-Notes/"
  },
  "7": {
    "id": "7",
    "title": "SLURM and MPICASA",
    "content": "Parallel CASA Using SLURM at IDIA Note: For details on how to setup SLURM batch and interactive jobs on the ilifu system, please look at the ilifu documentation. This page deals with the specifics of using CASA in conjuction with the SLURM environment on the ilifu computing system. SLURM is a resource and job management system that is available on many clusters. Jobs/tasks are typically submitted to the job management system, and are inserted into a job queue; the job is executed when the requested resources become available. SLURM is the job management and scheduling software used on the ilifu cluster. While SLURM Clusters provide the option to request and reserve resources to work in an interactive mode, its preferred method is to submit jobs to the queue to be run in a non-interactive way. To run a CASA script in a non-interactive way in the SLURM cluster, you would use the following steps. Write your CASA script. Write an associated SBATCH script for your job. Submit the script (i.e., your job) to the queue using sbatch. Write your CASA Script CASA scripts are written in Python. An entire pipeline can be written in such a script, that includes flagging, initial calibration and imaging. It is important to note that different CASA tasks use different schemes for parallelism, when writing your script. For example, flagdata parallelises by scan and is thus RAM intensive; tclean splits up the input data to occupy as many processes as are specified, and is this CPU &amp; RAM intensive. Therefore, a single script that includes flagging and imaging could have sub-optimal usage of a cluster resources for some tasks, and optimal usage for others. Keep this in mind when writing your script. Here’s an example of a python script that calls tclean in CASA 6: import os,sys import casampi from casatasks import tclean,casalog logfile=casalog.logfile() casalog.setlogfile(&#39;logs/{SLURM_JOB_NAME}-{SLURM_JOB_ID}.casa&#39;.format(**os.environ)) vis=sys.argv[-1] imagename=vis + &#39;.image&#39; tclean(vis=vis, imagename=imagename, imsize=[6144,6144], cell=&#39;1.5arcsec&#39;, gridder=&#39;wproject&#39;, wprojplanes=512, deconvolver=&#39;mfmfs&#39;, weighting=&#39;briggs&#39;,robust=-0.5, niter=50000, threshold=10e-6, nterms=2, pblimit=-1, parallel = True) if os.path.exists(logfile): os.rename(logfile,&#39;logs/{SLURM_JOB_NAME}-{SLURM_JOB_ID}.mpi&#39;.format(**os.environ)) Write your SBATCH Script An SBATCH script is a bash script that wraps the relevant SLURM parameters needed for your script. Consult the following website for more details on how to use SBATCH: https://slurm.schedmd.com/sbatch.html Here’s an example of an SBATCH script that submits a tclean job for CASA 6: #!/bin/bash #SBATCH --account=b05-pipelines-ag #SBATCH --nodes=1 #SBATCH --ntasks-per-node=32 #SBATCH --cpus-per-task=1 #SBATCH --mem=232GB #SBATCH --job-name=tclean #SBATCH --output=logs/%x-%j.out #SBATCH --error=logs/%x-%j.err #SBATCH --partition=Main #SBATCH --time=03:00:00 #Run the application: module load openmpi/2.1.1 mpirun singularity exec /path/to/casa/container.simg python tclean.py /path/to/data.ms More details can be found in the ilifu documentation about using Singularity and parallel processing. Some other helpful resources include the resource allocation guide and the ilifu training. There are a few important SBATCH parameters to define: --nodes or -N specifies the node count, i.e., the nodes requested for the job. --tasks-per-node specifies the number of parallel tasks to execute on each node. --distribution or -m specifies the mode in which the tasks are distributed to each node. This parameter is useful for scripts that include flagging. Since flagging is parallelised by scan, the first node(s) could run out of RAM for a particular flagging job. This would lead to SLURM killing the offending task(s), hence killing the main job. There are two distribution modes that can be used to solve this problem. plane=X distributes X jobs at a time, in a round-robin fashion across nodes. The cyclic mode distributes single tasks at a time in a round-robin fashion across nodes. plane=X or cyclic modes are useful for jobs that are RAM limited, i.e., when you need to use the aggregated RAM that’s in the total pool requested. More about SLURM SLURM is a workload manager that will distribute jobs across a specified cluster environment. It understands how to control message passing interface (MPI) jobs across multiple tasks (or nodes) via mpirun and for CASA 5, mpicasa. In principle this means that SLURM should be able to schedule and manage a CASA job that is running across a cluster. Following is a “practical” definition of some SLURM keywords that should help clarify how to best to allocate resources. task : A “task” by SLURM’s definition is what one would usually call a “process” on a regular computer. Similar to a process, a task has its own memory allocation that it does not share with other tasks. Each task is then operated on independently via MPI. This also means a more fine-grained parallelism can be employed per task, by using multiple threads (e.g. via OpenMP) to work on a single task. –cpus-per-task : Defines the number of CPUs to dedicate to a single task. If each task can take advantage of multiple threads, setting this value to more than one can speed things up further (e.g., tclean in CASA is parallelised across OpenMP and MPI) –mem-per-cpu : The RAM dedicated to each CPU in the node. Currently, the ilifu cluster is set to 3096 MB per CPU. If a job is running out of memory, setting this to a larger value can help. Alternatively, as mentioned above, if the task can take advantage of more threads, it may be preferable to set --cpus-per-task instead. Notes on CASA Tasks and Parallelism Running CASA through SLURM requires calling CASA via mpirun, or for CASA 5, mpicasa. CASA understands how to use mpi on tasks that are optimised for mpi (such as flagdata, tclean, setjy, and applycal) while operating as per usual on tasks that are not mpi aware (like gaincal). Ideally, the only change to an existing script would be to add a call to partition (or mstransform(createmms=True)) at the top. Below are some notes on tasks. partition (basic): In order to run tasks (except tclean) across a cluster, the partition (or mstransform) task needs to be called prior to running any other tasks. partition creates a multi-measurement set (MMS) that is a collection of multiple sub-MSs, each of which will be operated upon as a task in SLURM. By default, CASA will split the MS across scans, and a spectral window (spw) axis, given by the default separationaxis=&#39;auto&#39; parameter. However, this partitioning of data gives a data format that does not appear to perform well during processing. A better partitioning of the data is given by simply partitioning by scan, given by separationaxis=&#39;scan&#39;, numsubms=msmd.nscans() mstransform (advanced): the mstransform task (called under the hood by partition) is better suited to partition an MS into an MMS, as it allows for more control via several additional parameters, such as averaging in time and frequency. For example, the following call averages by 8 frequency channels (from selected frequency range 880-1680 MHz) and 16 second integrations, as well as selecting only two (parallel-hand) correlations (controlled by one core each, given by nthreads=2), and no autocorrelations: mstransform(vis=visname, outputvis=mvis, spw=&#39;*:880~1680MHz&#39;, createmms=True, datacolumn=&#39;DATA&#39;, chanaverage=True, chanbin=8, timeaverage=True, timebin=&#39;16s&#39;, separationaxis=&#39;scan&#39;, numsubms=msmd.nscans(), nthreads=2, antenna=&#39;*&amp;&#39;, correlation=&#39;XX,YY&#39;) tclean: In order to run across a cluster, parallel=True should be specified in tclean. However, if savemodel=&#39;modelcolumn&#39; is also specified, it triggers some kind of a race condition between the different nodes where they are competing for write access, and the task crashes. So setting savemodel=&#39;virtual&#39; or savemodel=&#39;none&#39; are the only options that work when running tclean in parallel. Both the makePSF step and the major cycles of deconvolution are MPI and OpenMP aware, and can exploit additional resources specified via --cpus-per-task in the SLURM sbatch file.",
    "url": "http://localhost:4000/docs/processMeerKAT/SLURM-and-MPICASA/",
    "relUrl": "/docs/processMeerKAT/SLURM-and-MPICASA/"
  },
  "8": {
    "id": "8",
    "title": "Access to IDIA Machines",
    "content": "Accessing the ilifu cluster You can request access to the ilifu cluster using the form found here. You will need to specify which team/project you are a part of while signing up. Joining the IDIA Pipelines Team Access to the IDIA-Pipeline resources can be arranged if you are part of the Pipelines Team. Please contact Bradley Frank (bradley@idia.ac.za) if you are interested in joining. Documentation and wiki access are provided through a GitHub project, so please sign-up to GitHub (if you haven’t already) and join the IDIA Pipelines repo as a contributor. Access A summary of how to access the ilifu services is provided here. Storage There are several storage areas available on the ilifu cluster, and access to data is managed via Unix groups. There are several mounted volumes, intended for different use cases. These are summarised here.",
    "url": "http://localhost:4000/docs/access/",
    "relUrl": "/docs/access/"
  },
  "9": {
    "id": "9",
    "title": "Configuration Files",
    "content": "Configuration files The config file is where you set parameters affecting how you run the pipeline. The default config contains the following: [data] vis = &#39;&#39; [fields] bpassfield = &#39;&#39; fluxfield = &#39;&#39; phasecalfield = &#39;&#39; targetfields = &#39;&#39; extrafields = &#39;&#39; [slurm] # See processMeerKAT.py -h for documentation nodes = 1 ntasks_per_node = 8 plane = 1 mem = 232 # Use this many GB of memory (per node) partition = &#39;Main&#39; # SLURM partition to use exclude = &#39;&#39; # SLURM nodes to exclude time = &#39;12:00:00&#39; submit = False container = &#39;/idia/software/containers/casa-6.5.0-modular.sif&#39; mpi_wrapper = &#39;mpirun&#39; name = &#39;&#39; dependencies = &#39;&#39; account = &#39;b03-idia-ag&#39; reservation = &#39;&#39; modules = [&#39;openmpi/4.0.3&#39;] verbose = False precal_scripts = [(&#39;calc_refant.py&#39;,False,&#39;&#39;), (&#39;partition.py&#39;,True,&#39;&#39;)] postcal_scripts = [(&#39;concat.py&#39;,False,&#39;&#39;), (&#39;plotcal_spw.py&#39;, False, &#39;&#39;), (&#39;selfcal_part1.py&#39;,True,&#39;&#39;), (&#39;selfcal_part2.py&#39;,False,&#39;&#39;), (&#39;science_image.py&#39;, True, &#39;&#39;)] scripts = [ (&#39;validate_input.py&#39;,False,&#39;&#39;), (&#39;flag_round_1.py&#39;,True,&#39;&#39;), (&#39;calc_refant.py&#39;,False,&#39;&#39;), (&#39;setjy.py&#39;,True,&#39;&#39;), (&#39;xx_yy_solve.py&#39;,False,&#39;&#39;), (&#39;xx_yy_apply.py&#39;,True,&#39;&#39;), (&#39;flag_round_2.py&#39;,True,&#39;&#39;), (&#39;xx_yy_solve.py&#39;,False,&#39;&#39;), (&#39;xx_yy_apply.py&#39;,True,&#39;&#39;), (&#39;split.py&#39;,True,&#39;&#39;), (&#39;quick_tclean.py&#39;,True,&#39;&#39;)] [crosscal] minbaselines = 4 # Minimum number of baselines to use while calibrating chanbin = 1 # Number of channels to average before calibration (during partition) width = 1 # Number of channels to (further) average after calibration (during split) timeavg = &#39;8s&#39; # Time interval to average after calibration (during split) createmms = True # Create MMS (True) or MS (False) for cross-calibration during partition keepmms = True # Output MMS (True) or MS (False) during split spw = &#39;*:880~933MHz,*:960~1010MHz,*:1010~1060MHz,*:1060~1110MHz,*:1110~1163MHz,*:1299~1350MHz,*:1350~1400MHz,*:1400~1450MHz,*:1450~1500MHz,*:1500~1524MHz,*:1630~1680MHz&#39; # Spectral window / frequencies to extract for MMS nspw = 11 # Number of spectral windows to split into calcrefant = False # Calculate reference antenna in program (overwrites &#39;refant&#39;) refant = &#39;m059&#39; # Reference antenna name / number standard = &#39;Stevens-Reynolds 2016&#39;# Flux density standard for setjy badants = [] # List of bad antenna numbers (to flag) badfreqranges = [ &#39;933~960MHz&#39;, # List of bad frequency ranges (to flag) &#39;1163~1299MHz&#39;, &#39;1524~1630MHz&#39;] [run] # Internal variables for pipeline execution continue = True dopol = False If you’re also performing self-calibration (option [-2 --do2GC] - see here), the default config will also contain the [selfcal] section: [selfcal] nloops = 2 # Number of clean + bdsf loops. loop = 0 # If nonzero, adds this number to nloops to name images or continue previous run cell = &#39;1.5arcsec&#39; robust = -0.5 imsize = [6144, 6144] wprojplanes = 512 niter = [10000, 50000, 50000] threshold = [&#39;0.5mJy&#39;, 10, 10] # After loop 0, S/N values if &gt;= 1.0, otherwise Jy nterms = 2 # Number of taylor terms gridder = &#39;wproject&#39; deconvolver = &#39;mtmfs&#39; calmode = [&#39;&#39;,&#39;p&#39;] # &#39;&#39; to skip solving (will also exclude mask for this loop), &#39;p&#39; for phase-only and &#39;ap&#39; for amplitude and phase solint = [&#39;&#39;,&#39;1min&#39;] uvrange = &#39;&#39; # uv range cutoff for gaincal flag = True # Flag residual column after selfcal? gaintype = &#39;G&#39; # Use &#39;T&#39; for polarisation on linear feeds (e.g. MeerKAT) discard_nloops = 0 # Discard this many selfcal solutions (e.g. from quick and dirty image) during subsequent loops (only considers when calmode !=&#39;&#39;) outlier_threshold = 0.0 # S/N values if &gt;= 1.0, otherwise Jy outlier_radius = 0.0 # Radius in degrees for identifying outliers in RACS If you’re also performing science imaging (option [-I --science_image] - see here), the default config will also conatin the [image] section: [image] cell = &#39;1.5arcsec&#39; robust = -0.5 imsize = [6144, 6144] wprojplanes = 512 niter = 50000 threshold = 10 # S/N value if &gt;= 1.0 and rmsmap != &#39;&#39;, otherwise Jy multiscale = [0, 5, 10, 15] nterms = 2 # Number of taylor terms gridder = &#39;wproject&#39; deconvolver = &#39;mtmfs&#39; restoringbeam = &#39;&#39; specmode = &#39;mfs&#39; stokes = &#39;I&#39; pbthreshold = 0.1 # Threshold below which to mask the PB for PB correction mask = &#39;&#39; rmsmap = &#39;&#39; outlierfile = &#39;&#39; If you do not perform either self-calibration or science imaging, the script related to those steps will be stripped from your config. Similarly calc_refant will be stripped from your config if calcrefant=False (the default), in which case m059 will be used as a good default reference antenna, or the pipeline will output a warning if this antenna doesn’t exist in your input MS, and will reommend other good reference antennas. When the pipeline is run, the contents of your config file are copied to the hidden file .config.tmp and each python script reads the parameters from this file as it is run. This way, the user cannot easily break the pipeline during the time it is running. This means changing the [slurm] section in your config file will have no effect unless you once again run processMeerKAT.py -R. Polarisation config If you select [-P --dopol] during the [-B --build] step of the pipeline, your config file be similar to the above, except xy_yx_solve and xy_yx_apply replace the 2nd call of the xx_yy_solve and xx_yy_apply scripts. Furthermore, in the [run] section, dopol=True will be set, which will cause all four correlations to be included during the initial partition step (or the pipeline will output a warning during the [-B --build] step if only two correlations exist). Additionally, we recommend setting gaintype = &#39;T&#39; in the [selfcal] section, if performing self-calibration for polarisation processing. Manually selecting field IDs As discussed here, the pipeline selects field IDs for you by default, during the [-B --build] step. However, these can be overwritten after this step by editing the [fields] section of your config file and manually selecting field IDs. Both field names (i.e. strings) and field IDs (i.e. strings of integers) are supported, although field names are preferable. Example use cases for manually selecting field IDs include when your input MS has mislabelled (or missing) INTENT, and when multiple flux/bandpass calibrators were used (or labelled as such via their INTENT), but where the default is less preferred (as the pipeline chooses the one with the most scans by default). For example, you may have two scans on field J0408-6545, but only one on J1939-6342, but the latter is still preferred as its model is more well constrained. Lastly, you may wish to remove extrafields to reduce processing time, if you have no interest in calibrating these fields and producing quick-look images of them. SPW Splitting Version 1.1 introduced spectral window (SPW) splitting, where each separate SPW is processed concurrently. Here we discuss all that is relevant to this functionality. This mode is the default mode, invoked by selecting a value greater than one with the config parameter nspw. This mode is recommended for most use cases, when the input dataset is TB in size, and the number of scans is typical (tens of scans). When the input data (after pre-processing, including selection of spw) is small (tens to hundreds of GB), or when the number of scans is very large (hundreds), the single MS mode (see MS only) is recommended. Alternatively, for such a use case, the user could set nspw=1, resulting in the previous behaviour from version 1.0, but with potential polarisation calibration and flux scale issues. When selecting nspw &gt; 1 and passing in a single SPW range via the spw config parameter (e.g. spw=*:880~1680MHz), the data will split into nspw SPWs during the [-R --run] step, which will be equally-sized frequency ranges encompassing the frequency range provided. The calibration algorithm within each SPW remains the same. However, the resulting Stokes I calibration when nspw &gt; 1 is improved overall, as it accounts for the intrinsic spectrum of the phase calibrator, which would otherwise be treated as flat across the entire spw. Furthermore, the polarisation calibration is improved, since each SPW, within which the phase calibrator’s leakages and Stokes Q and U are assumed to be constant, is solved separately, which accounts for the wideband polarisation structure. Setting spw and nspw The SPWs into which the pipeline splits and independently (and concurrently) processes, is determined by the final values stored in the config parameter spw. Multiple SPWs are listed as comma-seperated values, but will only be considered as separate SPWs when nspw is equal to the number of comma-separated SPWs given by the spw parameter. If nspw=1 and spw contains comma-separated values, only a single SPW will be processed, and the comma-separated values will be passed into the CASA tasks that set the spw parameter. If nspw is not equal to the number of comma-separated SPWs, the pipeline will output a warning and set nspw in your config to be equal to the number of comma-separated SPWs. Therefore, the SPWs can be manually set to ranges that will be separately (and concurrently) processed, such as manual frequency ranges that avoid regions of persistent RFI. We recommend the following default for this purpose: nspw=11 spw = &#39;*:880~933MHz,*:960~1010MHz,*:1010~1060MHz,*:1060~1110MHz,*:1110~1163MHz,*:1299~1350MHz,*:1350~1400MHz,*:1400~1450MHz,*:1450~1500MHz,*:1500~1524MHz,*:1630~1680MHz&#39; This also enables you to set badfreqranges = [], as the SPWs have already been constructed to avoid these regions of persistent RFI. Alternatively, if spw is equal to a single value (i.e. not a comma-separated list), the pipeline can split the frequency range given by spw into nspw equal ranges, which is done during the [-R --run] step. This is the default behaviour when spw is not manually set. By default, nspw is set to 16 during the [-B --build] step, and then updated to 12 during the [-R --run] step, since 4 SPWs are completely encompassed by the default flagging mask given by the config parameter badfreqranges, which has a default value of [&#39;933~960MHz&#39;, &#39;1163~1299MHz&#39;,&#39;1524~1630MHz&#39;]. So after the [-B --build] step, the default spw is *:860~1680MHz, and during the [-R --run] step, the pipeline will remove *:1167.5~1218.75MHz, *:1218.75~1270.0MHz, *:1526.25~1577.5MHz and *:1577.5~1628.75MHz, so that the final default spw is *:860.0~911.25MHz,*:911.25~962.5MHz,*:962.5~1013.75MHz,*:1013.75~1065.0MHz,*:1065.0~1116.25MHz,*:1116.25~1167.5MHz,*:1270.0~1321.25MHz,*:1321.25~1372.5MHz,*:1372.5~1423.75MHz,*:1423.75~1475.0MHz,*:1475.0~1526.25MHz,*:1628.75~1680.0MHz Any frequency unit can be used for each comma-separated spw, such as GHz, kHz, or no unit, which selects channel indices. However, the removal of SPWs that are encompassed by the RFI mask will only be removed when using MHz. After updating the config’s SPWs, the [-R --run] step creates directories for each of the SPWs, named as 880~933MHz, 960~1010MHz, etc. It then copies your config file into each of them but overwrites spw to be the single SPW that will be processed, and nspw=1. Furthermore, if you have not requested all 32 tasks per node (i.e. each SPW will be processed by an entire node) with the config parameter ntasks_per_node=32, it will overwrite the config parameter mem with the integet part of the previous value divided by nspw/2. Lastly, it will set precal_script and postcal_scripts to empty lists (see below). Each SPW directory created during the [-R --run] step will be processed independently and concurrently, after the initial partition job that runs over the single input MS (see below). This is achieved by running an instance of the pipeline within each SPW directory, with nspw=1, following the same calibration recipe within each separate SPW as within version 1.0. Pre-cal and post-cal scripts When nspw &gt; 1, the config parameter scripts refers to the separate scripts that will be run as single jobs within each SPW directory. Any scripts that should be run within the top-level working directory (i.e. above the SPW directories) are stored in precal_scripts, and postcal_scripts, respectively run before and after running the scripts within each SPW directory. By default, precal_scripts = [(&#39;calc_refant.py&#39;,False,&#39;&#39;), (&#39;partition.py&#39;,True,&#39;&#39;)], and postcal_scripts = [(&#39;concat.py&#39;,False,&#39;&#39;), (&#39;plotcal_spw.py&#39;, False, &#39;&#39;), (&#39;selfcal_part1.py&#39;,True,&#39;&#39;), (&#39;selfcal_part2.py&#39;,False,&#39;&#39;), (&#39;science_image.py&#39;, True, &#39;&#39;)], although calc_refant.py will be stripped if calcrefant=False, and the selfcal and imaging scripts will be stripped if you do not select these routines during the [-B --build] step with [-2 --do2GC] and [-I --science_image], respectively. The partition.py and concat.py steps are discussed below. The steps following concat are run over the concatenated dataset that spans all of the SPWs. Alternatively, these scripts (e.g. for self-carlibation and science imaging) can be appended to the end of scripts, which will then be run on the targets split from each of the SPWs. Similar to the behaviour during the cross-calibration steps, self-calibrating separately over SPWs will partially solve for frequency dependence of your gain, albeit with a loss of signal-to-noise (compared to the full-band self-calibration run after concatenating). After doing this, concat can be run, and then one final science imaging step over the whole (concatenated) band. Alternatively, you could perform a custom combination of your SPW images, such as a weighted average image, assuming you’ve matched the resolution with a common restoringbeam and/or uvtaper. During the [-R --run] step, if nspw=1 and precal_scripts or postcal_scripts are not empty, a warning will be output, and these lists will be respectively prepended and appended to scripts, and then set to empty lists. Partition The initial partition, given by the partiton.py script (by default the last script in precal_scripts), which reads your input MS and partitions out your selected spw into an MMS, is run as a SLURM array job when nspw &gt; 1. This step allows multiple concurrently-running partitions over several SPWs, but limited to &lt; 200 cores, meaning that sometimes not every SPW is partitioned concurrently. After the first SPW is partitioned, the first SPW launches an instance of the pipeline, and so on until the last SPW is processing. This behaviour is a special case that only works when partition.py is the last script within precal_scripts, otherwise the processing of every SPW waits until the last script within precal_scripts has been run. Bash jobScripts When using nspw &gt; 1, the behaviour of the bash scripts within the jobScripts directory (symlinked from the working directory) is different. Each bash script will iterate through the SPW directories and display the output for that SPW, running each SPW directory’s instance of that bash script, which remains the same as above. Since there are more than 100 jobs by default, ./summary.sh will display only the running or failed jobs, and will not display completed or pending jobs, whereas ./fullSummary.sh will display all of the jobs. Additionally, when precal_scripts and postcal_scripts are not empty lists, there will be a version of each of these jobScripts starting with allSPW_. These correspond to the pipeline jobs that are run at top-level directory over all SPWs, which is also displayed when calling the other jobScripts. For example, when running ./summary.sh -X (where -X outputs ones line per job) during the middle of your processing with nspw=2, you will see something similar to the following: jcollier@slurm-login:/scratch/users/jcollier/MIGHTEE/nspw2$ ./summary.sh -X SPW #1: /scratch/users/jcollier/MIGHTEE/nspw2/1350~1375MHz JobID JobName Partition Elapsed NNodes NTasks NCPUS MaxDiskRead MaxDiskWrite NodeList TotalCPU CPUTime MaxRSS State ExitCode - - -- -- - - - - -- 1838779 setjy Main 00:02:31 1 4 compute-039 00:00:00 00:10:04 RUNNING 0:0 -- SPW #2: /scratch/users/jcollier/MIGHTEE/nspw2/1375~1400MHz JobID JobName Partition Elapsed NNodes NTasks NCPUS MaxDiskRead MaxDiskWrite NodeList TotalCPU CPUTime MaxRSS State ExitCode - - -- -- - - - - -- 1838788 flag_round_1 Main 00:06:22 1 4 compute-059 00:00:00 00:25:28 RUNNING 0:0 -- All SPWs: /scratch/users/jcollier/MIGHTEE/nspw2 JobID JobName Partition Elapsed NNodes NTasks NCPUS MaxDiskRead MaxDiskWrite NodeList TotalCPU CPUTime MaxRSS State ExitCode - - -- -- - - - - -- 1838776_0 partition Main 00:03:49 1 8 compute-058 00:00:00 00:30:32 COMPLETED 0:0 1838776_1 partition Main 00:03:19 1 8 compute-018 00:00:00 00:26:32 COMPLETED 0:0 1838797 concat Main 00:02:13 1 1 compute-020 00:00:00 00:00:00 PENDING 0:0 1838798 plotcal_spw Main 00:00:19 1 1 compute-020 00:00:00 00:00:00 PENDING 0:0 Concatenation and further imaging After all of the SPWs have completed, irrespective of their completion status, the scripts within postcal_scripts are run at the top level directory (i.e. above the SPW directories). By default, the first of these is concat.py, which concatenates any output MMSs/MSs and quick-look images. By default these are the split calibrator and target fields, and their quick-look images. If the config parameter keepmms=True, virtualconcat will be run for each field that has been split into its own MMS, resulting in an MMS with nspw x nscans sub-MSs. If keepmms=False, concat is run over each field that has been split into its own MS, resulting in a single concatenated MS. In both cases, the resulting MMS/MS will now contain multiple SPWs, given by nspw (assuming that all SPWs successfully completed processing). These can then be imaged over the whole concatenated frequency range via further image processing, such as self-calibration and science imaging. Furthermore, each split field’s quick-look image is concatenated together into a quick-look continuum cube. If any SPWs failed to split out a field or create an image for that field, they will be excluded from the (image and MMS/MS) concatenation.",
    "url": "http://localhost:4000/docs/processMeerKAT/config-files/",
    "relUrl": "/docs/processMeerKAT/config-files/"
  },
  "10": {
    "id": "10",
    "title": "Singularity Containers",
    "content": "Singularity Containers At IDIA, astronomical software packages are provided and managed using Singularity containers. Containers are managed and built by our developers. If you have any questions/issues relating to containers, please send an email to support@ilifu.ac.za. You can find more documentation about containers on Ilifu here. Available Containers The most recent, stable versions of containers are available in /idia/software/containers/. Some of these containers can be accessed via JupyterHub, but all of them can be accessed via the terminal (i.e., once you’ve ssh’d into an IDIA machine). Some additional CASA-specific information can be found here. Here are a few examples of the latest stable Singularity container builds that are available in /idia/software/containers/: casa-6.5.0-modular.sif Contains the modular CASA 6.5 install, which can be accessed via Python using import casatools etc. kern7.simg Contains all the software packages provided by the Kern repository. ASTRO-PY3.simg Builds of commonly used source finding packages, e.g., PyBDSF. Reporting a bug / Requesting Software Please send an email to support@ilifu.ac.za to report issues, or to request new containers, or for existing containers to be updated. Using Containers Shell You can access software in Singularity containers using two methods. Firstly, you can shell into the container. You will enter a shell which provides access to the software provided in that container: $ singularity shell /idia/software/containers/casa-stable-5.3.0.simg Singularity: Invoking an interactive shell within container... Singularity casa-stable-5.3.0.simg:~&gt; casa --nologger --log2term --nogui ========================================= The start-up time of CASA may vary depending on whether the shared libraries are cached or not. ========================================= IPython 5.1.0 -- An enhanced Interactive Python. CASA 5.3.0-143 -- Common Astronomy Software Applications 2018-08-07 12:37:28 INFO ::casa CASA Version 5.3.0-143 --&gt; CrashReporter initialized. Enter doc(&#39;start&#39;) for help getting started with CASA... Using matplotlib backend: TkAgg CASA &lt;1&gt;: print &quot;Hello World&quot; Hello World Exec You can also pass a command to a container (using the exec argument), which then gets executed via the shell in that container. For example, here’s an illustration of how to use the exec argument to jump into an interactive CASA session: $ singularity exec /idia/software/containers/casa-stable-5.3.0.simg casa --nologger --log2term --nogui ========================================= The start-up time of CASA may vary depending on whether the shared libraries are cached or not. ========================================= IPython 5.1.0 -- An enhanced Interactive Python. CASA 5.3.0-143 -- Common Astronomy Software Applications 2018-08-07 12:41:19 INFO ::casa CASA Version 5.3.0-143 --&gt; CrashReporter initialized. Enter doc(&#39;start&#39;) for help getting started with CASA... Using matplotlib backend: TkAgg CASA &lt;1&gt;: inp listobs --&gt; inp(listobs) # listobs :: List the summary of a data set in the logger or in a file vis = &#39;&#39; # Name of input visibility file (MS) selectdata = True # Data selection parameters field = &#39;&#39; # Selection based on field names or field index numbers. Default is all. spw = &#39;&#39; # Selection based on spectral-window/frequency/channel. antenna = &#39;&#39; # Selection based on antenna/baselines. Default is all. timerange = &#39;&#39; # Selection based on time range. Default is entire range. correlation = &#39;&#39; # Selection based on correlation. Default is all. scan = &#39;&#39; # Selection based on scan numbers. Default is all. intent = &#39;&#39; # Selection based on observation intent. Default is all. feed = &#39;&#39; # Selection based on multi-feed numbers: Not yet implemented array = &#39;&#39; # Selection based on (sub)array numbers. Default is all. uvrange = &#39;&#39; # Selection based on uv range. Default: entire range. Default units: meters. observation = &#39;&#39; # Selection based on observation ID. Default is all. verbose = True # Controls level of information detail reported. True reports more than False. listfile = &#39;&#39; # Name of disk file to write output. Default is none (output is written to logger only). listunfl = False # List unflagged row counts? If true, it can have significant negative performance impact. cachesize = 50 # EXPERIMENTAL. Maximum size in megabytes of cache in which data structures can be held. CASA &lt;2&gt;: The true utility of the exec argument is to execute commands non-interactively: $ singularity exec /idia/software/containers/casa-stable-5.3.0.simg casa --nologger --log2term --nogui -c &quot;print &#39;Hello World&#39;&quot; ========================================= The start-up time of CASA may vary depending on whether the shared libraries are cached or not. ========================================= IPython 5.1.0 -- An enhanced Interactive Python. CASA 5.3.0-143 -- Common Astronomy Software Applications 2018-08-07 12:45:43 INFO ::casa CASA Version 5.3.0-143 --&gt; CrashReporter initialized. Hello World $ While the command may seem cumbersome, it is very useful when trying to build scripts that utilise several containers.",
    "url": "http://localhost:4000/docs/containers/",
    "relUrl": "/docs/containers/"
  },
  "11": {
    "id": "11",
    "title": "Cross-calibration in processMeerKAT",
    "content": "Cross-calibration processMeerKAT implements a CASA-based wide-band full Stokes calibration pipeline (in the linear basis). Broadly, the pipeline aims to “do the right thing” and by keeping the steps as general as possible we believe that there should be no need for fine tuning in order to obtain a well calibrated dataset. The pipeline is implemented as a series of SLURM sbatch scripts that in turn call CASA scripts. The scripts are separated out to make optimal use of the cluster, by splitting out sections that can be run in parallel (via casampi and SLURM) and sections that must be run in serial. New in Version 1.1 : The MeerKAT band can now be optionally separated out into multiple spectral windows (SPWs) which are processed in parallel. Each SPW is processed simultaneously (assuming there are a sufficient number of free nodes on the cluster) bringing down the total runtime to . The steps outlined below are run per-SPW. If this option is turned off, these steps are run over the entire band. The logical steps are: Input validation : This script performs a few basic validity checks, on the default config file, and on the input MS. The existence of the input MS, and the data types of the inputs specified in the config file are all verified before the pipeline continues to the next steps. If reference antenna calculation is not requested, a simple check is performed to verify that the input reference antenna exists in the MS. Otherwise, the following paragraph describes the details of reference antenna calculation. Reference antenna calculation : If the calcrefant parameter in the config file is set to True, then this script is executed. It attempts to determine the reference antenna by selecting the antenna with the smallest number of flags. Data partition : The input measurement set (MS) is partitioned into a multi-measurement set (MMS) using the CASA task partition. This task splits up the main MS into smaller SUBMSs that are individual units of a larger logical MMS. The number of SUBMSs created is equal to the number of scans in the input MS. Partitioning the data in this manner allows for more efficient use of computation while using MPI. If multiple SPWs are specified in the config file, each SPW is partitioned and calibrated independently and concurrently. Flagging (round 1) : The first of two rounds of pre-calibration flagging. If badfreqranges and badants are specified in the config file, the specified frequency ranges and antennas are flagged prior to any further flagging operations. These lists are also allowed to be empty. Following that the data are clipped a nominal level of 50 Jy to eliminate the strongest RFI. The tfcrop algorithm is run (with a conservative flagging threshold) independently on the primary and secondary calibrators and the target(s). setjy : The setjy task is run on the specified primary calibrators, which writes the source model into the MS/MMS. Optionally if the --dopol option is specified, this script detects if either 3C286 or 3C138 is present in the data and includes the full Stokes model. Parallel hand calibration : Standard delay, bandpass and gain calibration is run on the data. The time-dependent gain table contains solutions for both the primary and secondary calibrators. The fluxes are then bootstrapped from the primary to the secondary calibrator. Flagging (round 2) : This time, both the tfcrop and rflag algorithms are run independently on the primary and secondary calibrator and the target(s). The rflag algorithm was not included in the first round of flagging because it is sensitive to changes in the bandpass shape. After calibration, the bandpass is much flatter allowing a median-filtering approach such as rflag to operate much more effectively. The thresholds are lower than the first round as the data is now calibrated, and therefore more well behaved. calibrated data. Cross hand calibration : The cross-hand calibration (necessary for polarization observations) utilizes the unpolarized primary to solve for the leakages, and the polarization calibrator to solve for the XY phase. The pipeline uses the published full Stokes models as an input to solve for the XY phase. This approach requires only a single scan on the polarization calibrator. At the time of writing (April 2022) the standard polarization calibrators are all in the Northern hemisphere, and hence most MeerKAT observations are only able to have a single (or two) scans on the calibrator source, and our approach ensures sensible polarization results with such constraints. Splitting out calibrated data : Finally each field in the MMS (whether calibrator or target) is split out and optionally averaged in time and frequency. If multiple SPWs are specified, this process is done per SPW. At this stage it is possible to either retain the data in an MMS or to convert it to an MS. Concatenation (only multi-SPW) : If multiple SPWs are specified in the config, the split out MS/MMSs from each SPW are concatenated together. This final MS/MMS will internally have multiple SPWs. Detailed description What follows is a more detailed description of each of the steps described above, where applicable. Partition* : During the partition stage, autocorrelations are excluded from the data in order to reduce data volume. Further, if --dopol is not specified, the cross-hand correlations (XY and YX) are also excluded from the data to reduce overall data volume. Flagging (round 1) : If a list of bad frequency ranges and bad antennas is specified, those are flagged. Subsequently, flagdata is called on the calibrators and target sources with conservative limits to clip out the worst RFI. It also makes a single call to tfcrop to flag data at a limit. tfcrop in this case is preferred, since the as yet uncalibrated bandpass shape should be taken care of by fitting a piecewise polynomial across the band. setjy : By default, the ‘Stevens-Reynolds 2016’ flux scale is used. If the primary calibrator J0408-6545 is present and is specified as the flux calibrator, a broadband Stokes I model is used via the manual mode of setjy. This model is courtesy SARAO. Cross hand calibration : The full Stokes calibration procedure is done identically across each SPW specified in the config file. The cross-hand calibration performs the following steps: Bandpass calibration on the primary calibrator Leakage calibration on the unpolarized primary calibrator (typically one of either J1939-6342 or J0408-6545) Gain calibration (using gaintype=’T’ in CASA) on the primary, secondary and polarization calibrators XY Phase calibration on the polarization calibrator We opt to use gaintype=’T’ in the CASA gaincal task since it preserves the relative gains of the X &amp; Y feeds. This is important because in the case of the secondary and polarization calibrators, the Stokes Q power of the source is retained in the difference of the XX and YY correlations, and correcting for each correlation independently without accounting for the source polarization will reduce the Stokes Q signal to 0 since any relative change between the two will be assumed to be due to instrumental effects. While in principle during the setjy step we set the full Stokes model of the polarization calibrator, we find that the CASA gaincal task does not reliably pick this model up while calibrating with gaintype=&#39;G&#39;. However the gaintype=&#39;XYf+QU&#39; mode does recognize the input model and yields sensible XY phase solutions. Please note that after XY phase solutions the resulting calibration solutions could still contain an arbitrary polarization angle offset corresponding to a feed offset. This offset can be measured by generating cubes of the polarization calibrator, and any measured offsets can be corrected for in the image domain or by modifying the calibration tables directly. At this time the pipeline does not support either of these operations, and it will have to be done by the user after the pipeline has completed.",
    "url": "http://localhost:4000/docs/processMeerKAT/cross-calibration-in-processmeerkat/",
    "relUrl": "/docs/processMeerKAT/cross-calibration-in-processmeerkat/"
  },
  "12": {
    "id": "12",
    "title": "DEEP 2 Tutorial",
    "content": "DEEP 2 Tutorial This tutorial walks you through running the various steps of the pipeline for a single DEEP 2 dataset, which is a snapshot (~20 minutes on source), 16-dish MeerKAT observation of a radio-quiet patch of sky using the old ROACH-2 correlator, 11 GB in size. It was written for v1.0 of the pipeline during July 2020, but can be reproduced with v1.1 upwards by setting nspw=1. To begin, ssh into the ilifu cluster (slurm.ilifu.ac.za), and create a working directory somewhere on the filesystem (e.g. /scratch/users/your_username/tutorial/). 1. Source setup.sh, which will add to your PATH and PYTHONPATH source /idia/software/pipelines/master/setup.sh 2. Build a config file, using verbose mode, and pointing to the DEEP 2 dataset processMeerKAT.py -B -C tutorial_config.txt -M /idia/projects/deep/1491550051.ms -v You should get the following output, with different timestamps 2020-07-06 09:28:19,943 INFO: Extracting field IDs from MeasurementSet &quot;/idia/projects/deep/1491550051.ms&quot; using CASA. 2020-07-06 09:28:19,966 DEBUG: Using the following command: srun --time=10 --mem=4GB --partition=Main --qos qos-interactive singularity run /idia/software/containers/casa-stable-5.6.2-2.simg /idia/software/pipelines/master/processMeerKAT/read_ms.py -B -M /idia/projects/deep/1491550051.ms -C tutorial_config.txt -N 1 -t 8 -v 2&gt;&amp;1 | grep -v &#39;msmetadata_cmpt.cc::open |MSMetaData::_computeScanAndSubScanProperties |MeasIERS::fillMeas(MeasIERS::Files, Double) |Position:&#39; 2020-07-06 09:29:25,672 INFO: Multiple fields found with intent &quot;CALIBRATE_FLUX&quot; in dataset &quot;/idia/projects/deep/1491550051.ms&quot; - [0 1]. 2020-07-06 09:29:26,539 WARNING: Only using field &quot;0&quot; for &quot;fluxfield&quot;, which has the most scans (1). 2020-07-06 09:29:26,540 WARNING: Putting extra fields with intent &quot;CALIBRATE_FLUX&quot; in &quot;extrafields&quot; - [1] 2020-07-06 09:29:26,541 INFO: Multiple fields found with intent &quot;CALIBRATE_BANDPASS&quot; in dataset &quot;/idia/projects/deep/1491550051.ms&quot; - [0 1]. 2020-07-06 09:29:26,541 WARNING: Only using field &quot;0&quot; for &quot;bpassfield&quot;, which has the most scans (1). 2020-07-06 09:29:26,541 INFO: Multiple fields found with intent &quot;CALIBRATE_PHASE&quot; in dataset &quot;/idia/projects/deep/1491550051.ms&quot; - [1 2]. 2020-07-06 09:29:26,542 WARNING: Only using field &quot;2&quot; for &quot;phasecalfield&quot;, which has the most scans (5). 2020-07-06 09:29:26,542 INFO: [fields] section written to &quot;tutorial_config.txt&quot;. Edit this section if you need to change field IDs (comma-seperated string for multiple IDs, not supported for calibrators). 2020-07-06 09:29:27,156 DEBUG: Delta parang: 7.83458165019 2020-07-06 09:29:27,156 WARNING: Parallactic angle coverage is &lt; 30 deg. Polarisation calibration will most likely fail, so setting dopol=False in [run] section of &#39;tutorial_config.txt&#39;. 2020-07-06 09:29:27,163 INFO: Using reference antenna &#39;m059&#39;. 2020-07-06 09:29:27,163 INFO: This is usually a well-behaved (stable) antenna. Edit &#39;tutorial_config.txt&#39; to change this, by updating &#39;refant&#39; in [crosscal] section. 2020-07-06 09:29:27,163 DEBUG: Alternatively, set &#39;calcrefant=True&#39; in [crosscal] section of &#39;tutorial_config.txt&#39;, and include &#39;calc_refant.py&#39; in &#39;scripts&#39; in [slurm] section. 2020-07-06 09:29:27,847 WARNING: The number of threads (1 node(s) x 8 task(s) = 8) is not ideal compared to the number of scans (12) for &quot;/idia/projects/deep/1491550051.ms&quot;. 2020-07-06 09:29:27,847 WARNING: Config file has been updated to use 1 node(s) and 6 task(s) per node. 2020-07-06 09:29:27,900 DEBUG: Overwritting [run] section in config file &quot;tutorial_config.txt&quot; with: {&#39;dopol&#39;: False}. 2020-07-06 09:29:27,917 DEBUG: Overwritting [slurm] section in config file &quot;tutorial_config.txt&quot; with: {&#39;ntasks_per_node&#39;: 6, &#39;nodes&#39;: 1}. 2020-07-06 09:29:27,939 DEBUG: Overwritting [fields] section in config file &quot;tutorial_config.txt&quot; with: {&#39;bpassfield&#39;: &quot;&#39;0&#39;&quot;, &#39;fluxfield&#39;: &quot;&#39;0&#39;&quot;, &#39;phasecalfield&#39;: &quot;&#39;2&#39;&quot;, &#39;extrafields&#39;: &quot;&#39;1&#39;&quot;, &#39;targetfields&#39;: &quot;&#39;3&#39;&quot;}. 2020-07-06 09:29:27,960 DEBUG: Overwritting [crosscal] section in config file &quot;tutorial_config.txt&quot; with: {&#39;spw&#39;: &quot;&#39;0:880.0~1680.0MHz&#39;&quot;}. 2020-07-06 09:29:29,617 INFO: Config &quot;tutorial_config.txt&quot; generated. This calls CASA via the default singularity container without writing log files, and runs read_ms.py. It calls srun, requesting only 1 node, 1 task, 4 GB of memory, a 10 minute time limit, with interactive quality of service (qos) to increase the likelihood of launching srun immediately. The purpose of this call is to read the input MS and extract information used to build the pipeline run, such as the field IDs corresponding to our different fields, and the number of scans (to check against the nodes and tasks per node, each of which is handled by a MPI worker - see step 3). The output statements with DEBUG correspond to those output during [-v --verbose] mode. Warnings are display when multiple calibrator fields are present with the same intent, but only one is extracted, corresponding to the field with the most scans. In this case the extras fields are moved to extrafields (i.e. for applying calibration and imaging). For more information about MPI and parallelism, see ilifu training slides (slides 12-16) and video. 3. View the config file created, which has the following contents: [data] vis = &#39;/idia/projects/deep/1491550051.ms&#39; [fields] bpassfield = &#39;0&#39; fluxfield = &#39;0&#39; phasecalfield = &#39;2&#39; targetfields = &#39;3&#39; extrafields = &#39;1&#39; [slurm] nodes = 1 ntasks_per_node = 6 plane = 1 mem = 232 partition = &#39;Main&#39; exclude = &#39;&#39; time = &#39;12:00:00&#39; submit = False container = &#39;/idia/software/containers/casa-stable-5.6.2-2.simg&#39; mpi_wrapper = &#39;/idia/software/pipelines/casa-pipeline-release-5.6.1-8.el7/bin/mpicasa&#39; name = &#39;&#39; dependencies = &#39;&#39; account = &#39;b03-idia-ag&#39; reservation = &#39;&#39; verbose = True precal_scripts = [(&#39;calc_refant.py&#39;, False, &#39;&#39;), (&#39;partition.py&#39;, True, &#39;&#39;)] postcal_scripts = [(&#39;concat.py&#39;, False, &#39;&#39;), (&#39;plotcal_spw.py&#39;, False, &#39;&#39;)] scripts = [(&#39;validate_input.py&#39;, False, &#39;&#39;), (&#39;flag_round_1.py&#39;, True, &#39;&#39;), (&#39;calc_refant.py&#39;, False, &#39;&#39;), (&#39;setjy.py&#39;, True, &#39;&#39;), (&#39;xx_yy_solve.py&#39;, False, &#39;&#39;), (&#39;xx_yy_apply.py&#39;, True, &#39;&#39;), (&#39;flag_round_2.py&#39;, True, &#39;&#39;), (&#39;xx_yy_solve.py&#39;, False, &#39;&#39;), (&#39;xx_yy_apply.py&#39;, True, &#39;&#39;), (&#39;split.py&#39;, True, &#39;&#39;), (&#39;quick_tclean.py&#39;, True, &#39;&#39;), (&#39;plot_solutions.py&#39;, False, &#39;&#39;)] [crosscal] minbaselines = 4 # Minimum number of baselines to use while calibrating chanbin = 1 # Number of channels to average before calibration (during partition) width = 1 # Number of channels to (further) average after calibration (during split) timeavg = &#39;8s&#39; # Time interval to average after calibration (during split) createmms = True # Create MMS (True) or MS (False) for cross-calibration during partition keepmms = True # Output MMS (True) or MS (False) during split spw = &#39;0:880.0~1680.0MHz&#39; nspw = 16 # Number of spectral windows to split into calcrefant = False # Calculate reference antenna in program (overwrites &#39;refant&#39;) refant = &#39;m059&#39; # Reference antenna name / number standard = &#39;Stevens-Reynolds 2016&#39;# Flux density standard for setjy badants = [] # List of bad antenna numbers (to flag) badfreqranges = [ &#39;933~960MHz&#39;, # List of bad frequency ranges (to flag) &#39;1163~1299MHz&#39;, &#39;1524~1630MHz&#39;] [run] continue = True dopol = False This config file contains five sections - data, fields, slurm, crosscal, and run. The fields IDs that we just extracted, seen in section [fields], correspond to field 0 for the bandpass calibrator, field 0 for the total flux calibrator, field 2 for the phase calibrator, fields 3 for the science target (i.e. the DEEP 2 field) and field 1 for an extra calibrator field for which we’ll apply solutions and produce a quick-look image. Only the target and extra fields may have multiple fields, separated by a comma. If a field isn’t found according to its intent, a warning is displayed, and the field for the total flux calibrator is selected. If the total flux calibrator isn’t present, the program will display an error and terminate. The [run] section is used internally by the pipeline, and should be ignored. The SLURM parameters in section [slurm] correspond to those seen by running processMeerKAT.py -h. The pipeline executes all the scripts from the scripts parameter in order, including any of your own that you can insert (see Advanced Usage). The precal_scripts and postcal_scripts are only relevant when nspw &gt; 1 (the default is nspw=16), where as we will set nspw=1 for this tutorial, meaning that in the next step, the scripts in precal_scripts will be prepended to the beginning of scripts, and the scripts in postcal_scripts will be appended to the end of scripts. By default, for this particular MS, for all threadsafe scripts (i.e. those with True in the list(s) of scripts), we use 1 node, 6 tasks per node, 232 GB of memory (per node), and plane=1 (an argument that distributes N tasks onto one node before moving onto next node). During step 2, only 12 scans were found, and since partition.py partitions the data into one sub-MeasurementSet (sub-MS) per scan, only 12 sub-MSs will exist in the multi-MeasurementSet (MMS - see step 10 below). Assuming that each observation has a phase calibrator bracketing each target scan, and includes at least one other calibrator scan (i.e. the bandpass/flux calibrator), at most, half the sub-MSs will be generally operated on at any given time, each handled by one MPI worker, and a master MPI worker (the MPIClient). So we aim to have a limit of nscans/2 threads, including the MPIClient. For this dataset, the limit is 6 threads, so read_ms.py attempts to match this number by starting with one node and increasing the number of tasks (and then nodes) until the number of threads is more than the limit, terminating at 1 nodes x 6 tasks per node = 6 threads. For scripts that aren’t threadsafe (i.e. those with False in the list(s) of scripts), we use a single node, and a single task per node. For the majority scripts that are threadsafe and those that aren’t, we use a single CPU per task, and explicitly export OMP_NUM_THREADS=1, since there is no documentation or evidence of a speedup with more than one CPU per task. However, for partition.py we use between 2-4 CPUs per task (equal to the number of polarisations, which is 2 by default, but 4 if [-D --dopol] is used, which adds the xy_yx_solve.py or xy_yx_apply.py scripts to the scripts parameter in your config). Furthermore, quick_tclean.py will use as many CPUs as it can without exceeding 32 in total. The cross-calibration parameters in section [crosscal] correspond to various CASA parameters passed into the calibration tasks that the pipeline uses, following an algorithm that is documented here. By default all frequency ranges listed in badfreqranges, and all antenna numbers listed in badants, will be flagged out entirely. If the calc_refant.py script is run by the pipeline (i.e. when calcrefant=True and calc_refant.py is in the list of scripts), this will likely change the value of refant, and possibly add a list of bad antennas to badants. 4. Edit your config file to set nspw=1, mem=5GB, postcal_scripts=[] and then run the pipeline using your config file processMeerKAT.py -R -C tutorial_config.txt You should get the following output, with different timestamps 2020-07-06 10:21:59,009 WARNING: Appending &quot;precal_scripts&quot; to beginning of &quot;scripts&quot;, and &quot;postcal_scripts&quot; to end of &quot;scripts&quot;, since nspw=1. Overwritting this in &quot;tutorial_config.txt&quot;. 2020-07-06 10:21:59,661 DEBUG: Copying &#39;tutorial_config.txt&#39; to &#39;.config.tmp&#39;, and using this to run pipeline. 2020-07-06 10:21:59,773 WARNING: Changing [slurm] section in your config will have no effect unless you [-R --run] again. 2020-07-06 10:21:59,943 DEBUG: Wrote sbatch file &quot;partition.sbatch&quot; 2020-07-06 10:21:59,979 DEBUG: Wrote sbatch file &quot;validate_input.sbatch&quot; 2020-07-06 10:22:00,005 DEBUG: Wrote sbatch file &quot;flag_round_1.sbatch&quot; 2020-07-06 10:22:00,050 DEBUG: Wrote sbatch file &quot;setjy.sbatch&quot; 2020-07-06 10:22:00,162 DEBUG: Wrote sbatch file &quot;xx_yy_solve.sbatch&quot; 2020-07-06 10:22:00,195 DEBUG: Wrote sbatch file &quot;xx_yy_apply.sbatch&quot; 2020-07-06 10:22:00,256 DEBUG: Wrote sbatch file &quot;flag_round_2.sbatch&quot; 2020-07-06 10:22:00,301 DEBUG: Wrote sbatch file &quot;xx_yy_solve.sbatch&quot; 2020-07-06 10:22:00,358 DEBUG: Wrote sbatch file &quot;xx_yy_apply.sbatch&quot; 2020-07-06 10:22:00,405 DEBUG: Wrote sbatch file &quot;split.sbatch&quot; 2020-07-06 10:22:00,427 DEBUG: Wrote sbatch file &quot;quick_tclean.sbatch&quot; 2020-07-06 10:22:00,455 DEBUG: Wrote sbatch file &quot;plot_solutions.sbatch&quot; 2020-07-06 10:22:00,639 INFO: Master script &quot;submit_pipeline.sh&quot; written, but will not run. A number of sbatch files have now been written to your working directory, each of which corresponds to the python script in the list of scripts set by the scripts parameter in our config file. Our config file was copied to .config.tmp, which is the config file written and edited by the pipeline, which the user should not touch. A logs directory was created, which will store the CASA and SLURM log files. Lastly, a bash script called submit_pipeline.sh was written, however, this script was not run, since we set submit = False in our config file (to immediately submit to the SLURM queue, you can change this in your config file, or by using option [-s --submit] when you build your config file with processMeerKAT.py). Normally, we would run ./submit_pipeline.sh to run the pipeline, and return later when it is completed. However, we will look at later, as we first want to get a handle of how the pipeline works. 5. View validate_input.sbatch, which has the following contents: #!/bin/bash #SBATCH --account=b03-idia-ag #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=5GB #SBATCH --job-name=validate_input #SBATCH --distribution=plane=1 #SBATCH --output=logs/%x-%j.out #SBATCH --error=logs/%x-%j.err #SBATCH --partition=Main #SBATCH --time=12:00:00 export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK srun singularity run /idia/software/containers/casa-stable-5.6.2-2.simg /idia/software/pipelines/master/processMeerKAT/validate_input.py --config .config.tmp 2&gt;&amp;1 | grep -v &#39;msmetadata_cmpt.cc::open |MSMetaData::_computeScanAndSubScanProperties |MeasIERS::fillMeas(MeasIERS::Files, Double) |Position:&#39; Since this script is not threadsafe, the job is called with srun, and is configured to run a single task on a single node. The last line shows the call of the validate_input.py script, which will validate the parameters in your config file. 6. Run the first sbatch job sbatch validate_input.sbatch You should see the following output, corresponding to your SLURM job ID Submitted batch job 1491583 7. View your job in the SLURM queue (if you weren’t quick enough, repeat step 6, and quickly do step 7) squeue You will see something similar to the following, with other people’s jobs mixed into the queue. JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1491583 Main validate jcollier R 0:13 1 slwrk-121 We can see the job with name validate was submitted to SLURM worker node 121, amongst a number of jobs in the Main partition, the Jupyter Spawner partition, and possible other partitions. Your job may list (Priority), which means it is too low a priority to be run at this point, or (Resources), which means it is waiting for resources to be made available. NOTE: You can view just your jobs with squeue -u your_username, an individual job with squeue -j 1491583, and just the jobs in the main partition with squeue -p Main. You can view which nodes are allocated, which are idle, which are mixed (i.e. partially allocated), and which are down in the Main partition with sinfo -p Main. Often it is good idea to check this before selecting your SLURM parameters. More more information, see the ilifu documentation 8. View partition.sbatch, which has the following contents: #!/bin/bash #SBATCH --account=b03-idia-ag #SBATCH --nodes=1 #SBATCH --ntasks-per-node=6 #SBATCH --cpus-per-task=2 #SBATCH --mem=5GB #SBATCH --job-name=partition #SBATCH --distribution=plane=1 #SBATCH --output=logs/%x-%j.out #SBATCH --error=logs/%x-%j.err #SBATCH --partition=Main #SBATCH --time=12:00:00 export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK /idia/software/pipelines/casa-pipeline-release-5.6.1-8.el7/bin/mpicasa singularity exec /idia/software/containers/casa-stable-5.6.2-2.simg casa --nologger --nogui --logfile logs/${SLURM_JOB_NAME}-${SLURM_JOB_ID}.casa -c /idia/software/pipelines/master/processMeerKAT/crosscal_scripts/partition.py --config .config.tmp Here we see the same default SLURM parameters for threadsafe tasks, as discussed in step 3. We now use mpicasa as the MPI wrapper, since we are calling a threadsafe script partition.py, which calls CASA task mstransform, which partitions a selection of the data (e.g. selecting only frequencies specified by your spectral window with parameter spw in your config file) into your working directory. When createmms=True (the default), a multi-MeasurementSet (MMS) is created and the data are partitioned into several sub-MeasurementSets (sub-MSs - see step 10 below), otherwise a single MS is created. 9. Submit your job and watch it in the queue sbatch partition.sbatch Submitted batch job 1491788 squeue -j 1491788 You will see something similar to the following, showing that SLURM worker 101 is now being used. JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1491788 Main partitio jcollier R 0:02 1 slwrk-101 Wait until the job completes, before step 10. 10. View the contents of 1491550051.880.0~1680.0MHz.mms. You should see 1491550051.880.0~1680.0MHz.mms, which corresponds to your multi-MeasurementSet (MMS). From now on, the pipeline operates on these data, rather than the raw data stored in /idia/projects/. Inside this MMS, you will find the same tables and metadata as in a normal MS, but you will also see a SUBMSS directory, which should have the following contents. 1491550051.880.0~1680.0MHz.mms.0000.ms 1491550051.880.0~1680.0MHz.mms.0004.ms 1491550051.880.0~1680.0MHz.mms.0008.ms 1491550051.880.0~1680.0MHz.mms.0001.ms 1491550051.880.0~1680.0MHz.mms.0005.ms 1491550051.880.0~1680.0MHz.mms.0009.ms 1491550051.880.0~1680.0MHz.mms.0002.ms 1491550051.880.0~1680.0MHz.mms.0006.ms 1491550051.880.0~1680.0MHz.mms.0010.ms 1491550051.880.0~1680.0MHz.mms.0003.ms 1491550051.880.0~1680.0MHz.mms.0007.ms 1491550051.880.0~1680.0MHz.mms.0011.ms These are the 12 sub-MSs, partitioned by this observation’s 12 scans of the various fields. If we now view the CASA log (logs/partition-1491788.casa), you will find a bunch of junk output from mpicasa (often including nominal “errors”, sometimes severe), and 13 calls of mstransform, corresponding to 12 MPI workers for your 12 sub-MSs, and the master MPIClient. The master call from the MPIClient is the same one written to the standard error log (logs/partition-1491788.err). Your standard output log (logs/partition-1491788.out) will contains 6 sets of output from CASA launching, corresponding to the 6 threads (i.e. 1 node x 6 tasks per node) and some junk output from mpicasa. 11. Edit your config file to run the next steps Edit tutorial_config.txt to remove the tuples for the first two and last six scripts in the scripts parameter, update vis to the MMS and select the submit option, so it looks like the following: [data] vis = 1491550051.880.0~1680.0MHz.mms . . [slurm] . . submit = True . . scripts = [(&#39;flag_round_1.py&#39;, True, &#39;&#39;), (&#39;calc_refant.py&#39;, False, &#39;&#39;), (&#39;setjy.py&#39;, True, &#39;&#39;), (&#39;xx_yy_solve.py&#39;, False, &#39;&#39;), (&#39;xx_yy_apply.py&#39;, True, &#39;&#39;)] 12. Run the pipeline using your config file processMeerKAT.py -R -C tutorial_config.txt You should see the following output, with different timestamps 2020-07-06 13:47:18,390 DEBUG: Copying &#39;tutorial_config.txt&#39; to &#39;.config.tmp&#39;, and using this to run pipeline. 2020-07-06 13:47:18,608 WARNING: Changing [slurm] section in your config will have no effect unless you [-R --run] again. 2020-07-06 13:47:18,711 DEBUG: Wrote sbatch file &quot;flag_round_1.sbatch&quot; 2020-07-06 13:47:18,774 DEBUG: Wrote sbatch file &quot;setjy.sbatch&quot; 2020-07-06 13:47:18,833 DEBUG: Wrote sbatch file &quot;xx_yy_solve.sbatch&quot; 2020-07-06 13:47:18,878 DEBUG: Wrote sbatch file &quot;xx_yy_apply.sbatch&quot; 2020-07-06 13:47:19,127 INFO: Running master script &quot;submit_pipeline.sh&quot; Copying tutorial_config.txt to .config.tmp, and using this to run pipeline. Submitting flag_round_1.sbatch to SLURM queue with following command: sbatch flag_round_1.sbatch Submitting setjy.sbatch SLURM queue with following command sbatch -d afterok:1491808 --kill-on-invalid-dep=yes setjy.sbatch Submitting xx_yy_solve.sbatch to SLURM queue with following command sbatch -d afterok:1491808,1491809 --kill-on-invalid-dep=yes xx_yy_solve.sbatch Submitting xx_yy_apply.sbatch to SLURM queue with following command sbatch -d afterok:1491808,1491809,1491810 --kill-on-invalid-dep=yes xx_yy_apply.sbatch Submitted sbatch jobs with following IDs: 1491808,1491809,1491810,1491811 Run ./killJobs.sh to kill all the jobs. Run ./summary.sh to view the progress. Run ./findErrors.sh to find errors (after pipeline has run). Run ./displayTimes.sh to display start and end timestamps (after pipeline has run). Run ./cleanup.sh to remove MSs/MMSs from this directory (after pipeline has run). As before, we see the sbatch files being written to our working directory. Since we set submit=True, submit_pipeline.sh has been run, and all output after that (without the timestamps) comes from this bash script. After the first job is run (sbatch flag_round_1.sbatch), each other job is run with a dependency on all previous jobs (e.g. sbatch -d afterok:1491808,1491809,1491810 --kill-on-invalid-dep=yes xx_yy_apply.sbatch). We can see this by calling squeue -u your_username, which shows those jobs (Dependency). submit_pipeline.sh then writes five job scripts, all of which are explained in the output, written to the jobScripts directory with a timestamp appended to the filename, and symlinked from your working directory. findErrors.sh finds errors after this pipeline run has completed, ignoring all MPI errors. These tasks follow the first step of a two-step calibration process that is summarised here. 13. Run ./summary.sh This script simply calls sacct for all jobs submitted within this pipeline run. You should get output similar to the following. JobID JobName Partition Elapsed NNodes NTasks NCPUS MaxDiskRead MaxDiskWrite NodeList TotalCPU CPUTime MaxRSS State ExitCode - - -- -- - - - - -- 1491808 flag_round_1 Main 00:05:55 1 6 slwrk-143 00:00:00 00:35:30 RUNNING 0:0 1491809 setjy Main 00:00:00 1 6 None assigned 00:00:00 00:00:00 PENDING 0:0 1491810 xx_yy_solve Main 00:00:00 1 1 None assigned 00:00:00 00:00:00 PENDING 0:0 1491811 xx_yy_apply Main 00:00:00 1 6 None assigned 00:00:00 00:00:00 PENDING 0:0 Those PENDING are the jobs with dependencies, or jobs waiting for resources. Once this pipeline run has completed, ./summary.sh should give output similar to the following. JobID JobName Partition Elapsed NNodes NTasks NCPUS MaxDiskRead MaxDiskWrite NodeList TotalCPU CPUTime MaxRSS State ExitCode - - -- -- - - - - -- 1491808 flag_round_1 Main 00:18:06 1 6 slwrk-143 07:45.358 01:48:36 COMPLETED 0:0 1491808.batch batch 00:18:06 1 1 6 8.90G 1.03G slwrk-143 07:45.358 01:48:36 3.05G COMPLETED 0:0 1491809 setjy Main 00:05:47 1 6 slwrk-140 00:53.310 00:34:42 FAILED 1:0 1491809.batch batch 00:05:47 1 1 6 8.45G 8.28G slwrk-140 00:53.310 00:34:42 2.31G FAILED 1:0 1491810 xx_yy_solve Main 00:03:09 1 1 slwrk-134 01:22.545 00:03:09 COMPLETED 0:0 1491810.batch batch 00:03:09 1 1 1 0.23M 0.15M slwrk-134 00:00.716 00:03:09 0.01G COMPLETED 0:0 1491810.0 singularity 00:03:09 1 1 1 7.66G 0.00G slwrk-134 01:21.828 00:03:09 0.39G COMPLETED 0:0 1491811 xx_yy_apply Main 00:02:50 1 6 slwrk-118 01:57.609 00:17:00 COMPLETED 0:0 1491811.batch batch 00:02:50 1 1 6 12.56G 7.53G slwrk-118 01:57.609 00:17:00 2.27G COMPLETED 0:0 SLURM will most likely report the setjy job as FAILED, even though the job has not failed (see known issues). 14. View caltables directory The calibration solution tables have been written to caltables/1491550051.880.0~1680.0MHz.*, including bcal, gcal, fluxscale and kcal, corresponding to the calibration solutions for bandpass, complex gains, flux-scaled complex gains, and delays, respectively. 15. Run ./displayTimes.sh You should see output similar to the following, which shows this run took ~30 minutes to complete, the longest of which was flagging for ~18 minutes. In this particular run, there was a ~51 minute wait time after flag_round_1 had completed, before setjy was launched. logs/flag_round_1-1491808.casa logs/flag_round_1-1491808.err logs/flag_round_1-1491808.out 2020-07-06 13:47:56 2020-07-06 14:05:36 logs/setjy-1491809.casa logs/setjy-1491809.err logs/setjy-1491809.out 2020-07-06 14:56:40 2020-07-06 15:02:15 logs/xx_yy_solve-1491810.casa logs/xx_yy_solve-1491810.err logs/xx_yy_solve-1491810.out 2020-07-06 15:02:39 2020-07-06 15:05:36,296 logs/xx_yy_apply-1491811.casa logs/xx_yy_apply-1491811.err logs/xx_yy_apply-1491811.out 2020-07-06 15:06:10 2020-07-06 15:08:46 16. Run ./findErrors.sh You should see similar output to the following: logs/flag_round_1-1491808.casa logs/flag_round_1-1491808.err logs/flag_round_1-1491808.out logs/setjy-1491809.casa logs/setjy-1491809.err logs/setjy-1491809.out error message you will receive is this one. logs/xx_yy_solve-1491810.casa logs/xx_yy_solve-1491810.err logs/xx_yy_solve-1491810.out 2020-07-06 15:02:58 SEVERE MeasTable::dUTC(Double) (file ../../measures/Measures/MeasTable.cc, line 4290) Leap second table TAI_UTC seems out-of-date. 2020-07-06 15:02:58 SEVERE MeasTable::dUTC(Double) (file ../../measures/Measures/MeasTable.cc, line 4290)+ Until the table is updated (see the CASA documentation or your system admin), 2020-07-06 15:02:58 SEVERE MeasTable::dUTC(Double) (file ../../measures/Measures/MeasTable.cc, line 4290)+ times and coordinates derived from UTC could be wrong by 1s or more. 2020-07-06 15:02:58 SEVERE MeasTable::dUTC(Double) (file ../../measures/Measures/MeasTable.cc, line 4290) Leap second table TAI_UTC seems out-of-date. 2020-07-06 15:02:58 SEVERE MeasTable::dUTC(Double) (file ../../measures/Measures/MeasTable.cc, line 4290)+ Until the table is updated (see the CASA documentation or your system admin), 2020-07-06 15:02:58 SEVERE MeasTable::dUTC(Double) (file ../../measures/Measures/MeasTable.cc, line 4290)+ times and coordinates derived from UTC could be wrong by 1s or more. logs/xx_yy_apply-1491811.casa logs/xx_yy_apply-1491811.err logs/xx_yy_apply-1491811.out The repeated error during the xx_yy_solve is a false positive error (see diagnosing errors). 17. Build a new config file pointing to your MMS, without verbose mode processMeerKAT.py -B -C tutorial_config_part2.txt -M 1491550051.880.0~1680.0MHz.mms This way we reset the list of scripts in our config file, and set verbose=False and submit=False. We will manually remove the scripts that we already ran in step 20, so leave the scripts parameter as is for now. 18. Edit your config file Edit tutorial_config.txt once again to set nspw=1, mem=5GB, precal_scripts=[] and postcal_scripts=[]. 19. Run the pipeline using your updated config file processMeerKAT.py -R -C tutorial_config_part2.txt Since we have set verbose=False and submit=False, the pipeline will not yet run, and you should see simplified output like the following: 2020-07-06 14:36:30,247 WARNING: Changing [slurm] section in your config will have no effect unless you [-R --run] again. 2020-07-06 14:36:30,759 INFO: Master script &quot;submit_pipeline.sh&quot; written, but will not run. 20. Edit submit_pipeline.sh You will see in submit_pipeline.sh that each sbatch job is submitted on its own line, and that the job ID is extracted. Remove everything from #partition.sbatch to one line before #flag_round_2.sbatch (i.e. the previous jobs we already ran). Edit the line with the first sbatch call to replace +=, with = and remove -d afterok:$IDs --kill-on-invalid-dep=yes, since the first job does not have any dependencies. After this, submit_pipeline.sh should look like the following: #!/bin/bash cp tutorial_config_part2.txt .config.tmp #flag_round_2.sbatch IDs=$(sbatch flag_round_2.sbatch | cut -d &#39; &#39; -f4) #xx_yy_solve.sbatch IDs+=,$(sbatch -d afterok:$IDs --kill-on-invalid-dep=yes xx_yy_solve.sbatch | cut -d &#39; &#39; -f4) #xx_yy_apply.sbatch IDs+=,$(sbatch -d afterok:$IDs --kill-on-invalid-dep=yes xx_yy_apply.sbatch | cut -d &#39; &#39; -f4) #split.sbatch IDs+=,$(sbatch -d afterok:$IDs --kill-on-invalid-dep=yes split.sbatch | cut -d &#39; &#39; -f4) #quick_tclean.sbatch IDs+=,$(sbatch -d afterok:$IDs --kill-on-invalid-dep=yes quick_tclean.sbatch | cut -d &#39; &#39; -f4) #plot_solutions.sbatch IDs+=,$(sbatch -d afterok:$IDs --kill-on-invalid-dep=yes plot_solutions.sbatch | cut -d &#39; &#39; -f4) #Output message and create jobScripts directory echo Submitted sbatch jobs with following IDs: $IDs mkdir -p jobScripts . . . 21. Run ./submit_pipeline.sh Again, we see simplified output Submitted sbatch jobs with following IDs: 1492690,1492691,1492692,1492693,1492694,1492695 Run ./killJobs.sh to kill all the jobs. Run ./summary.sh to view the progress. Run ./findErrors.sh to find errors (after pipeline has run). Run ./displayTimes.sh to display start and end timestamps (after pipeline has run). Run ./cleanup.sh to remove MSs/MMSs from this directory (after pipeline has run). These job IDs comprise the new pipeline run we’ve just launched. So now ./summary.sh will display sacct for the new job IDs, similar to the following: JobID JobName Partition Elapsed NNodes NTasks NCPUS MaxDiskRead MaxDiskWrite NodeList TotalCPU CPUTime MaxRSS State ExitCode - - -- -- - - - - -- 1492690 flag_round_2 Main 00:05:00 1 6 slwrk-118 00:00:00 00:30:00 RUNNING 0:0 1492691 xx_yy_solve Main 00:00:00 1 1 None assigned 00:00:00 00:00:00 PENDING 0:0 1492692 xx_yy_apply Main 00:00:00 1 6 None assigned 00:00:00 00:00:00 PENDING 0:0 1492693 split Main 00:00:00 1 6 None assigned 00:00:00 00:00:00 PENDING 0:0 1492694 quick_tclean Main 00:00:00 1 30 None assigned 00:00:00 00:00:00 PENDING 0:0 1492695 plot_solutions Main 00:00:00 1 1 None assigned 00:00:00 00:00:00 PENDING 0:0 The five new ancillary (bash) jobScripts will now correspond to these six new job IDs. If you want to see the output from the jobScripts referring to the old pipeline runs, don’t worry, they’re still in the jobScripts directory with an older timestamp in the filename. Only the symlink in your working directory has been updated. Wait until the run finishes before step 22. You may want to come back later, as it takes ~45 minutes. 22. View the pipeline output After this pipeline run has completed, viewing the output of ./summary.sh or ./displayTimes.sh shows this run took ~45 minutes, including ~20 minutes for quick-look imaging all fields, and ~14 minutes for plotting (a known issue). These new tasks follow the second step of a two step calibration process that is summarised on this page. After split.py has run, you will see four new files 1491550051.880.0~1680.0MHz.0252-712.mms 1491550051.880.0~1680.0MHz.0408-65.mms 1491550051.880.0~1680.0MHz.1934-638.mms 1491550051.880.0~1680.0MHz.DEEP_2_off.mms These correspond to the data split out from 1491550051.880.0~1680.0MHz.mms, for the bandpass/flux calibrator (0408-65), the phase calibrator (0252-712), the science target (DEEP_2_off), and an extra field (0408-65 - often used as a flux/bandpass calibrator). 1491550051.880.0~1680.0MHz.mms itself has roughly doubled in size, since it has added columns for corrected data (from applycal) and model data (from setjy). This file can be safely removed now, as the corrected data for the fields of interest have been split into their own MMSs, as listed above. If you remove it and later need to derive the same data, you could run partition, apply the solutions stored in caltables, and the flags stored in 1491550051.880.0~1680.0MHz.mms.flagversions, which together take up ~1.5 GB, compared to ~17 GB for the MMS. 23. View the images in the images directory quick_tclean.py creates quick-look images (i.e. with no selfcal, w-projection, thresholding, multiscale, etc) with robust weighting 0, for all fields specified in the config file, creating 512x512 images of the calibrator and extra fields, and 2048x2048 images of the target field(s), both with 2 arcsec pixel sizes. For data with &gt; 100 MHz bandwidth, two taylor terms are used, otherwise the ‘clark’ deconvolver is used. Convert the quick-look image for the science target (DEEP_2_off) from FITS to a HDF5 file, so that we can inspect it with CARTA: srun --mem=1GB --time=1 /carta_share/hdf_convert/run_hdf_converter -o /carta_share/users/your_username/1491550051.880.0~1680.0MHz_DEEP_2_off.im.hdf5 images/1491550051.880.0~1680.0MHz_DEEP_2_off.im.fits Connect to https://carta.idia.ac.za/, and open 1491550051.880.0~1680.0MHz_DEEP_2_off.im.hdf5. Alternatively, you can view the images by connecting to a compute/worker node (ensure you use ssh -YA when connecting to ilifu - see ilifu docs) with: salloc --qos qos-interactive and launch ds9 or CASA viewer, respectively with the syntax (replace /scratch/users/your_username/tutorial/ below): singularity exec /idia/software/containers/SF-PY3-bionic.simg ds9 -log /scratch/users/your_username/tutorial/images/*fits singularity exec /idia/software/containers/casa-stable-5.6.2-2.simg casa --nologger --log2term -c &quot;viewer(infile=&#39;/scratch/users/your_username/tutorial/images/1491550051.880.0~1680.0MHz_DEEP_2_off.im.image.tt0&#39;); raw_input()&quot; Here’s what your images of the flux calibrator (1934-638) and target (DEEP_2_off) should look like. Since we imaged a snapshot 16-dish MeerKAT observation using the old ROACH-2 correlator, with an on source time of ~20 minutes, we do not get very good image quality. Below is a more typical image produced by quick_tclean.py for a 64-dish observation using the SKARAB correlator, spanning ~8 hours, and only 10 MHz bandwidth. 24. View the figures in plots directory The last script that runs is plot_solutions.py, which calls CASA task plotms to plot the corrected data to eyeball for RFI. Below are a few selected plots. That’s it! You have completed the tutorial! Now go forth and do some phenomenal MeerKAT science! Also see Calibration in processMeerKAT Diagnosing Errors Using the pipeline Release Notes",
    "url": "http://localhost:4000/docs/processMeerKAT/deep-2-tutorial/",
    "relUrl": "/docs/processMeerKAT/deep-2-tutorial/"
  },
  "13": {
    "id": "13",
    "title": "Home",
    "content": "IDIA Pipelines Welcome to the IDIA Pipelines website! Here you’ll find documentation to help you do calibration and imaging of radio data using the ilifu. The IDIA Pipelines Team The team includes members from many of the MeerKAT Large Survey Projects. This list includes the active members who are working on the IDIA system, or the related software/algorithms: Jordan Collier Bradley Frank (Project Lead) Srikrishna Sekhar Russ Taylor Contact Email the project lead at bradley@idia.ac.za, or submit a ticket to our helpdesk: support@ilifu.ac.za.",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },
  "14": {
    "id": "14",
    "title": "processMeerKAT",
    "content": "processMeerKAT The processMeerKAT software has been written to do calibration and imaging of MeerKAT interferometric data. Typical calibration and imaging of radio data comprises three steps: Initial or a priori calibration, which involves bootstrapping phase and flux gains from observations of reference calibrators. Self-calibration or a posteriori’ calibration, where the residual on-target errors are solved for by (iteratively) building a good representation of the field. 3rd Generation Calibration (3GC), aka post-selfcal, which deals with higher-order effects in the pursuit of high dynamic range imaging. This includes compensating for the primary beam and direction dependent effects. The processMeerKAT currently does full-polarisation a priori and a posteriori’ calibration on MeerKAT data, and includes automated flagging. processMeerKAT is written solely for the processing of data on the ilifu SLURM cluster, but future revisions will allow you to run the software on any HPC platform. The main features of processMeerKAT are as follows: Is written in Python 3.8+ (for CASA 6.5+). Calibration algorithms only use CASA 6.5+ tasks and helper functions. Uses a purpose-built CASA Singularity container for parallel processing at IDIA (i.e. is fully thread-safe). Uses casampi to run parallel jobs over the cluster. Generates sbatch files and ancillary helper scripts for processing. Please read the documentation to learn how to use the pipeline for your imaging data at IDIA. License Copyright (C) 2022 Inter-University Institute for Data Intensive Astronomy. support@ilifu.ac.za This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see https://www.gnu.org/licenses/.",
    "url": "http://localhost:4000/docs/processMeerKAT",
    "relUrl": "/docs/processMeerKAT"
  },
  "15": {
    "id": "15",
    "title": "Science Imaging in processMeerKAT",
    "content": "Science Imaging Note: Our selfcal and science imaging workflow generally assumes a fixed image size, and may perform sub-optimally (e.g. sources missing from the CLEAN mask) if the image size changes at any point Imaging at higher spectral resolution (i.e. with more frequency channels) requires higher computational power / runtime, and may require increasing the time limit of your imaging jobs. In general we recommend self-calbrating at 1k (1024 channels). If your raw data is at higher spectral resolution, consider first averaging in frequency with the width or chanbin config parameters (also see spectral-line pre-processing here) processMeerKAT implements science imaging using the tclean task in CASA to generate the images, and the katbeam package to perform primary beam correction on the final image. The science imaging functionality exposes more of the advanced imaging capabilities of tclean and is intended to be run after all the self-calibration loops. By default the final imaging mask generated by self-calibration is passed in to science imaging, as well as the final set of outlier fields (if outlier imaging is enabled). Similarly, the final rmsmap is passed in to science imaging, to enable S/N-based thresholding (see below). It is possible to run science imaging immediately after cross-calibration, without running selfcal, although these three fields will not be populated, but would need to be manually set based on a pre-existing set of custom output (e.g. a mask, RMS map and outlier file). Science imaging is expected to be run once, preferably at the end of self-calibration, and unlike self-calibration it does not accept tuples as arguments. A note on PB correction If the CASA gridders standard or wproject are used the default .pb image generated is incorrect for MeerKAT. We therefore use the package katbeam to perform the PB correction. The final data products are the flat noise image (produced by CASA), the katbeam PB image as well as the PB corrected image. The katbeam package describes the MeerKAT PB as an axis-symmetric cos^2 function. In a future release of the pipeline we intend to support the AW projection algorithm, which will produce a correct .pb image for MeerKAT from within tclean, in full Stokes. Science Imaging Config Options The default science imaging section in the config file is shown below, and the details of each option are explained further down. [image] cell = &#39;1.5arcsec&#39; robust = -0.5 imsize = [6144, 6144] wprojplanes = 512 niter = 50000 threshold = 10 # S/N value if &gt;= 1.0 and rmsmap != &#39;&#39;, otherwise Jy multiscale = [0, 5, 10, 15] nterms = 2 # Number of taylor terms gridder = &#39;wproject&#39; deconvolver = &#39;mtmfs&#39; restoringbeam = &#39;&#39; specmode = &#39;mfs&#39; stokes = &#39;I&#39; pbthreshold = 0.1 # Threshold below which to mask the PB for PB correction mask = &#39;&#39; rmsmap = &#39;&#39; outlierfile = &#39;&#39; cell: The cell size of the pixels in the image plane, specified as a CASA compatible string (with units). robust: The Briggs’ robust parameter used during gridding. Specified as a floating point between -2 (uniform weighting) and +2 (natural weighting). imsize: The size of the image in pixels. The image dimensions can be specified in one or two dimensions, hence 6144 is treated as [6144,6144]. wprojplanes : The number of W-projection planes to use (in case the wproject gridder is specified). Must be specified as an integer. niter: The number of iterations to perform during deconvolution. Imaging will continue until either the stopping threshold (described below) is reached, or the iteration limit is reached. threshold: The stopping threshold for deconvolution. If the threshold value is larger than 1 and the rmsmap parameter is not blank, the value is treated as a signal-to-noise ratio. If the threshold value is less than 1, or if it is greater than 1 and the rmsmap parameter is blank, it is treated as an absolute value in Jy. If science imaging is requested after self-calibration, the rmsmap parameter will be auto-populated by the pipeline. multiscale; The number of individual scales to use if multi-scale imaging is desired (and the appropriate deconvolver is chosen - e.g. mtmfs or multiscale) specified as a list of integers such as [0, 5, 10] etc. To switch off multiscale imaging, specify a blank list (i.e., multiscale=[]). nterms : The number of Taylor terms to use for broadband imaging. gridder : The gridder to use for imaging, typically one of either standard or wproject. Any CASA gridder is a valid entry for this parameter, although all the necessary configuration options for different gridders will not be exposed through the configuration file. deconvolver : The deconvolver to use during imaging. Use clark or hogbom when the fractional bandwidth (bandwidth divided by band centre) is &lt; ~15%. restoringbeam : Specified as a string with units, such as &#39;20arcsec&#39;. If this is left blank, the default restoring beam is used. Specifying this parameter can be used to force a specific restoring beam during imaging. specmode : Specify whether to generate a multi-frequency synthesis image (mfs) or a spectral cube (cube or cubedata). stokes : The Stokes planes to image. Please note that generating multi-Stokes spectral cubes or multi-Stokes cubes with outliers, is not currently possible within CASA pbthreshold : The PB gain threshold below which to mask the PB and the PB corrected image. The PB is generated using the katbeam package. mask : Path to a CLEAN mask or a CASA region file. This is normally auto-populated by the pipeline if science imaging is queued up after self-calibration. However, if science imaging is being launched manually, this will have to be specified if masking is desired. If the parameter is left blank, no masking will be performed. rmsmap : Path to an RMS image, used to determine local thresholds during deconvolution. This is normally auto-populated by the pipeline if science imaging is queued up after self-calibration. If this is left blank, the threshold parameter is assumed to be an absolute (global) threshold in Jy. outlierfile : Path to a valid outlier fields file. This is normally auto-populated by the pipeline if science imaging is queued up after self-calibration. If this is left blank, no outlier imaging is performed.",
    "url": "http://localhost:4000/docs/processMeerKAT/science-imaging-in-processmeerkat/",
    "relUrl": "/docs/processMeerKAT/science-imaging-in-processmeerkat/"
  },
  "16": {
    "id": "16",
    "title": "Self-calibration in processMeerKAT",
    "content": "Self-calibration Note: Our selfcal and science imaging workflow generally assumes a fixed image size, and may perform sub-optimally (e.g. sources missing from the CLEAN mask) if the image size changes at any point Imaging at higher spectral resolution (i.e. with more frequency channels) requires higher computational power / runtime, and may require increasing the time limit of your imaging jobs. In general we recommend self-calbrating at 1k (1024 channels). If your raw data is at higher spectral resolution, consider first averaging in frequency with the width or chanbin config parameters (also see spectral-line pre-processing here) processMeerKAT implements the imaging &amp; self-calibration loop using a combination of CASA for imaging and calibration and PyBDSF for source-finding and masking. Briefly, the self-calibration implementation in processMeerKAT works as follows : The initial iteration of the loop performs a blind deconvolution of the sky, which is followed by running PyBDSF to identify and create a mask around the point sources detected in the image. The subsequent iterations use the source mask generated from the previous iteration, allowing for deeper imaging. Each iteration runs PyBDSF after the imaging stage in order to further refine the mask. By default, no gain calibration is performed after the first blind deconvolution (although this is configurable), and the desired type of gain calibration can be specified (phase-only, amplitude &amp; phase, etc.) per loop. The configuration options are intended to be as flexible as possible, with sensible defaults. Some configuration options can be specified per loop, and others only accept a single value for the entire self-calibration cycle. These options are summarized below : Single options - These config variables accept only a single argument, and will be applied identically to all iterations of the self-cal cycle: nloops, loop, discard_nloops, outlier_threshold, outlier_radius Gaincal options - These config variables accept either a single argument, or a list of length nloops: solint, calmode, gaintype, flag List of list options - These config variables accept either a single argument, or a list of lists of length nloop: imsize Any configuration options not listed above can contain either a single value (applied to all loops) or a list of length nloops + 1. This is explained further in the following section. Selfcal Config Options The default self-cal section in the config file is shown below, and the details of each option are explained further down. [selfcal] nloops = 2 # Number of clean + bdsf loops. loop = 0 # If nonzero, adds this number to nloops to name images or continue previous run cell = &#39;1.5arcsec&#39; robust = -0.5 imsize = [6144, 6144] wprojplanes = 512 niter = [10000, 50000, 50000] threshold = [&#39;0.5mJy&#39;, 10, 10] # After loop 0, S/N values if &gt;= 1.0, otherwise Jy nterms = 2 # Number of taylor terms gridder = &#39;wproject&#39; deconvolver = &#39;mtmfs&#39; calmode = [&#39;&#39;,&#39;p&#39;] # &#39;&#39; to skip solving (will also exclude mask for this loop), &#39;p&#39; for phase-only and &#39;ap&#39; for amplitude and phase solint = [&#39;&#39;,&#39;1min&#39;] uvrange = &#39;&#39; # uv range cutoff for gaincal flag = True # Flag residual column after selfcal? gaintype = &#39;G&#39; # Use &#39;T&#39; for polarisation on linear feeds (e.g. MeerKAT) discard_nloops = 0 # Discard this many selfcal solutions (e.g. from quick and dirty image) during subsequent loops (only considers when calmode !=&#39;&#39;) outlier_threshold = 0.0 # S/N values if &gt;= 1.0, otherwise Jy outlier_radius = 0.0 # Radius in degrees for identifying outliers in RACS nloops : The number of self calibration loops to perform. This determines the number of times gain calibration will be performed. The total number of images generated will be nloops+1. Each loop in general comprises imaging (with tclean), calibration (with gaincal) and source finding/masking (with PyBDSF). The final loop does not perform the gaincal stage since the gain solutions will not be applied for any further loops. loop : Set this to be a non-zero number if restarting from a previous selfcal run. Can be left to zero for all other cases. cell: The cell size of the pixels in the image plane, specified as a CASA compatible string (with units). This can be a single parameter, which will be applied identically to all loops, or specified as a list of strings if a different cell size per loop is desired. The length of the list must be nloops+1 to account for the final image after all the self-calibration loops are complete. robust: The Briggs’ robust parameter used during gridding. Specified identically to the cell parameter. imsize: The size of the image in pixels. The image dimensions must be specified in both dimensions, hence a list of [6144,6144] is treated as a single specification and will be applied identically to all loops. To specify a different image size per iteration, a list-of-lists syntax must be used, such as - [[6144, 6144], [8192,8192]]. wprojplanes : The number of W-projection planes to use (in case the wproject gridder is specified). Specified identically to the cell parameter. niter: The number of iterations to perform per loop. Imaging will continue until either the stopping threshold (described below) is reached, or the iteration limit is reached. Specified identically to the cell parameter. threshold: The stopping threshold per loop (a total of nloop+1). The stopping threshold for the initial loop (i.e. index 0 if setting threshold as a list) must be specified as a string with units, or a decimal &lt; 1 in units of Jy. For subsequent loops, if a decimal is specified, values &gt; 1 are considered as multipliers on the image RMS (as determined by the minimum value of the PyBDSF RMS map), while values &lt; 1 remain considered as a threshold value in Jy. nterms : The number of Taylor terms to use for broadband imaging, assuming deconvolver=mtmfs. Specified identically to the cell parameter. gridder : The gridder to use for imaging, typically one of either standard or wproject. Specified identically to the cell parameter. deconvolver : The deconvolver to use during imaging. Use clark or hogbom when the fractional bandwidth (bandwidth divided by band centre) is &lt; ~15%. Specified identically to the cell parameter. calmode : The calibration type per loop - one of either &#39;p&#39; (phase only) or &#39;ap&#39; (amplitude and phase). A blank string (&#39;&#39;) specifies that no calibration will be performed for that loop. Must be of length nloop, and we recommend skipping gaincal during the first blind imaging step. solint : The solution interval per loop, specified similar to calmode. We recommend &#39;inf&#39; for loops with calmode=&#39;ap&#39;, which will solve per scan. Specified identically to the calmode parameter. uvrange: The UV range limit to consider for gaincal. In the presence of strong RFI, it can be helpful to discard the shortest baselines while performing the gain calibration solve. This does not affect the imaging steps. Specified identically to the calmode parameter. flag: True/False - Determines whether flagging is performed on the residuals (data-model) between gain calibration solves. In cases where residual RFI remains, this can improve the image quality during subsequent loops. Setting it to False can provide a speedup to the imaging process. Specified identically to the calmode parameter. gaintype: The gaintype specified in the gaincal task. Typically one of either G or T. Use T if preserving the polarization in instruments with linear feeds is important (e.g., MeerKAT polarization processing). Specified identically to the calmode parameter. discard_nloops: Discard the first N loops. This is useful if the initial images are intentionally “quick-and-dirty” (e.g. large and poor resolution) and intended to generate more accurate source masks, for example. It will only count and discard the gain solutions from loops where calmode is not blank. outlier_threshold: The threshold value to identify an outlier. Considered to be an SNR threshold if &gt; 1 and an absolute value in Jy if &lt; 1. We recommend 0.5 as a good default, although using &#39;&#39; or 0.0 (the default) disables outlier imaging. outlier_radius: The radius in degrees used to identify outliers. When unset (&#39;&#39; or 0.0), the pipeline will calculate an appropriate radius based on the FWHM of the primary beam. The algorithm for determining and imaging outliers is explained in much more detail in the next section. Outlier Imaging An optional part of the self-calibration routine within processMeerKAT is the outlier imaging. Within this mode, we utilise the outlierfile parameter within CASA’s tclean task, which allows the positions of strong off-axis outliers to be specified, along with a number of other imaging parameters, as outlined below. Within this routine, CASA phase-rotates to the specified position, enabling a small image to be created that is centred on the source, without the need for W-projection. The motivation for including outlier imaging within processMeerKAT is to produce images equivalent in quality to much larger images (e.g. 10x10k pixels), with the strong sources far off axis being deconvolved, without the need for computational-heavy and long-running imaging over such a large area. However, an additional benefit is being able to specify precise positions of the outliers, centred within a pixel, with higher sampling of the beam over many smaller pixels, giving improved deconvolution that is not restricted by the pixel grid of the main image. For instance, without outliers, one may image 10,240 x 10,240 pixels with 1024 W-projection planes, deconvolving all the sources within the field of view, taking 10s of hours and lots of RAM to create an image. With outlier imaging, one could image 6144 x 6144 pixels with 512 W-projection planes within a few to several hours and with less RAM, and the few outliers above your threshold and beyond your main imaging area are deconvolved and included in your model for self-calibration. In order to construct a local sky model, from which outliers can be selected according to the specified threshold, we make use of the Rapid ASKAP Continuum Survey (RACS). Durning the [-R --run] step, we query the RACS catalogue for all sources (in and away from the galactic plane) within outlier_radius degrees (e.g. calculated at ~2 degrees) of the image’s phase centre, written to RACS_local.fits. We then select the sources above your threshold, written as an outlierfile to outliers.txt. We then select sources from this list that are outside the inner 99% of your main imaging region (i.e. include as outliers any sources within the outer 1% and beyond) for each selfcal loop, written to outliers_loop0.txt, outliers_loop1.txt, etc. This is performed separately for each loop since the image size may change between loops. Each outlierfile is passed into the tclean call for that loop, including selfcal_part2, in which the image (including outliers) is used to predict to the MODEL_DATA column, used for subsequest self-calibration. During each selfcal loop, the mask and position are updated based on the output from running PyBDSF. A mask is contructed corresponding to the island boundaries, the same as is done for the main image. The position is taken from the resulting PyBDSF catalogue, corresponding to the brightest Gaussian component fit to the source(s), which is often the only source found within the small image. If no source is found, or if the total integrated flux over the catalogue for that outlier is &lt; 1 mJy, the outlier is discarded. We recommend having fewer than ~10 outliers, since the overhead in run-time caused by imaging outliers outweighs any improvement in run-time caused by creating a smaller main image with fewer W-projection planes. When the [-R --run] step is performed, a warning will be displayed when the number of outliers exceeds 10 sources, in which case, a good compromose may be a larger image with fewer outliers, or a higher outlier threshold. Outlier imaging is disabled by default, but can be enabled by setting a value for the outlier_threshold parameter in the [selfcal] section of your config file. Similar to the threshold parameter, this is considered a S/N value if it’s greater than 1, or otherwise in Jy units. A good default is 0.5, which selects all sources above 0.5 Jy that are outside the your main image area. When a S/N value is used, the threshold corresponds to the value given by the integrated flux density divided by its uncertainty, taken from the RACS catalogue. For each outlier within your outlierfile, by default, we set a 128x128 imsize, 1 arcsec cell sizes, and the standard gridder, leaving all other options identical to what is specified for the main image (some of which is set in your config file). However, as specified in the CASA docs, the following parameters are fully configurable within your outlierfile: imagename, imsize, cell, phasecenter, startmodel, mask, specmode, nchan, start, width, nterms, reffreq, gridder, deconvolver, wprojplanes If you wish to use non-default values, you can do this by editing the selfcal_part2 script. Similarly, outlier imaging can be performed during the science imaging step of processMeerKAT, by passing an outlier file into the outlierfile parameter from the [imaging] section of your config file. If this section exists (enabled during the build step with processMeerKAT.py -B -I), the outlier file from your last self-calibration loop will be copied to the outlierfile parameter, in the same way that rmsmap and mask from your last selfcal loop are copied to this section, enabling S/N-based thresholding and masked deconvolution, respectively. Known Issues During outlier imaging, the resolution of the output data products, from both the outliers and main image, including the PSF, is observed to be broader, from an apparent difference in weighting. This may be due to the local PSF being worse at the far-off-axis position of the outlier, where it is more severely affected by bandwidth and time smearing, compared to the centre of the image. At its extreme, this effect significantly worsens the quality of the output images, which we speculate is due to CASA attempting to match the PSF of the main image to that of the worst outlier. CASA contains a bug such that Stokes=&#39;IQUV&#39; cannot be specified during outlier imaging, but only Stokes=&#39;I&#39; can be specified.",
    "url": "http://localhost:4000/docs/processMeerKAT/self-calibration-in-processmeerkat/",
    "relUrl": "/docs/processMeerKAT/self-calibration-in-processmeerkat/"
  },
  "17": {
    "id": "17",
    "title": "Using the Pipeline",
    "content": "Usage The usage can be seen by running processMeerKAT.py -h the output of which is documented below. Simple usage To get things working, source setup.sh, which will add to your $PATH and $PYTHONPATH (add this to your ~/.profile, for future use) source /idia/software/pipelines/master/setup.sh To print the version of the pipeline, run processMeerKAT.py -V To build a config file, which the pipeline reads as input for how to process the data, run processMeerKAT.py -B -C myconfig.txt -M mydata.ms To run the pipeline, run processMeerKAT.py -R -C myconfig.txt This will create submit_pipeline.sh, which you can then run to submit all pipeline jobs to a SLURM queue: ./submit_pipeline.sh Display a summary of the submitted jobs ./summary.sh Kill the submitted jobs ./killJobs.sh If the pipeline crashes, or reports an error, find the error(s) by running (after the pipeline has run) ./findErrors.sh Once the pipeline has completed, display the start and end times of each job by running ./displayTimes.sh Detailed usage Build config file using a custom SLURM configuration (nodes and tasks per node may be overwritten in your config file with something more appropriate by the end of the build step) processMeerKAT.py -B -C myconfig.txt -M mydata.ms -p Test02 -N 10 -t 8 -D 4 -m 100 -T 06:00:00 -n run1_ Build config file using different MPI wrapper and container processMeerKAT.py -B -C myconfig.txt -M mydata.ms --mpi_wrapper /path/to/another/mpi/wrapper --container /path/to/another/container Build config file with different set of (python) scripts processMeerKAT.py -B -C myconfig.txt -S /absolute/path/to/my/script.py False /absolute/path/to/container.simg -S partition.py True &#39;&#39; -S relative/path/to/my/script.py True relative/path/to/container.simg -S flag_round_1.py True &#39;&#39; -S script_in_bash_PATH.py False container_in_bash_PATH.simg -S setjy.py True &#39;&#39; Run the pipeline immediately in verbose mode processMeerKAT.py -R -v -s -C myconfig.txt NOTE: All other command-line arguments passed into processMeerKAT.py when using option [-R --run] will have no effect, since the arguments are read from the config file at this point. Only options [-s --submit], [-v --verbose] and [-C --config] will have any effect at this point. Similarly, changing the [slurm] section in your config file after using option [-R --run] will have no effect unless you [-R --run] again. Command-line options The command line help text is: usage: /idia/software/pipelines/master/processMeerKAT/processMeerKAT.py [-h] [-M path] [-C path] [-N num] [-t num] [-D num] [-m num] [-p name] [-T time] [-S script threadsafe container] [-b script threadsafe container] [-a script threadsafe container] [--modules [module [module ...]]] [-w path] [-c path] [-n unique] [-d list] [-e nodes] [-A group] [-r name] [-l] [-s] [-v] [-q] [-P] [-2] [-I] [-x] [-j] (-B | -R | -V | -L) Process MeerKAT data via CASA MeasurementSet. Version: 2.0 optional arguments: -h, --help show this help message and exit -M path, --MS path Path to MeasurementSet. -C path, --config path Relative (not absolute) path to config file. -N num, --nodes num Use this number of nodes [default: 1; max: 79]. -t num, --ntasks-per-node num Use this number of tasks (per node) [default: 16; max: 32]. -D num, --plane num Distribute tasks of this block size before moving onto next node [default: 1; max: ntasks-per-node]. -m num, --mem num Use this many GB of memory (per node) for threadsafe scripts [default: 232; max: 232]. -p name, --partition name SLURM partition to use [default: &#39;Main&#39;]. -T time, --time time Time limit to use for all jobs, in the form d-hh:mm:ss [default: &#39;12:00:00&#39;]. -S script threadsafe container, --scripts script threadsafe container Run pipeline with these scripts, in this order, using these containers (3rd value - empty string to default to [-c --container]). Is it threadsafe (2nd value)? -b script threadsafe container, --precal_scripts script threadsafe container Same as [-S --scripts], but run before calibration. -a script threadsafe container, --postcal_scripts script threadsafe container Same as [-S --scripts], but run after calibration. --modules [module [module ...]] Load these modules within each sbatch script. -w path, --mpi_wrapper path Use this mpi wrapper when calling threadsafe scripts [default: &#39;mpirun&#39;]. -c path, --container path Use this container when calling scripts [default: &#39;/idia/software/containers/casa-6.4.4-modular.simg&#39;]. -n unique, --name unique Unique name to give this pipeline run (e.g. &#39;run1_&#39;), appended to the start of all job names. [default: &#39;&#39;]. -d list, --dependencies list Comma-separated list (without spaces) of SLURM job dependencies (only used when nspw=1). [default: &#39;&#39;]. -e nodes, --exclude nodes SLURM worker nodes to exclude [default: &#39;&#39;]. -A group, --account group SLURM accounting group to use (e.g. &#39;b05-pipelines-ag&#39; - check &#39;sacctmgr show user $USER cluster=ilifu-slurm20 -s format=account%30&#39;) [default: &#39;b03-idia-ag&#39;]. -r name, --reservation name SLURM reservation to use. [default: &#39;&#39;]. -l, --local Build config file locally (i.e. without calling srun) [default: False]. -s, --submit Submit jobs immediately to SLURM queue [default: False]. -v, --verbose Verbose output? [default: False]. -q, --quiet Activate quiet mode, with suppressed output [default: False]. -P, --dopol Perform polarization calibration in the pipeline [default: False]. -2, --do2GC Perform (2GC) self-calibration in the pipeline [default: False]. -I, --science_image Create a science image [default: False]. -x, --nofields Do not read the input MS to extract field IDs [default: False]. -j, --justrun Just run the pipeline, don&#39;t rebuild each job script if it exists [default: False]. -B, --build Build config file using input MS. -R, --run Run pipeline with input config file. -V, --version Display the version of this pipeline and quit. -L, --license Display this program&#39;s license and quit. Selecting MS and field IDs As previously stated, to build a config file, run processMeerKAT.py -B -C myconfig.txt -M mydata.ms This calls CASA and adds a [data] section to your config file, which points to your MS, and a [fields] section, which points to the field IDs you want to process as bandpass, total flux and phase calibrators, and science target(s), as extracted from your input MS via their INTENT labels. Only targets and extra fields may have multiple fields separated by a comma, and all extra calibrator fields are appended as “targets”, to allow for solutions to be applied to them, and quick-look images to be made of them (see v1.0 release notes). The following is an example of what is appended to the bottom of your config file. [data] vis = &#39;/idia/software/pipelines/test_data/mightee_cdfs_1350_1400mhz.ms&#39; [fields] bpassfield = &#39;J1939-6342&#39; fluxfield = &#39;J1939-6342&#39; phasecalfield = &#39;J0240-2309&#39; targetfields = &#39;CDFS16&#39; extrafields = &#39;J0521+1638&#39; You can edit your config file and change the field IDs, as discussed in config files.",
    "url": "http://localhost:4000/docs/processMeerKAT/using-the-pipeline/",
    "relUrl": "/docs/processMeerKAT/using-the-pipeline/"
  }
  
}
